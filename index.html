<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>API API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>API</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
# coding: utf-8

# In[8]:


def time_lag(lcs,m_v,percentage_limit=90):
    &#34;&#34;&#34;
    Find the time lag between two light curves. 
    
    Parameters:
    -----------
    lcs: class: list of two &#39;Lightcurve&#39;-objects
                The light curves to be used in the coherence computation.
    
    m_v: np.ndarray
        Array of number of segments to use, in increasing order.
        format: m_v[m_1,m_2,...], where type(m_i) = int, preferably a power of 2, and m_1 &lt; m_2 etc.  
        
    percentage_limit: float, optional, default: 90
            Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.
    co
    Returns:
    --------
    xf_v: np.ndarray
        Frequencies.
    
    tau_v: np.ndarray
        Time lag.
    
    dtau_v: np.ndarray
        Error in time lag.
        
    num: int
        The number of logarithmic frequency bins to create before plotting.
    &#34;&#34;&#34;
    
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
    print(&#39;                           Computing the time lag...&#39;)
    print(&#39;---------------------------------------------------------------------------------------------------\n&#39;)
    start = timeit.default_timer()
        
    assert isinstance(lcs[0], lightcurve) and isinstance(lcs[1], lightcurve), &#39;The lcs-input does not contain two light curve objects.&#39;
    if len(m_v) &gt;= 2:
        for i in range(0,len(m_v)-1):
            assert m_v[i] &lt; m_v[i+1], &#39;Make sure that m_v[i] &lt; m_v[i+1] for all i&#39;
    
    xf_v = np.array([])
    tau_v = np.array([])
    dtau_v = np.array([])
    dt = lcs[0].dt
    
    freq_uplim = 1/(2*dt)
    for i in range(0,len(m_v)):
        m = m_v[i]
        print(&#39;Iteration {}) Computing using m = {} bins per segment, i.e. f in [{:.3f},{:.3f}]&#39;.format(i+1,m,1/(m*dt),freq_uplim))
        print(&#39;---------------------------------------------------------------------------------------------------&#39;)
        
        # Find cross spectra
        ps_v, C_v = cross_spec(lcs,m=m,percentage_limit=percentage_limit)

        # Coherence
        xf, gamma2, delta_gamma2, _ = coherence_noiseless(ps_v,m=m,C_v=C_v)
        
        K = np.size(C_v,axis=0) #number of segments 
        C = np.mean(C_v,axis=0) #mean cross spectra
        
        tau = np.angle(C)/(2*np.pi*xf)
        dtau = np.sqrt((1-gamma2)/(2*gamma2*K))/(2*np.pi*xf) 
        
        if freq_uplim != -1:
            print(&#39;Only use freq &lt; {:.3f} to avoid to much overlaping.&#39;.format(freq_uplim))
            tau = tau[xf &lt; freq_uplim]
            dtau = dtau[xf &lt; freq_uplim]
            xf = xf[xf &lt; freq_uplim]
        
        xf_v = np.append(xf_v,xf)
        tau_v = np.append(tau_v,tau)
        dtau_v = np.append(dtau_v,dtau)
        
        # If len(m) &gt;= 2, then we don&#39;t want the two computations to overlap too much... 
        # Now we make them overlap a factor 10 larger than the lowest frequency from the small m
        # E.g. if m = [2**8,2**17], dt = 0.002 --&gt; freq_uplim = 1.96*10, meaning that even though 
        # m=2**17 covers f\in[0.0038,249] we only look at f\in[0.0038,19.6]. The high freq interval was
        # taken care of by m = [2**8].
        freq_uplim = np.amin(xf_v)*10
        print(&#39;Time lag for m = {} computed. \n&#39;.format(m))
     
    time_taken = timeit.default_timer()-start
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
    print(&#39;                           Time lag found (in {:.2f} sec).&#39;.format(time_taken))
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
        
    return xf_v, tau_v, dtau_v

def plot_timelag(xf_v,tau_v,dtau_v,num=50,save_fig=False):
    &#34;&#34;&#34;
    Plot the time lag.
    
    Parameters:
    -----------
    xf_v: np.ndarray
        Frequencies.
    
    tau_v: np.ndarray
        Time lag.
    
    dtau_v: np.ndarray
        Error in time lag.
        
    save_fig: boolean, optional, default: False
         If True, asks for path to location to save plot.
    &#34;&#34;&#34;
    
    # Rebin
    xf_rebin,tau,dtau = log_rebin(xf_v, tau_v, dtau_v,num=num)
    
    ax = standard_plot(w=6)
    
    # Separate into pos/neg values of tau
    tau_n, tau_p = [-t for t in tau if t&lt;0 ],[t for t in tau if t&gt;0]
    x_n, x_p = [x for x,t in zip(xf_rebin,tau) if t&lt;0],[x for x,t in zip(xf_rebin,tau) if t&gt;0]
    dtau_n, dtau_p = [x for x,t in zip(dtau,tau) if t&lt;0],[x for x,t in zip(dtau,tau) if t&gt;0]
    
    # Loop over neg vs pos lags
    for x,tau,dtau,c,s in zip([x_n, x_p],[tau_n, tau_p],[dtau_n, dtau_p],[&#39;w&#39;,&#39;k&#39;],[5,4.5]):

        # Replace errors that go below 0 with a downarrow
        for i in range(0,len(x)):
            
            if abs(dtau[i]) &gt; abs(tau[i]): #if error is larger than tau-value
                # Plot lower error as an arrow
                ax.errorbar(x[i],tau[i],yerr=[[0.7*tau[i]],[0]], fmt = &#39;ok&#39;, uplims = True, mfc=c, markersize=s, elinewidth=1)
                # Plot upper error as normal
                ax.errorbar(x[i],tau[i],yerr=[[0],[dtau[i]]], fmt = &#39;ok&#39;, mfc=c, markersize=s, capsize=2, elinewidth=1, markeredgewidth=1)
            else:
                pass
                # Plot as normal
                ax.errorbar(x[i],tau[i],yerr=[[dtau[i]],[dtau[i]]], fmt = &#39;ok&#39;, mfc=c, markersize=s, capsize=2, elinewidth=1, markeredgewidth=1)

    ax.set_xscale(&#39;log&#39;)
    ax.set_yscale(&#39;log&#39;)
    plt.xlabel(&#39;Frequency [Hz]&#39;)
    plt.ylabel(&#39;Lag (sec.)&#39;)
    first_band = &#39;0-3.9&#39;
    second_band = &#39;3.9-6.0&#39;
    ax.text(0.73,0.93,&#39;({} keV) vs. ({} keV)&#39;.format(first_band,second_band),fontsize=12,horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;, transform=ax.transAxes)
    if save_fig:
        path = input(&#39;Path to location to save plot [end with .png]: &#39;) 
        plt.savefig(path,bbox_inches=&#39;tight&#39;)
    plt.show()

def rms_freqband(lc,ps,m,low_freq=None,up_freq=None,units=&#39;abs&#39;):
    &#34;&#34;&#34;
    Computes the rms in a given frequency band for a given energy (as given by 
    the lightcurve and corresponding power spectrum).
    
    Parameters:
    -----------
    lc: class &#39;Lightcurve&#39;-object
            The light curve, whose rms is to be found.
    
    ps: class: &#39;PowerSpectrum&#39;-object
            The corresponding (important!) power spectra to be used.
    
    m: int
        Number of bins per segment.
        
    low_freq/up_freq: floats
        Lower and upper frequency limits.
        
    units: {&#39;abs&#39;,&#39;frac&#39;}, optional, default: &#39;abs&#39;
        What unit to return the rms in.
        
    Returns:
    --------
    sigma: np.float
        Absolute/fractional rms.
    
    sigma_err: np.float
        The error in (absolute/fractional) rms. See Eq. (14) of Uttley (2014): &#34;X-ray reverberation around accreting black holes&#34;
    &#34;&#34;&#34;
    
    # If no low/high freq limits given:
    if low_freq == None:
        low_freq = ps.xf[0]
    if up_freq == None:
        up_freq = ps.xf[-1]
    # Frequency range
    dnu = up_freq-low_freq
    print(&#39;Freq range = {:.5f}-{:.5f} Hz&#39;.format(low_freq,up_freq))
    
    # Noise Power is constant in freq!
    P_noise = ps.averagePnoise
    
    # Error variance
    if units == &#39;abs&#39;:
        sigma_noise2 = P_noise*dnu*lc.R**2
        print(&#39;R = &#39;,lc.R)
    else:
        sigma_noise2 = P_noise*dnu #now it is rather rms, not sigma...
    
    # Find rms within the given interval
    xf, fft_rate = remove_freq(ps.xf,ps.fft_rate,limit=low_freq,geq=True,disregard=True)
    xf, fft_rate = remove_freq(xf,fft_rate,limit=up_freq,leq=True,disregard=True)
    
    rms = np.sqrt(dnu * np.mean(fft_rate)) #\approx same as Fvar_from_ps(xf,fft_rate)
    #rms_lc = Fvar_from_lc(lc,m,percentage_limit=90) # --&gt; yield the same value
    
    if units == &#39;abs&#39;: 
        sigma = rms*lc.R
    else:
        sigma = rms #now it is rather rms, not sigma...
    
    # Error: 
    sigma_err = np.sqrt((2*sigma**2*sigma_noise2+sigma_noise2**2)/(len(lc.t)*sigma**2))
    
    return sigma, sigma_err

def rms_vs_energy(lcs,ps_v,channel_to_kev,m=2**13,low_freq=None,up_freq=None,units=&#39;abs&#39;):
    &#34;&#34;&#34;
    Compute the fractional variance amplitude (rms) for multiple light curves in a given frequency band.
    
    Parameters:
    -----------
    lcs: class: list of &#39;Lightcurve&#39;-objects
                The light curves to be used.
    
    ps_v: class: list of &#39;PowerSpectrum&#39;-objects
                The corresponding (important!) power spectras to be used.
    
    m: int
        Number of bins per segment.
        
    low_freq/up_freq: floats
        Lower and upper frequency limits.
        
    units: {&#39;abs&#39;,&#39;frac&#39;}, optional, default: &#39;abs&#39;
        What unit to return the rms in.        

           
    Returns:
    --------
    energy_mid_v: np.ndarray 
        The average energy from each light curve&#39;s energy band. 
    
    energy_err_v: np.ndarray 
        Half the size of each light curve&#39;s energy band. 
        
    rms_v: np.ndarray 
        Absolute/fractional rms.
        
    rms_err_: np.ndarray 
        The error in (absolute/fractional) rms.
    &#34;&#34;&#34;
    
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
    print(&#39;                           Computing the rms...&#39;)
    print(&#39;---------------------------------------------------------------------------------------------------\n&#39;)
    start = timeit.default_timer()
    
    rms_v, rms_err_v = [], []
    
    for lc,ps in zip(lcs,ps_v):     
        # Find rms                
        rms, rms_err = rms_freqband(lc,ps,m,low_freq,up_freq,units=units)
        
        rms_v.append(rms)
        rms_err_v.append(rms_err)
        
    time_taken = timeit.default_timer()-start
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
    print(&#39;                           Rms found (in {:.2f} sec).&#39;.format(time_taken))
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
     
    return np.array(rms_v),np.array(rms_err_v)


def multiply_w_spectra(lcs,rms_v,rms_err_v,channel_to_kev,spectral_data):
    &#34;&#34;&#34;
    Multiplies rms (or covariance) with spectra do obtain rms/covariance spectra.
    Might only work well for RXTE (where channels are given by their energy max): https://heasarc.gsfc.nasa.gov/docs/xte/e-c_table.html 
    
    Parameters:
    -----------
    lcs: class: list of &#39;Lightcurve&#39;-objects
                The light curves to be used.
            
    rms_v: np.ndarray 
        Absolute/fractional rms.
        
    rms_err_v: np.ndarray 
        The error in (absolute/fractional) rms.
        
    channel_to_kev: np.ndarray
        Conversion from channel (index) to energy (keV).
    
    spectral_data: dict or None, optional, default: None
        Spectral data of the observation. If dict, rms is scaled with the spectral_data to yield the rms spectra.
        
    Returns:
    --------
    rms_v: np.ndarray 
        Absolute/fractional rms mulitplied with the spectra.
        
    rms_errv_: np.ndarray 
        The error in (absolute/fractional) rms mulitplied with the spectra.
    &#34;&#34;&#34;
    
    for i in range(0,len(rms_v)):
        lc = lcs[i]

        # Covert channel min/max to energy min/max
        minchan = lc.minchan
        if minchan != 0:
            minchan -= 1 # For RXTE: since each channel only corresponds to its energy max, we need to sub to get its min (the former channel&#39;s max)
        minene = channel_to_kev[minchan]
        if minchan == 0:
            minene = 0
        maxchan = lc.maxchan
        maxene = channel_to_kev[maxchan]

        scale_rms = np.mean(spectral_data[&#39;COUNTS/SEC&#39;][minchan:maxchan])/lc.deltaE
        scale_err = np.mean(spectral_data[&#39;STATERR/SEC&#39;][minchan:maxchan])/lc.deltaE
        #print(&#39;dkeV, counts, scale_rms, scale_err = &#39;,lc.deltaE,&#39;, &#39;,np.mean(data[&#39;COUNTS/SEC&#39;][minchan:maxchan]),&#39;, &#39;,scale_rms,&#39;, &#39;,scale_err,&#39;\n&#39;)

        rms_v[i] *= scale_rms
        rms_err_v[i] *= scale_err
        
    return rms_v, rms_err_v
        

def subtract_overlapping_energybands(lcs):
    &#34;&#34;&#34;
    When the covariance is being calculated for an energy channel inside the reference band, 
    the channel of interest is removed from the reference band. The reasoning behind this is 
    that if the channel of interest is duplicated in the reference band, the Poisson error 
    contribution for that channel will not cancel and will contaminate the covariance.
    
    Parameters:
    -----------
    lcs: list of two &#39;Lightcurve&#39;-objects
            The light curves to be used in the covariance computation. 
            lcs[0] = light curve of interest, lcs[1] = reference band.
            
    Returns:
    ------------
    lcs: list of two &#39;Lightcurve&#39;-objects
        The reference light curve, from which we potentially have subtracted the light curve of interest. 
        Or the light curve of interest, from which we potentially have subtracted the reference light curve.
    &#34;&#34;&#34;
    
    lc_X = lcs[0]
    lc_Y = lcs[1]
    
    ebandX = lc_X.channels
    eminX = lc_X.minchan
    emaxX = lc_X.maxchan
    ebandY = lc_Y.channels
    eminY = lc_Y.minchan
    emaxY = lc_Y.maxchan
    
    # X entirely within Y
    if eminX &gt;= eminY and emaxX &lt;= emaxY: 
        lc_Y.rate -= lc_X.rate
        lc_Y.err -= lc_X.err
        lc_Y.R = np.mean(lc_Y.rate)
        print(&#39;Removed rate and err of light curve of interest from the reference light curve.&#39;)
        
    # Y entirely within X
    if eminX &lt;= eminY and emaxX &gt;= emaxY: 
        lc_X.rate -= lc_Y.rate
        lc_X.err -= lc_Y.err
        lc_X.R = np.mean(lc_X.rate)
        print(&#39;Removed rate and err of the reference light curve from the light curve of interest.&#39;)

    return [lc_X,lc_Y]

def covariance_spectrum(lcs,m,alt=1,freq_low=None,freq_high=None,percentage_limit=90,units=&#39;abs&#39;,to_plot=False):
    &#34;&#34;&#34;
    Compute the covariance spectrum, which is more robust than the rms-spectra.
    
    Parameters:
    -----------
    lcs: list of two &#39;Lightcurve&#39;-objects
                The light curves to be used in the covariance computation.
    m: int
        Number of time bins per segment. 
        
    alt: int {1, 2}, optional, default: 1
        What method to compute the covariance with.
        If full freq range: 1 = for whole light curve directly, 2 = segment-wise 
        If smaller freq range: 1 = using FFT and inverseFFT, 2 = using coherence
    
    low_freq/up_freq: floats, optional, default: None
        Lower and upper frequency limits.
     
    percentage_limit: float, optional, default: 90
            Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.
    
    units: {&#39;abs&#39;,&#39;frac&#39;}, optional, default: &#39;abs&#39;
        What unit to return the rms in.
    
    to_plot: boolean (default: False)
        If True, a figure for different ways to compute the covariance is displayed. 
        
    Returns:
    -----------
    cov_norm: float
        The normalised coviarance as calculated by Eq(2) and Eq(3) from Wilkinson(2009).
        
    cov_err: float 
        The statistical error of the covariance.  
    &#34;&#34;&#34;
    start = timeit.default_timer()
    
    lcs = subtract_overlapping_energybands(lcs)
    
    # Extract data
    lc_X = lcs[0]
    t_X, rate_X, err_X, R_X = lc_X.t, lc_X.rate, lc_X.err, lc_X.R
    # and from Reference Band:
    lc_Y = lcs[1]
    t_Y, rate_Y, err_Y, R_Y = lc_Y.t, lc_Y.rate, lc_Y.err, lc_Y.R
    
    errX = np.mean(err_X**2)
    errY = np.mean(err_Y**2)
    
    comparison = t_X == t_Y
    assert comparison.all(), &#39;Time arrays are not identical. The two light curves must come from the same observation...&#39;

    # Useful parameters (take from reference band, but is the same as the corresponding X-band values)
    dt = t_Y[1]-t_Y[0]
    N = len(t_Y)
    
    # Full frequency range
    if freq_low == None and freq_high == None: 
        
        if alt == 1:
            ## ------------------------------------ Cov for whole light curve ---------------------------------------------
            print(&#39;---------------------------------------------------------------------------------------------------&#39;)
            print(&#39;                           Computing covariance for whole light curve directly...&#39;)
            print(&#39;---------------------------------------------------------------------------------------------------\n&#39;)

            # Prep Work to normalize covariance and to compute covariance error
            errX = np.mean(err_X**2)
            errY = np.mean(err_Y**2)

            var_ex_X_lc = 1/(len(rate_X)-1)*np.sum((rate_X-np.mean(rate_X))**2)-errX
            var_ex_Y_lc = 1/(len(rate_Y)-1)*np.sum((rate_Y-np.mean(rate_Y))**2)-errY
            assert var_ex_X_lc &lt; var_ex_Y_lc, &#34;The reference band should be the band with highest absolute variability.&#34;
            Fvar_lc = np.sqrt(var_ex_Y_lc)/np.mean(rate_Y) 

            # Compute Covariance
            cov = 1/(len(rate_Y)-1)*np.sum((rate_X-np.mean(rate_X))*(rate_Y-np.mean(rate_Y)))
                
            # Normalize
            cov = cov/np.sqrt(var_ex_Y_lc)
            # Error
            cov_err = np.sqrt((var_ex_X_lc*errY+var_ex_Y_lc*errX+errX*errY)/(len(t_X)*var_ex_Y_lc))
            
            if units != &#39;abs&#39;: 
                cov /= R_X 
                cov_err /= R_X
                
            print(&#39;Cov = &#39;,cov,&#39; pm &#39;,cov_err,&#39;\n&#39;)

        elif alt == 2:
            ## ------------------------------------- Cov for segments ---------------------------------------------------------
            print(&#39;---------------------------------------------------------------------------------------------------&#39;)
            print(&#39;                           Computing covariance segment-wise...&#39;)
            print(&#39;---------------------------------------------------------------------------------------------------\n&#39;)
            
            # Prep Work to normalize covariance and to compute covariance error
            ps_X = PowerSpectrum(lcs[0],m=m,percentage_limit=percentage_limit,timer_on=False,save_all=True)
            ps_Y = PowerSpectrum(lcs[1],m=m,percentage_limit=percentage_limit,timer_on=False,save_all=True)
            
            fft_rate_meanX = ps_X.fft_rate
            fft_rate_meanY = ps_Y.fft_rate

            var_ex_X = [1/(m-1)*np.sum((rate_seg-R_seg)**2)-np.mean(err_seg**2) for rate_seg, R_seg, err_seg in zip(ps_X.rate_seg, ps_X.R_seg, ps_X.err_seg)]
            var_ex_Y = [1/(m-1)*np.sum((rate_seg-R_seg)**2)-np.mean(err_seg**2) for rate_seg, R_seg, err_seg in zip(ps_Y.rate_seg, ps_Y.R_seg, ps_Y.err_seg)]

            var_err_X = [np.mean(e**2) for e in ps_X.err_seg]
            var_err_Y = [np.mean(e**2) for e in ps_Y.err_seg]
            
            P_X_mean = np.mean(ps_X.fft_rate)
            P_Y_mean = np.mean(ps_Y.fft_rate)

            # Should we take the average over all segments to get the final variance error? 
            var_err_X_mean = np.mean(var_err_X,axis=0)
            var_err_Y_mean = np.mean(var_err_Y,axis=0)

            # Excess Variance 
            var_ex_X_mean = np.mean(var_ex_X)
            var_ex_Y_mean = np.mean(var_ex_Y)
            assert var_ex_X_mean &lt; var_ex_Y_mean, &#34;The reference band should be the band with highest absolute variability.&#34;

            # Quantities neeeded:
            cov = [1/(m-1)*np.sum((rate_seg_X-R_X_seg)*(rate_seg_Y-R_Y_seg)) for rate_seg_X, rate_seg_Y, R_X_seg, R_Y_seg in zip(ps_X.rate_seg, ps_Y.rate_seg, ps_X.R_seg, ps_Y.R_seg)]
            if units != &#39;abs&#39;: 
                cov = [c/R for c,R in zip(cov,ps_X.R_seg)]
            
            # 1) Small difference between taking average over all covariances and using full excess variance to normalize
            cov_mean = np.mean(cov,axis=0)
            cov_norm_alt1 = cov_mean/np.sqrt(var_ex_Y_mean)
            print(&#39;cov_norm = &#39;,cov_norm_alt1)
            # 2) vs using excess variance from each segment to normalize and then taking the average 
            cov_seg_norm = np.array(cov)/np.sqrt(var_ex_Y)
            cov_norm_alt2 = np.mean(cov_seg_norm,axis=0)
            print(&#39;cov_norm = &#39;,cov_norm_alt2)
            
            cov = cov_norm_alt2
            cov_err = np.sqrt((var_ex_X_mean*var_err_Y_mean+var_ex_Y_mean*var_err_X_mean+var_err_X_mean*var_err_Y_mean)/(N*var_ex_Y_mean))
            if units != &#39;abs&#39;: 
                cov_err /= R_X
            print(&#39;Cov = &#39;,cov,&#39; pm &#39;,cov_err,&#39;\n&#39;)
                  
    # If not full freq. range
    else:
        if alt == 1:
            ## ------------------------------------ Cov using FFT and inverseFFT ---------------------------------------------       
            print(&#39;---------------------------------------------------------------------------------------------------&#39;)
            print(&#39;                           Computing covariance using FFT and inverseFFT...&#39;)
            print(&#39;---------------------------------------------------------------------------------------------------\n&#39;)
            
            xf = np.array(fftfreq(N, dt))[1:N//2]
            # Pick out frequency range 
            if freq_low == None:
                freq_low = xf[0]
            if freq_high == None:
                freq_high = xf[-1]
                
            rate_X = pick_out_freq_from_lc(lcs[0], freq_low, freq_high)
            rate_Y = pick_out_freq_from_lc(lcs[1], freq_low, freq_high)
                        
            # rms needs to be taken from normalized power spectra in the relevant freq range
            ps_X, ps_Y = PowerSpectrum(lcs[0],m=N,timer_on=False,save_all=True,percentage_limit=0), PowerSpectrum(lcs[1],m=N,timer_on=False,save_all=True,percentage_limit=0)
            fft_rate_meanX, fft_rate_meanY = ps_X.fft_rate, ps_Y.fft_rate
            
            xf, fft_rates = remove_freq(xf,[fft_rate_meanX,fft_rate_meanY],freq_low,geq=True,disregard=False)
            xf, fft_rates = remove_freq(xf,[fft_rates[0],fft_rates[1]],freq_high,leq=True,disregard=False)
            
            FvarX, FvarY = Fvar_from_ps(xf, fft_rates[0]), Fvar_from_ps(xf, fft_rates[1])
            #print(&#39;FvarY = &#39;,FvarY)
            var_ex_X_lc, var_ex_Y_lc = (FvarX * R_X)**2, (FvarY * R_Y)**2
            assert var_ex_X_lc &lt; var_ex_Y_lc, &#34;The reference band should be the band with highest absolute variability.&#34;
            
            # Compute cov
            cov = 1/(len(rate_Y)-1)*np.sum((rate_X-np.mean(rate_X))*(rate_Y-np.mean(rate_Y)))
            # Normalize                                     
            cov = cov/np.sqrt(var_ex_Y_lc)
            
            # Error
            P_noise_X_full = np.mean(err_X**2)/(R_X**2*1/(2*dt))
            P_noise_Y_full = np.mean(err_Y**2)/(R_Y**2*1/(2*dt))
            errX = P_noise_X_full*R_X**2*(freq_high-freq_low)
            errY = P_noise_Y_full*R_Y**2*(freq_high-freq_low)
            cov_err = np.sqrt((var_ex_X_lc*errY+var_ex_Y_lc*errX+errX*errY)/(len(t_X)*var_ex_Y_lc))
            
            if units != &#39;abs&#39;: 
                cov /= R_X 
                cov_err /= R_X
            print(&#39;Cov = &#39;,cov,&#39; pm &#39;,cov_err,&#39;\n&#39;)
            
        elif alt == 2:
            ## ----------------------------------------------- Cov using coherence ------------------------------------------------------------
            print(&#39;---------------------------------------------------------------------------------------------------&#39;)
            print(&#39;                          Covariance using coherence...&#39;)
            print(&#39;---------------------------------------------------------------------------------------------------\n&#39;)

            upper_freq_lim = freq_high
            assert upper_freq_lim != None, &#34;You need to set an upper freq. limit; will be the same for all lc.&#34;

            # Compute intrinsic coherence 
            xf_coh, gamma2, delta_gamma2_int = coherence_intrinsic(lcs,m_init=2**16)
            xf_coh, gamma2 = remove_freq(xf_coh,gamma2,limit=upper_freq_lim,leq=True)
            dnu = upper_freq_lim-xf_coh[0]

            # Should use the fft_rate_meanX/fft_rate_meanY from the coherence computation (that uses different m), 
            # but this is not implemented...
            # ---- This part will do until fixed.  Small difference... -----------
            ps_X = PowerSpectrum(lcs[0],m=m,percentage_limit=percentage_limit,timer_on=False,save_all=True)
            ps_Y = PowerSpectrum(lcs[1],m=m,percentage_limit=percentage_limit,timer_on=False,save_all=True)
            xf = ps_X.xf

            fft_rate_meanX = ps_X.fft_rate
            fft_rate_meanY = ps_Y.fft_rate
            
            # ----------------------------------------------------------------------------------------------------

            _, fft_rate_meanX_test = remove_freq(xf,fft_rate_meanX,limit=upper_freq_lim,leq=True)
            P_X_mean = np.mean(fft_rate_meanX_test)

            _, fft_rate_meanY_test = remove_freq(xf,fft_rate_meanY,limit=upper_freq_lim,leq=True)
            P_Y_mean = np.mean(fft_rate_meanY_test)

            # Compute covariance
            cov = R_X*np.sqrt(np.mean(gamma2)*P_X_mean*dnu)
            print(&#39;Cov = &#39;,cov)

            # Error
            P_noise_X_full = np.mean(err_X**2)/(R_X**2*1/(2*dt))
            P_noise_Y_full = np.mean(err_Y**2)/(R_Y**2*1/(2*dt))

            errX = R_X*P_noise_X_full*dnu
            errY = R_Y*P_noise_Y_full*dnu
            var_ex_X_lc = R_X*P_X_mean*dnu
            var_ex_Y_lc = R_Y*P_Y_mean*dnu
            assert var_ex_X_lc &lt; var_ex_Y_lc, &#34;The reference band should be the band with highest absolute variability.&#34;
            
            cov_err = np.sqrt((cov_through_gamma2**2*errY+var_ex_Y_lc*errX+errX*errY)/(len(t_X)*var_ex_Y_lc))

            if units != &#39;abs&#39;: 
                cov /= R_X 
                cov_err /= R_X
            
            print(&#39;Cov = &#39;,cov,&#39; pm &#39;,cov_err,&#39;\n&#39;)
    
    time_taken = timeit.default_timer()-start
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
    print(&#39;                           Covariance found (in {:.2f} sec).&#39;.format(time_taken))
    print(&#39;---------------------------------------------------------------------------------------------------&#39;) 
    
    return cov, cov_err

def pick_out_freq_from_lc(lc, low_freq, up_freq, to_plot=False):
    &#34;&#34;&#34;
    Fourier transforms a light curve, sets the power = 0 for all freq outside freq_range: freq_low-freq_high,
    and then performs an inverse Fourier transform back to time-domain.
    
    Parameters:
    -----------
    lc: class &#39;Lightcurve&#39;-object
            The light curve, whose rms is to be found.
            
    low_freq/up_freq: floats, optional, default: None
        Lower and upper frequency limits.
    
    Returns:
    --------
    rate_ifft: np.ndarray
        The inverse Fourier transformed rate-vector, with mean = 0.
    &#34;&#34;&#34;
    
    # Pick out relevant quantties from the lightcurves
    t, dt, rate, err, R, N = lc.t, lc.dt, lc.rate, lc.err, lc.R, lc.N
    
    # FFT on full light curve
    xf = np.array(fftfreq(N, dt))[1:N//2]
    fft_rate_unnormalized = fft(rate-R)[1:N//2]
                      
    if low_freq == xf[0] and up_freq == xf[-1]: 
        print(&#34;You&#39;re using the full freq. range.&#34;)
    else:
        print(&#39;Prepare inverse FFT using only the freq interval: [{},{}]&#39;.format(low_freq, up_freq)) 
    
    xf, fft_rate = remove_freq(xf,fft_rate_unnormalized,low_freq,geq=True,disregard=False)
    xf, fft_rate = remove_freq(xf,fft_rate,up_freq,leq=True,disregard=False)
    
    # Transform back 
    rate_ifft = ifft_smallfreqband(fft_rate)
    print(&#39;Inverse FFT performed.&#39;)

    if to_plot:
        standard_plot()
        plt.plot(t,rate-R,label=&#39;Lc with all freq&#39;)
        plt.plot(t,rate_ifft,label=&#39;Lc using only f = [{},{}]&#39;.format(low_freq,up_freq))
        plt.legend()
        ax = plt.gca()
        ax.set_xlim([t[0],t[500]])
        plt.show()
        
    return rate_ifft
    
def ifft_smallfreqband(fft_rate):
    &#34;&#34;&#34;
    Inverse transformation from freq domain to time domain. Is called upon in pick_out_freq_from_lc().
    
    Parameters:
    -----------
    fft_rate: np.ndarray
        The power spectra after having set values outside freq range to 0.
        
    Returns:
    --------
    rate: np.ndarray
        New rate vector, now only containing the frequencies in fft_rate. 
    &#34;&#34;&#34;
    
    # Add the negative freq again (that were removed due to [1:m//2])
    y_together = np.append(np.zeros(1),fft_rate)
    y_together = np.append(y_together,np.zeros(1))
    y_together = np.append(y_together,np.conjugate(np.flip(fft_rate)))
    # Perform ifft
    yinv = ifft(y_together)
    # Make to np.ndarray and extract only the real values
    yinv = np.array(yinv)
    rate = yinv.real  
    return rate 


class PowerSpectrum():
    &#34;&#34;&#34;
    Finds power spectra by splitting the light curve into segments. A power spectra for 
    each segment is computed and the final power spectra is the average over all these.

    Parameters:
    -----------
    lc: class: &#39;Lightcurve&#39;-object
        The light curve data to be Fourier-transformed.

    m: int
        Number of time bins per segment. 

    normalization: {&#39;rms&#39; (Miyamoto), &#39;abs&#39;, &#39;Leahy&#39;, or &#39;none&#39;}, optional, default: &#39;rms&#39;.
        What normalization to use, see Vaughan(2003, MNRAS 345).

    noise: {&#39;Poisson&#39;,&#39;Gaussian&#39;}, optional, default: &#39;Gaussian&#39;.
        For a light curve with &#39;Poisson&#39;/&#39;Gaussian&#39; errors. 

    percentage_limit: float, optional, default: 90
        Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.

    timer_on: boolean, optional, default: True
        Prints out progress during computation.
        
    return_noise_wo_sub: boolean, optional, default: False
        If True, noise is returned but not subtracted from powspec.
        If False, returned and subtracted.
        
    save_all: boolean, optional, default: False
        If True, save rate_seg, err_seg, R_seg. These are needed for covariance-computation.
    
    Attributes:
    -----------
    m: int
        Number of time bins per segment. 
    
    channels: string
        Channels used during the observation.
    
    minchan, maxchan: ints
        Min and max channels used.
    
    xf: np.ndarray
        The Fourier frequencies. 
    
    df: float
        Frequency resolution.
    
    fft_rate: np.ndarray
        Power spectra. Fast fourier transformation of, and average over all, rate_seg.
    
    fft_rate_v: list of np.ndarrays
        Power spectra for each segment.
        
    averagePnoise: float
        Average noise power for whole light curve.
    
    Pnoise_v: list of floats
        Noise power for each segment.
    
    Fvar: float
        Fractional variance amplitude (=rms) computed from the power spectra by integration.
    
    middle_of_log_bins: np.ndarray
        Logarithmic mid point 
    
    middle_of_log_bins: np.ndarray 
        The logarithmic midpoint of each log-bin, computed as: 10**(1/2*(np.log10(log_bins[i])+np.log10(log_bins[i+1])))
    
    fPf: np.ndarray
        Power spectra multiplied with f and re-binned logarithmically.
    
    fPferror: np.ndarray
        The standard deviation of all points within a log-bin for the fPf-vector.
    
    Pf: np.ndarray
        Power spectra re-binned logarithmically.
    
    Pferror: np.ndarray
        The standard deviation of all points within a log-bin for the Pf-vector.      
        
    In addition, if save_all:
        rate_seg: np.ndarray
            All segments&#39; rate-vectors.
            
        err_seg: np.ndarray
            All segments&#39; rate-error-vectors.
            
        R_seg: np.ndarray
            All segments&#39; mean count rates.
    &#34;&#34;&#34;
    
    def __init__(self, lc, m=2**13, normalization=&#39;rms&#39;, noise=&#39;Gaussian&#39;,                  percentage_limit=90, timer_on=True, return_noise_wo_sub=False, save_all = False):
        self.m = m
        self.channels, self.minchan, self.maxchan = lc.channels, lc.minchan, lc.maxchan
        self.xf, self.fft_rate, self.fft_rate_v, self.Pnoise_v = self.powspec(lc, normalization, noise, percentage_limit, timer_on, return_noise_wo_sub, save_all)
        self.df = self.xf[1]-self.xf[0]
        self.averagePnoise = np.mean(self.Pnoise_v)
        self.Fvar = self.Fvar_from_ps()
        self.middle_of_log_bins, self.fPf, self.fPferror, self.Pf, self.Pferror = self.rebin(init=True)
    
    def powspec(self,lc,normalization,noise,percentage_limit,timer_on,return_noise_wo_sub, save_all):
        print(&#39;Computing the power spectra using {} bins per segment, normalization &#34;{}&#34;, and noise dist &#34;{}&#34;...&#39;.format(self.m,normalization,noise))
        
        # Useful parameters
        #dt = (lc.t[-1]-lc.t[0])/lc.N
        dt = lc.dt
        K = int(np.floor(lc.N/self.m)) #num_of_line_seg

        # Average over time segments 
        fft_rate_v = []
        P_noise_v = []
        rate_seg_v = []
        err_seg_v = []
        R_seg_v = []
        num_discarded_segments = 0
        start = timeit.default_timer()
        perc_temp_v = []
        for i in range(0,K):

            if timer_on:
                timer(i,K-1,start,clear=True)

            # Pick out one line segment (ls)
            t_seg, rate_seg, err_seg, N_gamma, R_seg, T_seg = lc.extract_seg(self.m,n=i,to_print=False,to_plot=False)

            if save_all:
                rate_seg_v.append(rate_seg)
                err_seg_v.append(err_seg)
                R_seg_v.append(R_seg)
            
            # Check gaps:mu
            ## Old version: if abs(dt*m - T_seg) &lt; 1e1: 
            perc_temp = percentage_of_filled_time_bins(t_seg,dt,to_return=True)
            if timer_on:
                print(&#39;Percentage of filled time bins (segment {}): {:.2f}&#39;.format(i,perc_temp))

            if perc_temp &gt; percentage_limit: # if True, then there is no large gap in the segment
                # Perform FFT to find Power spectra for one seg
                xf, fft_rate_normalized, P_noise = self.fft_seg(dt,t_seg,rate_seg,err_seg,N_gamma,R_seg,noise=noise,normalization=normalization,return_noise_wo_sub=return_noise_wo_sub)
                fft_rate_v.append(fft_rate_normalized)
                P_noise_v.append(P_noise)
            else:
                perc_temp_v.append([i,perc_temp])
                num_discarded_segments += 1

        print(&#39;{} of {} segments were disregarded due to lower percentage limit set to {:.2f}%:&#39;.format(num_discarded_segments,K,percentage_limit))
        print(&#39;\n&#39;.join(&#39;Seg nr = {}, Percentage of filled time bins = {:.2f}&#39;.format(k[1][0],k[1][1]) for k in enumerate(perc_temp_v)))
        print(&#39;Power spectra done! \n&#39;)

        if save_all:
            setattr(self, &#39;rate_seg&#39;, rate_seg_v)
            setattr(self, &#39;err_seg&#39;, err_seg_v)
            setattr(self, &#39;R_seg&#39;, R_seg_v)
        
        fft_rate_mean = np.mean(fft_rate_v,axis=0)
        return xf, fft_rate_mean, fft_rate_v, P_noise_v

    def fft_seg(self,dt,t_seg,rate_seg,err_seg,N_gamma,R_seg,normalization=&#39;rms&#39;,noise=&#39;Gaussian&#39;,t_d=0,return_noise_wo_sub=False,to_plot=True):
        &#34;&#34;&#34;
        Perform discrete FFT (fast-Fourier transform) on a light curve (lc) segment.

        Parameters: 
        -----------
        dt: float
            Time resolution.
        
        t_seg: np.ndarray
            The segment&#39;s time-vector in seconds.

        rate_seg: np.ndarray
            The segment&#39;s count rate(=flux)-vector in counts/seconds.

        N_gamma: int
            Number of counted photons in the segment.

        R_seg: float
            Mean count rate in the segment.

        normalization: {&#39;rms&#39; (Miyamoto), &#39;abs&#39;, &#39;Leahy&#39;, or &#39;none&#39;}, optional, default: &#39;rms&#39;.
            What normalization to use, see Vaughan(2003, MNRAS 345).

        noise: {&#39;Poisson&#39;,&#39;Gaussian&#39;}, optional, default: &#39;Gaussian&#39;.
            For a light curve with &#34;noise&#34; errors.

        t_d: float (not implemented yet)
            Dead-time of the instrument. 

        Returns:
        ------------
        xf: np.array
            The Fourier frequencies. 

        fft_rate_noise_subtracted: np.array
            Fast fourier transformation of rate_seg. Noise subtraction has been made.
        &#34;&#34;&#34;
        
        # Perform FFT
        xf = np.array(fftfreq(self.m, dt)[1:self.m//2]) #only want positive freq
        fft_rate = fft(rate_seg)[1:self.m//2]

        # Normalize
        fft_rate = self.normalize_power(fft_rate,R_seg,dt,normalization)

        # Remove Noise
        P_noise = self.noise_power(dt,err_seg,R_seg,noise,normalization)
        if not return_noise_wo_sub:
            fft_rate = np.array([x-P_noise for x in fft_rate])

        return xf, fft_rate, P_noise

    def noise_power(self,dt,err_seg,R,noise=&#39;Gaussian&#39;,normalization=&#39;rms&#39;,B_noise=0,t_d=0):
        &#34;&#34;&#34;
        Calculates and returns the Poisson noise power. See Appendix A of Vaughan (2003, MNRAS 345). Does not take dead time into account.

        Parameters:
        -----------
        
        R: float
            Mean count rate in the segment.

        noise: {&#39;Poisson&#39;,&#39;Gaussian&#39;}, optional, default: &#39;Gaussian&#39;.
            For a light curve with &#34;noise&#34; errors.

        normalization: {&#39;rms&#39; (Miyamoto), &#39;abs&#39;, &#39;Leahy&#39;, or &#39;none&#39;}, optional, default: &#39;rms&#39;.
            What normalization to use, see Vaughan(2003, MNRAS 345).

        B_noise: float, optional, default: 0
            Background noise level to be used in the formula for the noise, see Eq. (A2) of Vaughan (2003, MNRAS 345).
            B_noise = 2 means that B=np.sqrt(R).

        t_d: float (not implemented)
            Dead time of instrument in seconds. 

        Returns:
        --------
        P_noise: float
            Poisson noise power.
        &#34;&#34;&#34;

        dT_samp_over_dT_bin = 1
        
        if noise == &#39;Poisson&#39;: 
            if normalization == &#39;rms&#39;:

                if B_noise &lt; 1:
                    B=B_noise*R #B_noise=0.1 works for &#34;first&#34; CYG X-1 data
                elif B_noise == 2:
                    B=np.sqrt(R)

                P_noise = 2*(R+B)/R**2*dT_samp_over_dT_bin
            elif normalization == &#39;Leahy&#39;: 
                P_noise = 2*(R+B)/R*dT_samp_over_dT_bin #should simply be =2 in most cases
            elif normalization == &#39;abs&#39;: 
                P_noise = 2*(R+B)/R**2*dT_samp_over_dT_bin
            else: #Poisson noise but w/o normalization
                P_noise = (R+B)*self.m/dt* dT_samp_over_dT_bin
        
        elif noise == &#39;Gaussian&#39;:
            if normalization == &#39;rms&#39;:
                P_noise = dT_samp_over_dT_bin*np.mean(err_seg**2)/(R**2*1/(2*dt))
            elif normalization == &#39;Leahy&#39; or normalization == &#39;abs&#39;:
                print(&#39;Sorry, cannot perform that normalization! Not implemented...!&#39;)
            else: #Gaussian noise but w/o normalization
                P_noise = np.mean(np.abs(err_seg)**2)*self.m
        
        else: #noise == &#39;None&#39;:
            P_noise = 0
            
        return P_noise

    def normalize_power(self,fft_rate,R,dt,normalization=&#39;rms&#39;):
        &#34;&#34;&#34;
        Normalize the power spectra. 

        Parameters:
        -----------
        fft_rate: np.array
            Fast fourier transformation of light curve. To be normalized.

        R: float
            Mean count rate in the segment.

        dt: float
            Time resoltuion in observation.

        normalization: {&#39;rms&#39; (Miyamoto), &#39;abs&#39;, &#39;Leahy&#39;, or &#39;none&#39;}, optional, default: &#39;rms&#39;.
            What normalization to use, see Vaughan(2003, MNRAS 345).

        Returns:
        ------------
        normalized_power: np.ndarray
            The normalized power. 
        &#34;&#34;&#34;
        
        if normalization == &#39;rms&#39;:
            return np.array([2*dt/(R**2*self.m)*np.abs(x)**2 for x in fft_rate])
        elif normalization == &#39;Leahy&#39;:
            return np.array([2*dt/(R*self.m)*np.abs(x)**2 for x in fft_rate])    
        elif normalization == &#39;abs&#39;:      
            return np.array([2*dt/self.m*np.abs(x)**2 for x in fft_rate])    
        else: # no-normalization
            return np.array(fft_rate)
    
    def Fvar_from_ps(self):
        &#34;&#34;&#34;
        Calculate the fractional root mean square (rms) variability amplitude by 
        integrating the power spectra over the full frequency interval.
        
        If wants the Fvar in a smaller freq-band, first use remove_freq() to set power to 
        zero outside given freq band and then use Fvar_from_ps.

        Returns:
        ------------
        F_var: float
            The fractional root mean square (rms) variability amplitude.
        &#34;&#34;&#34;

        return Fvar_from_ps(self.xf,self.fft_rate)

    def rebin(self, num=50, init=False):
        &#34;&#34;&#34;
        Logarithmic rebinning.
        
        Parameters:
        -----------
        num: int
            The number of logarithmic frequency bins to create.
        
        init: boolean, optional, default: False
            If False, creates the &#34;middle_of_log_bins, fPf, fPferror, Pf, Pferror&#34; for the first time with num=50 bins.
            If True, updates these attributes, using num bins.
            
        Returns:
        --------
        middle_of_log_bins, fPf, fPferror, Pf, Pferror: np.ndarrays
            See class documentation. 
        &#34;&#34;&#34;
        
        middle_of_log_bins, fPf, fPferror = log_rebin(self.xf,self.fft_rate*self.xf,num=num)
        middle_of_log_bins, Pf, Pferror = log_rebin(self.xf,self.fft_rate,num=num)
        
        if init:
            return middle_of_log_bins, fPf, fPferror, Pf, Pferror
        else:
            setattr(self, &#39;middle_of_log_bins&#39;, middle_of_log_bins)
            setattr(self, &#39;fPf&#39;, fPf)
            setattr(self, &#39;fPferror&#39;, fPferror)
            setattr(self, &#39;Pf&#39;, Pf)
            setattr(self, &#39;Pferror&#39;, Pferror)
    
    def plot(self,to_plot=&#39;fPf&#39;,w=4,with_noise=False):
        &#34;&#34;&#34;
        Plot Power Spectra times Freq vs Freq. (fPf) or Power Spectra vs Freq. (Pf).
        
        Parameters:
        -----------
        to_plot: {&#39;fPf&#39;,&#39;Pf&#39;}   
            What to plot.
        &#34;&#34;&#34;
        
        ax = standard_plot(w=w)
        if to_plot == &#39;fPf&#39;:
            plt.loglog(self.middle_of_log_bins,self.fPf,&#39;.-&#39;,label=&#39;Channels: {}&#39;.format(self.channels),markersize=7)
            plt.ylabel(&#39;$f \cdot P_f$&#39;) # P_f in [(RMS/Average)$^2$/Hz]
            if with_noise:
                plt.loglog(self.xf,self.xf*self.averagePnoise,label=&#39;Average Noise Power&#39;,color=&#39;k&#39;)
        elif to_plot == &#39;Pf&#39;:
            plt.loglog(self.middle_of_log_bins,self.Pf,&#39;.-&#39;,label=&#39;Channels: {}&#39;.format(self.channels),markersize=7)
            plt.ylabel(&#39;$P_f$&#39;) 
            if with_noise:
                ax.axhline(self.averagePnoise,label=&#39;Average Noise Power&#39;,color=&#39;k&#39;)
        plt.xlabel(&#39;Hz&#39;)
        plt.legend()
        plt.show()


class lightcurve:
    &#34;&#34;&#34;
    Make a light curve object from fits-file.
    
    Parameters:
    -----------
    filename: string
        The path to the .fits-file.
        
    keywords: list of strings
        Keywords (and corresponding values) apart from the ones mentioned below to return. 
        *All data-keys are always automatically returned (for a lightcurve, these are: {&#34;TIME&#34;, &#34;RATE&#34;, &#34;ERROR&#34;}).
        *Note also that the header-keys {&#34;CONTENT&#34;,&#34;OBJECT&#34;,&#34;CPIX&#34;,&#34;MINCHAN&#34;,&#34;MAXCHAN&#34;} are returned if they exist. 
    
    p: boolean
        If True, print the headers of the fits-file.
        
    Attributes:
    -----------
    t: np.ndarray 
        Time vector (in seconds).
        
    rate: np.ndarray
        Rate vector (in counts/s).
        
    err: np.ndarray
        Error vector in rate (in counts/s).
    
    object: string
        Object for which we have constructed the lightcurve object.
        
    dt: float
        Time resolution (in seconds).
        
    R: np.float
        Mean count rate.
        
    N: int
        Number of data points in observation.
    
    Fvar: float
        Fractional variance amplitude (=rms) computed from the light curve.
        
    channels: string
        Channels used during the observation.
    
    minchan, maxchan: ints
        Min and max channels used.
    
    # After having called lc.energies(channel_to_kev): 
    
    deltaE: float
        Energy range between min and max channels used.
    
    E_mean: float
        Energy mean over the range between min and max channels used.
    
    &#34;&#34;&#34;
      
    def __init__(self, filename, keywords=[], p=0):
        if isinstance(filename,str):
            self.t, self.rate, self.err, self.object, self.channels, self.minchan, self.maxchan = self.extract_lc(filename, keywords, p)
            self.dt = self.t[1]-self.t[0] # should not (but could) be a gap here... 
            self.R = np.mean(self.rate)
            self.N = len(self.t)  
            
            self.Fvar = self.Fvar_from_lc(m=self.N//100) #use 100 segments as default; can be changed.
            
    def extract_lc(self, filename, keywords, p):
        data = extract_fits(filename, keywords, p=p)
        assert data[&#39;CONTENT&#39;]==&#39;LIGHT CURVE&#39;, &#39;Data does not come from a light curve object.&#39;

        t = np.array(data[&#39;TIME&#39;])
        rate = np.array(data[&#39;RATE&#39;])
        err = np.array(data[&#39;ERROR&#39;])
        obj = data[&#39;OBJECT&#39;]
        channels = data[&#39;CPIX&#39;]
        minchan = data[&#39;MINCHAN&#39;]
        maxchan = data[&#39;MAXCHAN&#39;]
        
        if len(data) &gt; 8:
            keys = [key for key,val in data.items()]   
            for i in range(7,len(data)):
                setattr(self, keys[i], data[keys[i]])
        
        return t, rate, err, obj, channels, minchan, maxchan
    
    def plot(self,m=0,n=0):
        &#34;&#34;&#34;
        Display the full lightcurve.

        Parameters:
        -----------
        m: int, optional, default = 0
            Number of time bins per segment.

        n: int, optional, default: 0 
            The n:th segment will be displayed in the full light curve.
        &#34;&#34;&#34;
        
        # Parameters 
        if m!=0:
            K = int(np.floor(self.N/m))
            print(r&#34;Number of line segments of \approx {:.0f}s will be: &#34;.format((self.t[1]-self.t[0])*m),K)
        else: 
            K = 1

        # Plot 
        ax = standard_plot()
        #plt.errorbar(t_seg,rate_seg,err_seg,fmt=&#39;o&#39;,markersize=2)
        plt.plot(self.t,self.rate,&#39;-&#39;)
        ax.axhline(self.R,color=&#39;r&#39;,label=&#39;R = {:.0f}&#39;.format(self.R))
        if m &gt; 0:
            ax.axvline(self.t[n*m],color=&#39;k&#39;)
            ax.axvline(self.t[(n+1)*m],color=&#39;k&#39;)
            ax.axvspan(self.t[n*m], self.t[(n+1)*m], alpha=0.4, color=&#39;k&#39;,label=&#39;Line segment&#39;)

        plt.legend(loc=&#39;best&#39;)
        plt.xlabel(&#39;Time [$s$]&#39;)
        plt.ylabel(&#39;Rate [$c/s$]&#39;)
        plt.title(self.object.replace(&#34;_&#34;,&#34; &#34;))
        plt.show()
        
    def extract_seg(self,m,n=0,to_print=False,to_plot=False):
        &#34;&#34;&#34;
        Extract (and potentially display) a light curve segment.

        Parameters:
        -----------
        m: int
            Number of time bins per segment. 

        n: int, optional, default: 0 
            The start (end) of the segment will be bin number m*n (m*(n+1)) 

        Returns: 
        --------
        t_seg, rate_seg, err_seg: np.ndarrays
            Time, rate, and error vectors for the segment.

        N_gamma, R, T: floats
            Number of photons (in counts), mean count rate (in counts/s) and length (in seconds) respectively. 
        &#34;&#34;&#34;

        # Pick out segment
        start = m*n
        stop = m*(n+1)
        
        t_seg = self.t[start:stop]
        rate_seg = self.rate[start:stop]
        err_seg = self.err[start:stop]

        # Relevant parameters
        T = t_seg[-1]-t_seg[0]
        R = np.mean(rate_seg)
        N_gamma = T*R

        # Subtract mean rate 
        #rate_seg = [x-R for x in rate_seg]

        # Print
        if to_print:
            print(&#39;R = average count rate = &#39;,R)
            print(&#39;N_\gamma = total counts = &#39;,N_gamma)
            print(&#39;T = total time = &#39;,T) 
            print(&#39;128s corresponds to {} time-elements&#39;.format(int(128/(t_seg[1]-t_seg[0]))))

        # Plot
        if to_plot:
            ax = standard_plot()
            #plt.errorbar(t_seg,rate_seg,err_seg,fmt=&#39;o&#39;,markersize=2)
            plt.plot(t_seg,rate_seg,&#39;-&#39;)
            ax.axhline(np.mean(rate_seg),color=&#39;r&#39;,label=&#39;R = {}&#39;.format(R))
            plt.legend(loc=&#39;upper right&#39;)
            plt.xlabel(&#39;Time [$s$]&#39;)
            plt.ylabel(&#39;Rate [$c/s$]&#39;)
            plt.title(&#39;CygX1 - light curve segment {}&#39;.format(n+1))
            plt.show()
        else:
            return t_seg, rate_seg, err_seg, N_gamma, R, T
        
    def energies(self, channel_to_kev):
        &#34;&#34;&#34;
        Channel to corresponding energy.
        
        Parameters:
        -----------
        channel_to_kev: np.ndarray
            Conversion from channel (index) to energy (keV).
        
        &#34;&#34;&#34;
        
        minchan = self.minchan
        if minchan != 0:
            minchan -= 1
        minene = channel_to_kev[minchan]
        if minchan == 0:
            minene = 0
        maxchan = self.maxchan
        maxene = channel_to_kev[maxchan]
        print(&#39;Energy channels (keV): &#39;,self.minchan,&#39;-&#39;,self.maxchan,&#39;(&#39;,minene,&#39;-&#39;,maxene,&#39;)&#39;)
        
        setattr(self, &#39;deltaE&#39;, maxene-minene)
        setattr(self, &#39;E_mean&#39;, (maxene+minene)/2)
        # setattr(self, &#39;E_err&#39;, (maxene-minene)/2) = deltaE/2
        
    def Fvar_from_lc(self,m,percentage_limit=0):
        &#34;&#34;&#34;
        Calculate the fractional root mean square (rms) variability amplitude from 
        the variance (S^2) in the light curve, by averaging over K=int(np.floor(lc.N/m)) segments. 
        &#34;&#34;&#34;

        return Fvar_from_lc(self,m,percentage_limit)
    

def cross_spec(data,m,percentage_limit=90,noise=None,return_noise_wo_sub=False):
    &#34;&#34;&#34;
    Computes the cross spectrum.
    
    Parameters:
    ----------- 
    data: Either lc (1) or ps (2).
        (1) class: list of two &#39;Lightcurve&#39;-objects
                The light curves to be used in the coherence computation.
        (2) class: list of two &#39;PowerSpectrum&#39;-objects
                The power spectras to be used in the coherence computation.
        
    m: int 
        Number of time bins per segment.
        
    percentage_limit: float, optional, default: 90
            Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.
    
    noise: {&#39;Poisson&#39;,&#39;Gaussian&#39;}, optional, default: None.
        For a light curve with &#39;Poisson&#39;/&#39;Gaussian&#39; errors. 
        
    return_noise_wo_sub: boolean, optional, default: False
        If True, noise is returned but not subtracted from powspec.
        If False, returned and subtracted.
    
    Returns:
    --------
    C_v: np.ndarray
        The cross-spectrum (see e.g. section 3 of Epitropakis, A. (2017)) after averaging over K segments.
        
    S1_v, S2_v: np.ndarrays
        The discrete Fourier transform of the two signals.
    &#34;&#34;&#34;
    
    print(&#39;Computing the cross spectrum...\n&#39;)

    if isinstance(data[0], lightcurve) and isinstance(data[1], lightcurve):
    
        # Segment-wise: 
        ps_v = [PowerSpectrum(lc, m=m, normalization=None, noise=noise, percentage_limit=percentage_limit,                               timer_on=False, return_noise_wo_sub=return_noise_wo_sub) for lc in data]

    if isinstance(data[0], PowerSpectrum) and isinstance(data[1], PowerSpectrum):
        ps_v = data
        
    S_v = np.array([ps_v[0].fft_rate_v,ps_v[1].fft_rate_v])
    C_v = np.array([np.conjugate(S1)*S2 for S1,S2 in zip(S_v[0],S_v[1])])

    print(&#39;Cross spectrum computed.&#39;)    
    
    return ps_v, C_v

def coherence_noiseless(data,percentage_limit=90,m=2**13,C_v=None):
    &#34;&#34;&#34;
    The coherence function (Cf) when noise is not present in the signals.
    
    Cf is a Fourier-frequencydependent measure of the degree of linear correlation 
    between two concurrent time series. Specifically, it gives the fraction of the mean-squared variability 
    at f of one time series that can be attributed to, or equivalently predicted from, the other (Nowak,1999).
    
    Parameters:
    ----------- 
    data: Either lc (1) or ps (2).
        (1) class: list of two &#39;Lightcurve&#39;-objects
                The light curves to be used in the coherence computation.
        (2) class: list of two &#39;PowerSpectrum&#39;-objects
                The power spectras to be used in the coherence computation.
        
    m: int 
        Number of time bins per segment.
    
    C_v: np.ndarray
        If the cross spectrum has already been found, it can be used. In this case, we require data to be a list of PowerSpectrum-object.
    
    Returns:
    --------
    xf: np.ndarray
        The frequency-vector (with the current binning).
        
    gamma2: np.ndarray
        The coherence function.
    
    delta_gamma2: np.ndarray 
        One sigma uncertainty in the coherence function. 
    &#34;&#34;&#34;

    print(&#39;Computing the coherence...&#39;)
    
    # Find the Cross Spectrum
    if not isinstance(C_v, np.ndarray):
        ps_v, C_v = cross_spec(data, m=m, percentage_limit=percentage_limit)
        S_v = np.array([ps_v[0].fft_rate_v,ps_v[1].fft_rate_v])
        xf = ps_v[0].xf
    
    # Have already found the Cross Spectrum
    elif isinstance(C_v, np.ndarray):
        print(&#39;Cross spectra already found.&#39;)
        if isinstance(data[0], PowerSpectrum) and isinstance(data[1], PowerSpectrum):
            xf = data[0].xf
            S_v = np.array([data[0].fft_rate_v,data[1].fft_rate_v])
        else:
            print(&#39;If you provide me with the cross spectra C_v, the input data need to be a list of PowerSpectrum-objects.&#39;)
    
    # The Periodograms w/o normalization
    P1_v = [np.abs(x)**2 for x in S_v[0]]
    P2_v = [np.abs(x)**2 for x in S_v[1]]

    # The Coherence and its error
    K = np.size(C_v,axis=0) #number of segments
    gamma2 = abs(np.mean(C_v,axis=0))**2/(np.mean(P1_v,axis=0)*np.mean(P2_v,axis=0))
    delta_gamma2 = np.sqrt(2)*(1-gamma2)/(np.sqrt(abs(gamma2))*np.sqrt(K))
    
    print(&#39;Coherence computed. \n&#39;)
    
    return xf, gamma2, delta_gamma2, C_v

# See Appendix A of (Epitropakis,2017)

def compute_coherence_intrinsic(lcs,m,noise,percentage_limit,output=False):
    &#34;&#34;&#34;
    Computes the intrinsic coherence for a given number of bins per segment (m).
    
    Parameters:
    ----------- 
    lcs: class: list of two &#39;Lightcurve&#39;-objects
                The light curves to be used in the coherence computation.
        
    m: int
        Number of bins per segment.
    
    noise: {&#39;Poisson&#39;,&#39;Gaussian&#39;}.
        For a light curve with &#39;Poisson&#39;/&#39;Gaussian&#39; errors. 
    
    percentage_limit: float, optional, default: 90
            Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.
    
    output: boolean
        If True, plot the whole light curve as well as the &#34;conditions met&#34;-figures. Also, helpful statements are printed.
    
    Returns:
    ------------
    xf: np.ndarray
        The frequency-vector (which depends on m).
        
    gamma2: np.ndarray
        The coherence function.
    
    delta_gamma2: np.ndarray 
        One sigma uncertainty in the coherence function. 
        
    C_mean: np.ndarray
        The cross spectra, can be used for time lag estimations.
    &#34;&#34;&#34;
    
    # Compute power spectra, cross spectra, and signal powers
    ps_v, C_v = cross_spec(lcs,m,percentage_limit,noise=noise,return_noise_wo_sub=True)
    S_v = np.array([ps_v[0].fft_rate_v,ps_v[1].fft_rate_v])

    # Need to extract freq.vector (same for ps0 and ps1), number of segments K and mean cross spectra
    xf = ps_v[0].xf 
    K = len(C_v)
    C2_mean = np.array(np.abs(np.mean(C_v,axis=0))**2 )
    
    # Compute average for all power terms over all segments and then calculate n2 once
    P1_mean = np.mean(np.abs(S_v[0])**2,axis=0) #power of signal: |S|^2
    P2_mean = np.mean(np.abs(S_v[1])**2,axis=0)
    N1_mean = np.mean(ps_v[0].Pnoise_v,axis=0) #power of noise
    N2_mean = np.mean(ps_v[1].Pnoise_v,axis=0)
    n2 = (P1_mean*N2_mean + P2_mean*N1_mean - N1_mean*N2_mean)/(K)
    
    # Compute coherence
    gamma2 = comp_gamma2(C2_mean,n2,P1_mean,N1_mean,P2_mean,N2_mean)
    
    # Compute coherence error
    delta_gamma2_int = compute_delta_gamma2_int(gamma2,C2_mean,n2,P1_mean,N1_mean,P2_mean,N2_mean,K)
    
    # Check for what frequencies the intrinsic coherence can be usefully estimaed.
    upper_freq_lim = conditions(xf,gamma2,C2_mean,P1_mean,P2_mean,N1_mean,N2_mean,n2,K,output)
    
    return xf, gamma2, delta_gamma2_int, upper_freq_lim
    
def comp_gamma2(C2,n2,P1,N1,P2,N2):    
    &#34;&#34;&#34;
    Compute gamma2. See: Epitropakis, A. (2017), Eq. (A1). Compare with Eq. (3) (the noiseless case).
    &#34;&#34;&#34;
    
    return (C2-n2)/((P1-N1)*(P2-N2))
    
    
def compute_delta_gamma2_int(gamma2,C2,n2,P1,N1,P2,N2,K):
    &#34;&#34;&#34;
    Compute the error delta_gamma2_int. See: Epitropakis, A. (2017), Eq. (A3)
    &#34;&#34;&#34;
    
    delta_gamma2 = np.sqrt(2/K)*(1-gamma2)/(np.sqrt(abs(gamma2)))
    term1 = 2*n2**2*K/(C2-n2)**2
    term2 = N1**2/(P1-N1)**2
    term3 = N2**2/(P2-N2)**2
    term4 = K*delta_gamma2**2/gamma2**2
    
    return gamma2/np.sqrt(K)*(term1+term2+term3+term4)**(1/2)
    
    
def coherence_intrinsic(lcs,m_init,noise=&#39;Gaussian&#39;,return_jointly=True,percentage_limit=90,output=False):
    &#34;&#34;&#34;
    The coherence function (Cf) when noise is present in the signals. The Cf is computed at least
    twice, once for m (number of bins per segment) being very high to cover low frequencies and 
    once for m very small (256 as smallest) to cover high frequencies. 
    
    Parameters:
    -----------
    lcs: class: list of two &#39;Lightcurve&#39;-objects
                The light curves to be used in the coherence computation.
        
    m_init: int 
        Number of bins per segment to start with. m_init will be lowered during each interation.
        The higher m, the less number of segments. The fewer segments, the lower frequencies will 
            be considered, as f_min = 1/(m_init*dt).
       
    noise: {&#39;Poisson&#39;,&#39;Gaussian&#39;}, optional, default: &#39;Gaussian&#39;.
        For a light curve with &#39;Poisson&#39;/&#39;Gaussian&#39; errors. 
       
    return_jointly: boolean
        If True, the coherence computed for different m:s are merged. 
        If False, the coherence is returned as a list of np.ndarrays, so that the computation for different m:s can be compared.
        
    percentage_limit: float, optional, default: 90
            Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.
    
    output: boolean
        If True, plot the whole light curve as well as the &#34;conditions met&#34;-figures. Also, helpful statements are printed.
    
    Returns:
    --------
    xf_v: np.ndarray (or a list of np.ndarrays if return_jointly is False)
        The frequency-vector.
        
    gamma2_v: np.ndarray (or a list of np.ndarrays if return_jointly is False)
        The coherence function.
    
    delta_gamma2_v: np.ndarray (or a list of np.ndarrays if return_jointly is False)
        One sigma uncertainty in the coherence function. 
    &#34;&#34;&#34;
    
    assert isinstance(lcs[0], lightcurve) and isinstance(lcs[1], lightcurve), &#39;The data-input does not contain two light curve objects.&#39;
    assert len(lcs)==2, &#39;Coherence should be computed with 2 light curves, not {}&#39;.format(len(lcs))
    
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
    print(&#39;                           Computing the intrinsic coherence...&#39;)
    print(&#39;---------------------------------------------------------------------------------------------------\n&#39;)
    start = timeit.default_timer()
    
    N = m_init
    k = int(np.log2(N))
    dt = lcs[0].dt
    
    find_again = True
    xf_v, gamma2_v, delta_gamma2_int_v, upper_freq_lim_v = [], [], [], []
    
    i=0
    while find_again:
        i+=1
        print(&#39;Iteration {}) Computing using m = {} bins per segment, i.e. f in [{:.3f},{:.3f}]&#39;.format(i,N,1/(N*dt),1/(2*dt)))
        print(&#39;---------------------------------------------------------------------------------------------------&#39;)
        xf, gamma2, delta_gamma2_int, upper_freq_lim = compute_coherence_intrinsic(lcs,N,noise,percentage_limit,output=output)
    
        if return_jointly: 
            # ----- Don&#39;t use the full freq band found ------------------------------------------------
            # Only use freq up to multi_factor*upper_freq_lim, i.e. slightly under uppper freq lim.
            if k!=7:
                multi_factor = 0.9
                if upper_freq_lim != -1:
                    lim = upper_freq_lim*multi_factor
                else:
                    lim = xf[-1]
            else:
                lim = xf[-1]
                
            gamma2 = gamma2[xf &lt; lim]
            delta_gamma2_int = delta_gamma2_int[xf &lt; lim]
            xf = xf[xf &lt; lim]
            # -----------------------------------------------------------------------------------------            
        
            # Append to lists
            xf_v = np.append(xf_v,xf)
            gamma2_v = np.append(gamma2_v,gamma2) 
            delta_gamma2_int_v = np.append(delta_gamma2_int_v,delta_gamma2_int) 
            upper_freq_lim_v = np.append(upper_freq_lim_v,upper_freq_lim)
        
        # If not return_jointly; can see what freq-range the different m caught.
        else:
            # Rebin directly; only want to get a feeling for the different m
            num = 50
            lim = xf[-1]
            xf,gamma2,delta_gamma2_int = log_rebin(xf,gamma2,delta_gamma2_int,num=num)
            delta_gamma2_int = error_change(delta_gamma2_int)
            
            # Append to lists
            xf_v.append(xf)
            gamma2_v.append(gamma2) 
            delta_gamma2_int_v.append(delta_gamma2_int) 
            upper_freq_lim_v.append(upper_freq_lim)
    
        print(&#39;Intrinsic coherence found for f in [{:.3f},{:.3f}] \n&#39;.format(1/(N*dt),lim))
        
        # Change number of bins / segment until next time
        if k == 8:
            find_again = False
        else:
            multi_factor = 10 #discuss what this is... 
            while 2**(k-1) &gt; multi_factor/(dt*upper_freq_lim):
                k -= 1
                if k == 8:
                    break 
            N = 2**k
    
    time_taken = timeit.default_timer()-start
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
    print(&#39;             Intrinsic coherence found (in {:.2f} sec). return_jointly = {}&#39;.format(time_taken,return_jointly))
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
    
    return xf_v, gamma2_v, delta_gamma2_int_v


def conditions(xf,gamma2,C2,P1,P2,N1,N2,n2,K,output):
    &#34;&#34;&#34;
    The intrinsic coherence can be usefully estimated when the following conditions are met: 
    1) sqrt(C2) &gt; sqrt(n2)
    2) |S_1|^2/|N_1|^2 &gt; 1/\sqrt{m}
    3) |S_2|^2/|N_2|^2 &gt; 1/\sqrt{m}
    &#34;&#34;&#34;
    
    upper_freq_lim = []
    
    C = np.sqrt(np.abs(C2))
    n = np.sqrt(np.abs(n2))
    
    if output:
        standard_plot(h=8)
        plt.subplot(3,1,1)

        plt.loglog(xf,C,label=r&#34;$|&lt;C(f)&gt;|^2$&#34;)
        plt.loglog(xf,n,label=r&#34;$n^2$&#34;)
    
    try:
        # Condition 1
        idx = np.argwhere(np.diff(np.sign(C-n))).flatten()
        upper_freq_lim.append(xf[idx[0]])
        if output:
            plt.plot(upper_freq_lim[-1], n[idx[0]], &#39;ro&#39;,label=str(xf[idx[0]]))
    except:
        pass
    
    if output:
        plt.legend()
        plt.title(&#39;Conditions to be met for useful estimation of $\gamma_I^2$&#39;)

        plt.subplot(3,2,3)
        plt.title(r&#39;High Power (Term 2 of $\delta \gamma^2_{int}$)&#39;)
        plt.loglog(xf,(P1-N1),label=r&#39;$|S_1|^2$&#39;)
        plt.loglog(xf,N1*np.ones(np.size(xf))/np.sqrt(K),label=r&#39;$N_1/\sqrt{m}$&#39;)
    
    try:
        # Condition 2
        temp = N1*np.ones(np.size(xf))/np.sqrt(K)
        idx = np.argwhere(np.diff(np.sign(temp-(P1-N1)))).flatten()
        upper_freq_lim.append(xf[idx[0]])
        
        if output:
            plt.plot(upper_freq_lim[-1], temp[idx[0]], &#39;ro&#39;,label=str(xf[idx[0]]))
    except:
        pass
    
    if output:
        plt.legend()

        plt.subplot(3,2,4)
        plt.title(r&#39;High Power (Term 3 of $\delta \gamma^2_{int}$)&#39;)
        plt.loglog(xf,(P2-N2),label=r&#39;$|S_2|^2$&#39;)
        plt.loglog(xf,N2*np.ones(np.size(xf))/np.sqrt(K),label=r&#39;$N_2/\sqrt{m}$&#39;)
        
    try:
        # Condition 3
        temp = N2*np.ones(np.size(xf))/np.sqrt(K)
        idx = np.argwhere(np.diff(np.sign(temp-(P2-N2)))).flatten()
        upper_freq_lim.append(xf[idx[0]])
        if output:
            plt.plot(upper_freq_lim[-1], temp[idx[0]], &#39;ro&#39;,label=str(xf[idx[0]]))
    except:
        pass
    
    if output:
        plt.legend()

        plt.subplot(3,1,3)
        plt.title(&#39;High Coherence&#39;)
        plt.loglog(xf,gamma2,label=r&#39;$\gamma^2_{int}$&#39;)
        plt.loglog(xf,n**2/(P1*P2),label=r&#39;$n^2/(P_1P_2)$&#39;)
    try:
        idx = np.argwhere(np.diff(np.sign(gamma2-n**2/(P1*P2)))).flatten()
        upper_freq_lim.append(xf[idx[0]])
        if output:
            plt.plot(upper_freq_lim[-1], gamma2[idx[0]], &#39;ro&#39;,label=str(xf[idx[0]]))
    except:
        pass
    
    if output:
        plt.legend()
        plt.tight_layout()
        plt.show()
    
    if len(upper_freq_lim) == 0:
        return -1
    else:
        return np.amin(upper_freq_lim)
    
def plot_coherence(xf,gamma2,delta_gamma2_int,err_lim=1,num=75,save_fig = False):
    &#34;&#34;&#34;
    Plot the coherence.
    
    Parameters:
    -----------
    xf: np.ndarray
        The frequency-vector (which depends on m).
        
    gamma2: np.ndarray
        The coherence function.
    
    delta_gamma2: np.ndarray 
        One sigma uncertainty in the coherence function. 
        
    err_lim: float, optional, default: 1
        Error limit. If an error element is larger than err_lim, it is set to zero. 
    
    num: int, optional, default: 75
        The number of logarithmic frequency bins to create before plotting.
        
    save_fig: boolean, optional, default: False
         If True, asks for path to location to save plot.
    &#34;&#34;&#34;
    
    standard_plot(h=4,w=6)
    ax = plt.gca()
        
    first_band = input(&#39;First energyband (write in keV as &#34;?-?&#34;)&#39;)
    second_band = input(&#39;Second energyband (write in keV as &#34;?-?&#34;)&#39;)
    ax.text(0.3,0.1,&#39;({} keV) vs. ({} keV)&#39;.format(first_band,second_band),fontsize=14,horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;, transform=ax.transAxes)
    
    if len(xf) &lt;= 3:
        for i in range(0,len(xf)):
            ax.errorbar(xf[i],gamma2[i],yerr=delta_gamma2_int[i], fmt = &#39;.&#39;,mfc=&#39;w&#39;,capsize=2, elinewidth=1, markeredgewidth=1,label=r&#39;$N=2^6$&#39;)
    else:
        xf, gamma2, error = log_rebin(xf,gamma2,delta_gamma2_int,num=num)
        error = error_change(error,err_lim)
        ax.errorbar(xf,gamma2,yerr=error, fmt = &#39;.k&#39;,mfc=&#39;w&#39;,capsize=2, elinewidth=1, markeredgewidth=1,label=r&#39;$N=2^6$&#39;)
    
    ax.set_ylim([0,1.4])
    ax.axhline(1,color=&#39;k&#39;,linewidth=1,alpha=0.7)
    ax.set_xscale(&#34;log&#34;)
    plt.xlabel(&#39;Frequency [Hz]&#39;)

    plt.ylabel(&#39;$\gamma_{int}^2$&#39;)
    if save_fig:
        path = input(&#39;Path to location to save plot [end with .png]: &#39;) 
        plt.savefig(path,bbox_inches=&#39;tight&#39;)
    plt.show()

def standard_plot(h=4,w=10):
    &#34;&#34;&#34;Standard plot to enable use of the same figsize, fontsize and font.family in all figures.
    
    Parameters:
    -----------
    h,w: (float, float), optional, default: h=4, w=10
        Height and width of the figure, i.e., figsize=(w,h).
        
    Returns:
    --------
    ax: Axes,
        Axes of the figure.
    &#34;&#34;&#34;
    
    fig = plt.figure(figsize=(w,h))
    plt.rcParams.update(plt.rcParamsDefault)
    plt.rcParams.update({&#39;font.size&#39;: 16})
    plt.rcParams[&#39;font.family&#39;] = &#39;Times&#39;
    plt.rc(&#39;text&#39;, usetex=True) 
    
    return plt.gca()

def timer(i,nr,start,clear=True):
    &#34;&#34;&#34;
    Prints out the loop progress, and tries to estimate the time remaining.
    
    Parameters:
    -----------
    i, nr: ints
        The current loop, and totalt number of loops.
    
    start: float
        Start of the loop, as given by: timeit.default_timer(). 
    
    clear: boolean
        Clear the output between loops. 
    &#34;&#34;&#34;
    
    stop=timeit.default_timer()
    if (i/nr*100) &lt; 10:
        expected_time=&#34;Calculating...&#34;
    else:
        time_perc=timeit.default_timer()
        expected_time=np.round(((time_perc-start)/(i/nr))/60,2)
    if clear == True:
        clear_output(wait=True)
    
    print(&#34;Calculating the power spectra...&#34;)
    print(&#34;Current progress: {}%&#34;.format(np.round(i/nr*100,2)))
    print(&#34;Current run time: &#34;,np.round((stop-start)/60,2),&#34; minutes&#34;)
    print(&#34;Expected run time: &#34;,expected_time,&#34; minutes&#34;)
    

def extract_fits(filename,keywords=[],p=True):
    &#34;&#34;&#34; 
    Extract (and print info about) a fits-file. 
    
    Parameters:
    -----------
    filename: string
        The path to the .fits-file.
    
    keywords: list of strings
        Keywords (and corresponding values) apart from the ones mentioned below to return. 
        *All data-keys are always automatically returned (for a lightcurve, these are: {&#34;TIME&#34;, &#34;RATE&#34;, &#34;ERROR&#34;}).
        *Note also that the header-keys {&#34;CONTENT&#34;,&#34;OBJECT&#34;,&#34;CPIX&#34;,&#34;MINCHAN&#34;,&#34;MAXCHAN&#34;} are returned if they exist. 
    
    p: boolean
        If True, print the headers of the fits-file.
        
    Returns:
    --------
    data: dictionary
        Keys and the corresponding data values.
    &#34;&#34;&#34;
    
    print(&#39;Loading fits from filename: &#39;,filename)
    with fits.open(filename) as hdulist: 
    
        # HEADER
        header0 = hdulist[0].header
        header1 = hdulist[1].header
        
        # DATA KEYS
        binaryext = hdulist[1].data
        binarytable = Table(binaryext)
        keys = binarytable.keys()
        
        data = {}
        for key in keys:
            data[key] = hdulist[1].data[key]
        
        # HEADER KEYS
        try:
            data[&#39;CONTENT&#39;] = header0[&#39;CONTENT&#39;]
            content_exist = True
        except KeyError:
            print(&#39;There is no content-information for this fits.&#39;)
            content_exist = False
        try:
            data[&#39;OBJECT&#39;] = header0[&#39;OBJECT&#39;]
        except KeyError:   
            print(&#39;There is no object-information for this fits.&#39;)
        try:
            data[&#39;CPIX&#39;] = header1[&#39;CPIX&#39;]
            data[&#39;MINCHAN&#39;] = header1[&#39;MINCHAN&#39;]
            data[&#39;MAXCHAN&#39;] = header1[&#39;MAXCHAN&#39;]
        except KeyError:   
            print(&#39;There is no CPIX-information for this fits.&#39;)
            
        # Extract extra keyswords
        for extra_key in keywords:
            
            try: 
                data[extra_key] = header0[extra_key]
                print(&#34;Found key {} in header0&#34;.format(extra_key))
                cannot_find = False
            except KeyError:
                print(&#34;There is no key {} in header0, let&#39;s have a look at header1.&#34;.format(extra_key))
            try: 
                data[extra_key] = header1[extra_key]
                print(&#34;Found key {} in header1.&#34;.format(extra_key))
                cannot_find = False
            except KeyError:
                print(&#34;There is no key {} in header1 either, sorry...&#34;.format(extra_key))
                cannot_find = True
            
            if cannot_find: 
                matchingK0 = matchingKeys(header0, extra_key)
                matchingK1 = matchingKeys(header1, extra_key)
                print(&#39;Matching keys in header0 = &#39;,matchingK0)
                print(&#39;Matching keys in header1 = &#39;,matchingK1)
                for K in matchingK0:
                    use = input(&#39;Do you want to extract the header0 key {}? [y/n] &#39;.format(K))
                    if use == &#39;y&#39; or use == &#39;yes&#39; or use == &#39;Y&#39;:
                        data[K] = header0[K]
                for K in matchingK1:
                    use = input(&#39;Do you want to extract the header1 key {}? [y/n] &#39;.format(K))
                    if use == &#39;y&#39; or use == &#39;yes&#39; or use == &#39;Y&#39;:
                        data[K] = header1[K]
            
        if p:
            print(&#39;hdu.info()&#39;)
            print(hdulist.info(),&#39;\n&#39;)
            print(&#39;Header0:&#39;)
            print(repr(header0),&#39;\n&#39;) #repr() prints the info into neat columns
            print(&#39;Header1:&#39;)
            print(repr(header1),&#39;\n&#39;) #repr() prints the info into neat columns
            print(binarytable[0:10],&#39;\n&#39;)
        else:
            if content_exist:
                print(&#39;The keys to the {} data are: {}&#39;.format(data[&#39;CONTENT&#39;],data.keys()))
            else:
                print(&#39;The keys to the data are: {}&#39;.format(data.keys()))
        print(&#39;Loading fits done. \n&#39;)
        
        return data

def matchingKeys(dictionary, searchString):
    return [key for key,val in dictionary.items() if searchString in key]    

def log_rebin(xf,take_average_of,err=[],low_lim=None,high_lim=None,num=50):
    &#34;&#34;&#34;
    Distributes the data into logarithmic frequency bins and computes the bin-average of the data.
    
    Parameters:
    -----------
    xf: np.ndarray 
        The frequency-vector (with the current binning).
    
    take_average_of: np.ndarray 
        The quantity to take average of. 
        
    err: np.ndarray, optional, default: empty list
        Errors corresponding to the &#34;take_average_of&#34;-quantity.  
    
    low_lim, high_lim: floats, optional, default: None
        The interval to bin into log-bins: 10^(low_lim) to 10^(high_lim).
        If None, limits are given by the end points of the frequency vector.
    
    num: int
        The number of logarithmic frequency bins to create.
    
    Returns:
    --------
    middle_of_log_bins: np.ndarray 
        The logarithmic midpoint of each log-bin, computed as: 10**(1/2*(np.log10(log_bins[i])+np.log10(log_bins[i+1])))
        
    average: np.ndarray
        The average of the data within a log-bin.
        
    error: np.ndarray
        The standard deviation of all points within a log-bin. 
    &#34;&#34;&#34;
    
    # Average over the frequencies (logarithmic)
    if low_lim == None:
        low_lim = np.log10(np.amin(xf))
    if high_lim == None:
        high_lim = np.log10(np.amax(xf))
    log_bins = np.logspace(low_lim, high_lim, num=num)
    middle_of_log_bins = []
    for i in range(0,len(log_bins)-1):
        middle_of_log_bins.append(10**(1/2*(np.log10(log_bins[i])+np.log10(log_bins[i+1]))))
    
    # Determine what freq-values correspond to what bin
    digitized = np.digitize(xf, log_bins)
    
    # Sort into bins and make sure no bin is empty 
    middle_of_log_bins = [middle_of_log_bins[i-1] for i in range(1, len(log_bins)) if len(take_average_of[digitized == i])!=0]
    average = [take_average_of[digitized == i].mean() for i in range(1, len(log_bins)) if len(take_average_of[digitized == i])!=0]
    
    # Standard deviation for points in this bin if no error as input:
    if len(err) == 0:
        error = [take_average_of[digitized == i].std() for i in range(1, len(log_bins)) if len(take_average_of[digitized == i])!=0]
    # If error as input, compute error propagation:
    else:
        error = [np.sqrt(np.sum((err[digitized == i])**2))/len(err[digitized == i]) for i in range(1, len(log_bins)) if len(take_average_of[digitized == i])!=0]
        
    return np.array(middle_of_log_bins), np.array(average), np.array(error)

def percentage_of_filled_time_bins(t_seg,dt,to_return=True):
    &#34;&#34;&#34;
    Percentage of time bins being filled, i.e. without gaps.
    
    Parameters:
    ----------- 
    t_seg: np.ndarray
        Segment&#39;s time vector. Should start from zero, i.e. t_seg[0] = 0. 
        
    dt: np.float
        Time resolution of observation.
        
    to_return: boolean (default: False)
        If false, print gap percentage, otherwise return it. 
        
    Returns:
    --------
    perc_wo_gaps: np.float
        Percentage of time bins being filled, i.e. without gaps.
    &#34;&#34;&#34;
    
    t_seg_temp = np.copy(t_seg)
    
    # Linear transformation of segment&#39;s first element to t_seg=0    
    if int(t_seg_temp[0]) != 0:
        t_seg_temp -= t_seg_temp[0]
    
    # Count number of filled (= non-empty = no gap) bins
    num_bins = math.ceil(t_seg_temp[-1]/dt)
    hist, edges = np.histogram(t_seg_temp,bins=num_bins,range=(0, dt*num_bins))
    
    # Percentage of filled bins (i.e. without gap)
    perc_wo_gaps = np.sum(hist)/len(hist)*100
    
    if to_return:
        return perc_wo_gaps 
    else: 
        print(&#39;perc_wo_gaps = {:.4f}&#39;.format(perc_wo_gaps))

def Fvar_from_lc(lc, m, percentage_limit):
    &#34;&#34;&#34;
    Calculate the fractional root mean square (rms) variability amplitude from 
    the variance (S^2) in the light curve, by averaging over K=int(np.floor(lc.N/m)) segments. 
    
    Parameters:
    -----------
    lc: class: &#39;Lightcurve&#39;-object
        The light curve data to be Fourier-transformed.
        
    m: int
        Number of time bins per segment. 

    percentage_limit: float, optional, default: 90
        Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.
        
    Returns:
    --------
    F_var: float
        The fractional root mean square (rms) variability amplitude.
        See Eq. (10) from: &#34;On characterizing the variability properties of X-ray light curves from active galaxies&#34; (Vaughan, 2003)
    &#34;&#34;&#34;
    
    # Useful parameters
    dt = lc.dt
    K = int(np.floor(lc.N/m)) #num_of_line_seg

    # Average over time segments 
    rms_lc_seg_v = []
    for i in range(0,K):

        # Pick out one line segment (ls)
        t_seg, rate_seg, err_seg, N_gamma, R_seg, T_seg = lc.extract_seg(m,n=i,to_print=False,to_plot=False)

        if percentage_of_filled_time_bins(t_seg,dt,to_return=True) &gt; percentage_limit: # if True, then there is no large gap in the segment
            # Perform FFT to find Power spectra for one seg
            S2 = 1/(len(rate_seg)-1)*np.sum((rate_seg-R_seg)**2)
            MSE = np.mean(err_seg**2)
            F_var = np.sqrt((S2-MSE)/R_seg**2)
            rms_lc_seg_v.append(F_var)
        else:
            pass
            
    return np.mean(rms_lc_seg_v)

def Fvar_from_ps(xf,fft_rate):
    &#34;&#34;&#34;
    Calculate the fractional root mean square (rms) variability amplitude by 
    integrating the power spectra over the full frequency interval. 
    
    If want a smaller freq band, call remove_freq() prior to calling this function.
    
    Parameters:
    -----------
    xf, fft_rate: np.ndarrays
        Frequency vector and power spectra.
        
    Returns:
    --------
    F_var: float
        The fractional root mean square (rms) variability amplitude.
    &#34;&#34;&#34;
    
    F_var2 = 0
    df = xf[1]-xf[0]
    for i in range(0,len(fft_rate)):
        F_var2 += df*fft_rate[i]
    F_var = np.sqrt(F_var2)
    
    return F_var

def load_lightcurve(data):
    &#34;&#34;&#34; 
    Split the data from a light curve into time, flux, and error vectors. 
    
    Parameters:
    -----------
    data: dictionary
        Should contain the keys [&#39;TIME&#39;, &#39;RATE&#39;, &#39;ERROR&#39;, &#39;FRACEXP&#39;, &#39;CONTENT&#39;] with corresponding values.
        The data[&#39;CONTENT&#39;] ought to be &#39;LIGHT CURVE&#39;
        
    Returns:
    --------
    t, rate, error: np.ndarrays
        The data values for a light curve: time, rate, rate_error, respectively.
    &#34;&#34;&#34;

    assert data[&#39;CONTENT&#39;]==&#39;LIGHT CURVE&#39;, &#39;Data does not come from a light curve object.&#39;

    t = np.array(data[&#39;TIME&#39;])
    rate = np.array(data[&#39;RATE&#39;])
    err = np.array(data[&#39;ERROR&#39;])
    
    return t, rate, err

def remove_freq(xf,other_quantites,limit,leq=False,l=False,geq=False,g=False,disregard=True):
    &#34;&#34;&#34;
    Remove frequencies (f) and other quantites&#39; (OQ) corresponding values for those f. 
    Example: limit = 10 and leq = True: means that only f &lt;= 10 are saved.
    
    Parameters:
    -----------
    xf: np.ndarray
        
    other_quantites: list of np.ndarrays
    
    limit: float
        The numerical value of the limit. The type of limit is determined next
    
    leq, l, geq, g: Boolean (default: False)
        less or equal than (leq), less than (l), greater or equal to (geq), greater than (g) 
        the limit will be SAVED.
        
    disregard: Boolean (default: False)
        If True, then the frequency and OQ will be cropped and returned as shorter vectors.
        If False, returned in the same length with OQs&#39; elements outside given range being set to 0.

    Returns:
    --------
    freq, : np.ndarray
        The frequency vector. 
        
    other_quantites: list of np.ndarrays or np.ndarray (if just one quantity)
        After disregarding values (or setting them to zero) corresponding to frequencies outside the desired interval. 
    &#34;&#34;&#34;
    
    assert type(limit) != list, &#39;Unfortunately, you cannot fix more than one limit at once.&#39;
    
    try: 
        return_as_nparray = False
        if type(other_quantites) != list: # i.e. we only have one quantity 
            other_quantites = [other_quantites]
            return_as_nparray = True

        if leq:
            for i in range(0,len(other_quantites)):
                if disregard:
                    other_quantites[i] = np.array(other_quantites[i][xf&lt;=limit])
                else:
                    other_quantites[i][xf&gt;limit] = 0
            if disregard:
                xf = xf[xf&lt;=limit]
            else:
                pass

        if l:
            for i in range(0,len(other_quantites)):
                if disregard:
                    other_quantites[i] = np.array(other_quantites[i][xf&lt;limit])
                else:
                    other_quantites[i][xf&gt;=limit] = 0
            if disregard:
                xf = xf[xf&lt;limit]
            else:
                pass

        if geq:
            for i in range(0,len(other_quantites)):
                if disregard: 
                    other_quantites[i] = np.array(other_quantites[i][xf&gt;=limit])
                else:
                    other_quantites[i][xf&lt;limit] = 0
            if disregard:
                xf = xf[xf&gt;=limit]
            else:
                pass

        if g:
            for i in range(0,len(other_quantites)):
                if disregard: 
                    other_quantites[i] = np.array(other_quantites[i][xf&gt;limit])
                else:
                    other_quantites[i][xf&lt;=limit] = 0
            if disregard:
                xf = xf[xf&gt;limit]
            else:
                pass

        if return_as_nparray:
            return np.array(xf), other_quantites[0]
        else:
            return np.array(xf), other_quantites
    
    except IndexError:
        print(&#39;Could not change anything, try again with new limits.&#39;)
        
        return xf, other_quantites
    
def error_change(err,err_lim=1):
    &#34;&#34;&#34;
    Set error to zero if exceeds error_lim.
    
    Parameters:
    -----------
    err: np.ndarray
        Error vector.
        
    err_lim: float
        Error limit. If an error element is larger than err_lim, it is set to zero. 
    
    Returns:
    --------
    err: np.ndarray
        Error vector after setting elements that exceeds err_lim to zero.
    &#34;&#34;&#34;
    
    e_temp = []
    for e in err:
        if e &lt; err_lim:
            e_temp.append(e)
        else:
            e_temp.append(0)
    err = e_temp
    return err</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="API.Fvar_from_lc"><code class="name flex">
<span>def <span class="ident">Fvar_from_lc</span></span>(<span>lc, m, percentage_limit)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the fractional root mean square (rms) variability amplitude from
the variance (S^2) in the light curve, by averaging over K=int(np.floor(lc.N/m)) segments. </p>
<h2 id="parameters">Parameters:</h2>
<p>lc: class: 'Lightcurve'-object
The light curve data to be Fourier-transformed.</p>
<p>m: int
Number of time bins per segment. </p>
<p>percentage_limit: float, optional, default: 90
Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.</p>
<h2 id="returns">Returns:</h2>
<p>F_var: float
The fractional root mean square (rms) variability amplitude.
See Eq. (10) from: "On characterizing the variability properties of X-ray light curves from active galaxies" (Vaughan, 2003)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Fvar_from_lc(lc, m, percentage_limit):
    &#34;&#34;&#34;
    Calculate the fractional root mean square (rms) variability amplitude from 
    the variance (S^2) in the light curve, by averaging over K=int(np.floor(lc.N/m)) segments. 
    
    Parameters:
    -----------
    lc: class: &#39;Lightcurve&#39;-object
        The light curve data to be Fourier-transformed.
        
    m: int
        Number of time bins per segment. 

    percentage_limit: float, optional, default: 90
        Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.
        
    Returns:
    --------
    F_var: float
        The fractional root mean square (rms) variability amplitude.
        See Eq. (10) from: &#34;On characterizing the variability properties of X-ray light curves from active galaxies&#34; (Vaughan, 2003)
    &#34;&#34;&#34;
    
    # Useful parameters
    dt = lc.dt
    K = int(np.floor(lc.N/m)) #num_of_line_seg

    # Average over time segments 
    rms_lc_seg_v = []
    for i in range(0,K):

        # Pick out one line segment (ls)
        t_seg, rate_seg, err_seg, N_gamma, R_seg, T_seg = lc.extract_seg(m,n=i,to_print=False,to_plot=False)

        if percentage_of_filled_time_bins(t_seg,dt,to_return=True) &gt; percentage_limit: # if True, then there is no large gap in the segment
            # Perform FFT to find Power spectra for one seg
            S2 = 1/(len(rate_seg)-1)*np.sum((rate_seg-R_seg)**2)
            MSE = np.mean(err_seg**2)
            F_var = np.sqrt((S2-MSE)/R_seg**2)
            rms_lc_seg_v.append(F_var)
        else:
            pass
            
    return np.mean(rms_lc_seg_v)</code></pre>
</details>
</dd>
<dt id="API.Fvar_from_ps"><code class="name flex">
<span>def <span class="ident">Fvar_from_ps</span></span>(<span>xf, fft_rate)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the fractional root mean square (rms) variability amplitude by
integrating the power spectra over the full frequency interval. </p>
<p>If want a smaller freq band, call remove_freq() prior to calling this function.</p>
<h2 id="parameters">Parameters:</h2>
<p>xf, fft_rate: np.ndarrays
Frequency vector and power spectra.</p>
<h2 id="returns">Returns:</h2>
<p>F_var: float
The fractional root mean square (rms) variability amplitude.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Fvar_from_ps(xf,fft_rate):
    &#34;&#34;&#34;
    Calculate the fractional root mean square (rms) variability amplitude by 
    integrating the power spectra over the full frequency interval. 
    
    If want a smaller freq band, call remove_freq() prior to calling this function.
    
    Parameters:
    -----------
    xf, fft_rate: np.ndarrays
        Frequency vector and power spectra.
        
    Returns:
    --------
    F_var: float
        The fractional root mean square (rms) variability amplitude.
    &#34;&#34;&#34;
    
    F_var2 = 0
    df = xf[1]-xf[0]
    for i in range(0,len(fft_rate)):
        F_var2 += df*fft_rate[i]
    F_var = np.sqrt(F_var2)
    
    return F_var</code></pre>
</details>
</dd>
<dt id="API.coherence_intrinsic"><code class="name flex">
<span>def <span class="ident">coherence_intrinsic</span></span>(<span>lcs, m_init, noise='Gaussian', return_jointly=True, percentage_limit=90, output=False)</span>
</code></dt>
<dd>
<div class="desc"><p>The coherence function (Cf) when noise is present in the signals. The Cf is computed at least
twice, once for m (number of bins per segment) being very high to cover low frequencies and
once for m very small (256 as smallest) to cover high frequencies. </p>
<h2 id="parameters">Parameters:</h2>
<p>lcs: class: list of two 'Lightcurve'-objects
The light curves to be used in the coherence computation.</p>
<p>m_init: int
Number of bins per segment to start with. m_init will be lowered during each interation.
The higher m, the less number of segments. The fewer segments, the lower frequencies will
be considered, as f_min = 1/(m_init*dt).</p>
<p>noise: {'Poisson','Gaussian'}, optional, default: 'Gaussian'.
For a light curve with 'Poisson'/'Gaussian' errors. </p>
<p>return_jointly: boolean
If True, the coherence computed for different m:s are merged.
If False, the coherence is returned as a list of np.ndarrays, so that the computation for different m:s can be compared.</p>
<p>percentage_limit: float, optional, default: 90
Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.</p>
<p>output: boolean
If True, plot the whole light curve as well as the "conditions met"-figures. Also, helpful statements are printed.</p>
<h2 id="returns">Returns:</h2>
<p>xf_v: np.ndarray (or a list of np.ndarrays if return_jointly is False)
The frequency-vector.</p>
<p>gamma2_v: np.ndarray (or a list of np.ndarrays if return_jointly is False)
The coherence function.</p>
<p>delta_gamma2_v: np.ndarray (or a list of np.ndarrays if return_jointly is False)
One sigma uncertainty in the coherence function.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def coherence_intrinsic(lcs,m_init,noise=&#39;Gaussian&#39;,return_jointly=True,percentage_limit=90,output=False):
    &#34;&#34;&#34;
    The coherence function (Cf) when noise is present in the signals. The Cf is computed at least
    twice, once for m (number of bins per segment) being very high to cover low frequencies and 
    once for m very small (256 as smallest) to cover high frequencies. 
    
    Parameters:
    -----------
    lcs: class: list of two &#39;Lightcurve&#39;-objects
                The light curves to be used in the coherence computation.
        
    m_init: int 
        Number of bins per segment to start with. m_init will be lowered during each interation.
        The higher m, the less number of segments. The fewer segments, the lower frequencies will 
            be considered, as f_min = 1/(m_init*dt).
       
    noise: {&#39;Poisson&#39;,&#39;Gaussian&#39;}, optional, default: &#39;Gaussian&#39;.
        For a light curve with &#39;Poisson&#39;/&#39;Gaussian&#39; errors. 
       
    return_jointly: boolean
        If True, the coherence computed for different m:s are merged. 
        If False, the coherence is returned as a list of np.ndarrays, so that the computation for different m:s can be compared.
        
    percentage_limit: float, optional, default: 90
            Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.
    
    output: boolean
        If True, plot the whole light curve as well as the &#34;conditions met&#34;-figures. Also, helpful statements are printed.
    
    Returns:
    --------
    xf_v: np.ndarray (or a list of np.ndarrays if return_jointly is False)
        The frequency-vector.
        
    gamma2_v: np.ndarray (or a list of np.ndarrays if return_jointly is False)
        The coherence function.
    
    delta_gamma2_v: np.ndarray (or a list of np.ndarrays if return_jointly is False)
        One sigma uncertainty in the coherence function. 
    &#34;&#34;&#34;
    
    assert isinstance(lcs[0], lightcurve) and isinstance(lcs[1], lightcurve), &#39;The data-input does not contain two light curve objects.&#39;
    assert len(lcs)==2, &#39;Coherence should be computed with 2 light curves, not {}&#39;.format(len(lcs))
    
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
    print(&#39;                           Computing the intrinsic coherence...&#39;)
    print(&#39;---------------------------------------------------------------------------------------------------\n&#39;)
    start = timeit.default_timer()
    
    N = m_init
    k = int(np.log2(N))
    dt = lcs[0].dt
    
    find_again = True
    xf_v, gamma2_v, delta_gamma2_int_v, upper_freq_lim_v = [], [], [], []
    
    i=0
    while find_again:
        i+=1
        print(&#39;Iteration {}) Computing using m = {} bins per segment, i.e. f in [{:.3f},{:.3f}]&#39;.format(i,N,1/(N*dt),1/(2*dt)))
        print(&#39;---------------------------------------------------------------------------------------------------&#39;)
        xf, gamma2, delta_gamma2_int, upper_freq_lim = compute_coherence_intrinsic(lcs,N,noise,percentage_limit,output=output)
    
        if return_jointly: 
            # ----- Don&#39;t use the full freq band found ------------------------------------------------
            # Only use freq up to multi_factor*upper_freq_lim, i.e. slightly under uppper freq lim.
            if k!=7:
                multi_factor = 0.9
                if upper_freq_lim != -1:
                    lim = upper_freq_lim*multi_factor
                else:
                    lim = xf[-1]
            else:
                lim = xf[-1]
                
            gamma2 = gamma2[xf &lt; lim]
            delta_gamma2_int = delta_gamma2_int[xf &lt; lim]
            xf = xf[xf &lt; lim]
            # -----------------------------------------------------------------------------------------            
        
            # Append to lists
            xf_v = np.append(xf_v,xf)
            gamma2_v = np.append(gamma2_v,gamma2) 
            delta_gamma2_int_v = np.append(delta_gamma2_int_v,delta_gamma2_int) 
            upper_freq_lim_v = np.append(upper_freq_lim_v,upper_freq_lim)
        
        # If not return_jointly; can see what freq-range the different m caught.
        else:
            # Rebin directly; only want to get a feeling for the different m
            num = 50
            lim = xf[-1]
            xf,gamma2,delta_gamma2_int = log_rebin(xf,gamma2,delta_gamma2_int,num=num)
            delta_gamma2_int = error_change(delta_gamma2_int)
            
            # Append to lists
            xf_v.append(xf)
            gamma2_v.append(gamma2) 
            delta_gamma2_int_v.append(delta_gamma2_int) 
            upper_freq_lim_v.append(upper_freq_lim)
    
        print(&#39;Intrinsic coherence found for f in [{:.3f},{:.3f}] \n&#39;.format(1/(N*dt),lim))
        
        # Change number of bins / segment until next time
        if k == 8:
            find_again = False
        else:
            multi_factor = 10 #discuss what this is... 
            while 2**(k-1) &gt; multi_factor/(dt*upper_freq_lim):
                k -= 1
                if k == 8:
                    break 
            N = 2**k
    
    time_taken = timeit.default_timer()-start
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
    print(&#39;             Intrinsic coherence found (in {:.2f} sec). return_jointly = {}&#39;.format(time_taken,return_jointly))
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
    
    return xf_v, gamma2_v, delta_gamma2_int_v</code></pre>
</details>
</dd>
<dt id="API.coherence_noiseless"><code class="name flex">
<span>def <span class="ident">coherence_noiseless</span></span>(<span>data, percentage_limit=90, m=8192, C_v=None)</span>
</code></dt>
<dd>
<div class="desc"><p>The coherence function (Cf) when noise is not present in the signals.</p>
<p>Cf is a Fourier-frequencydependent measure of the degree of linear correlation
between two concurrent time series. Specifically, it gives the fraction of the mean-squared variability
at f of one time series that can be attributed to, or equivalently predicted from, the other (Nowak,1999).</p>
<h2 id="parameters">Parameters:</h2>
<p>data: Either lc (1) or ps (2).
(1) class: list of two 'Lightcurve'-objects
The light curves to be used in the coherence computation.
(2) class: list of two 'PowerSpectrum'-objects
The power spectras to be used in the coherence computation.</p>
<p>m: int
Number of time bins per segment.</p>
<p>C_v: np.ndarray
If the cross spectrum has already been found, it can be used. In this case, we require data to be a list of PowerSpectrum-object.</p>
<h2 id="returns">Returns:</h2>
<p>xf: np.ndarray
The frequency-vector (with the current binning).</p>
<p>gamma2: np.ndarray
The coherence function.</p>
<p>delta_gamma2: np.ndarray
One sigma uncertainty in the coherence function.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def coherence_noiseless(data,percentage_limit=90,m=2**13,C_v=None):
    &#34;&#34;&#34;
    The coherence function (Cf) when noise is not present in the signals.
    
    Cf is a Fourier-frequencydependent measure of the degree of linear correlation 
    between two concurrent time series. Specifically, it gives the fraction of the mean-squared variability 
    at f of one time series that can be attributed to, or equivalently predicted from, the other (Nowak,1999).
    
    Parameters:
    ----------- 
    data: Either lc (1) or ps (2).
        (1) class: list of two &#39;Lightcurve&#39;-objects
                The light curves to be used in the coherence computation.
        (2) class: list of two &#39;PowerSpectrum&#39;-objects
                The power spectras to be used in the coherence computation.
        
    m: int 
        Number of time bins per segment.
    
    C_v: np.ndarray
        If the cross spectrum has already been found, it can be used. In this case, we require data to be a list of PowerSpectrum-object.
    
    Returns:
    --------
    xf: np.ndarray
        The frequency-vector (with the current binning).
        
    gamma2: np.ndarray
        The coherence function.
    
    delta_gamma2: np.ndarray 
        One sigma uncertainty in the coherence function. 
    &#34;&#34;&#34;

    print(&#39;Computing the coherence...&#39;)
    
    # Find the Cross Spectrum
    if not isinstance(C_v, np.ndarray):
        ps_v, C_v = cross_spec(data, m=m, percentage_limit=percentage_limit)
        S_v = np.array([ps_v[0].fft_rate_v,ps_v[1].fft_rate_v])
        xf = ps_v[0].xf
    
    # Have already found the Cross Spectrum
    elif isinstance(C_v, np.ndarray):
        print(&#39;Cross spectra already found.&#39;)
        if isinstance(data[0], PowerSpectrum) and isinstance(data[1], PowerSpectrum):
            xf = data[0].xf
            S_v = np.array([data[0].fft_rate_v,data[1].fft_rate_v])
        else:
            print(&#39;If you provide me with the cross spectra C_v, the input data need to be a list of PowerSpectrum-objects.&#39;)
    
    # The Periodograms w/o normalization
    P1_v = [np.abs(x)**2 for x in S_v[0]]
    P2_v = [np.abs(x)**2 for x in S_v[1]]

    # The Coherence and its error
    K = np.size(C_v,axis=0) #number of segments
    gamma2 = abs(np.mean(C_v,axis=0))**2/(np.mean(P1_v,axis=0)*np.mean(P2_v,axis=0))
    delta_gamma2 = np.sqrt(2)*(1-gamma2)/(np.sqrt(abs(gamma2))*np.sqrt(K))
    
    print(&#39;Coherence computed. \n&#39;)
    
    return xf, gamma2, delta_gamma2, C_v</code></pre>
</details>
</dd>
<dt id="API.comp_gamma2"><code class="name flex">
<span>def <span class="ident">comp_gamma2</span></span>(<span>C2, n2, P1, N1, P2, N2)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute gamma2. See: Epitropakis, A. (2017), Eq. (A1). Compare with Eq. (3) (the noiseless case).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def comp_gamma2(C2,n2,P1,N1,P2,N2):    
    &#34;&#34;&#34;
    Compute gamma2. See: Epitropakis, A. (2017), Eq. (A1). Compare with Eq. (3) (the noiseless case).
    &#34;&#34;&#34;
    
    return (C2-n2)/((P1-N1)*(P2-N2))</code></pre>
</details>
</dd>
<dt id="API.compute_coherence_intrinsic"><code class="name flex">
<span>def <span class="ident">compute_coherence_intrinsic</span></span>(<span>lcs, m, noise, percentage_limit, output=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the intrinsic coherence for a given number of bins per segment (m).</p>
<h2 id="parameters">Parameters:</h2>
<p>lcs: class: list of two 'Lightcurve'-objects
The light curves to be used in the coherence computation.</p>
<p>m: int
Number of bins per segment.</p>
<p>noise: {'Poisson','Gaussian'}.
For a light curve with 'Poisson'/'Gaussian' errors. </p>
<p>percentage_limit: float, optional, default: 90
Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.</p>
<p>output: boolean
If True, plot the whole light curve as well as the "conditions met"-figures. Also, helpful statements are printed.</p>
<h2 id="returns">Returns:</h2>
<p>xf: np.ndarray
The frequency-vector (which depends on m).</p>
<p>gamma2: np.ndarray
The coherence function.</p>
<p>delta_gamma2: np.ndarray
One sigma uncertainty in the coherence function. </p>
<p>C_mean: np.ndarray
The cross spectra, can be used for time lag estimations.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_coherence_intrinsic(lcs,m,noise,percentage_limit,output=False):
    &#34;&#34;&#34;
    Computes the intrinsic coherence for a given number of bins per segment (m).
    
    Parameters:
    ----------- 
    lcs: class: list of two &#39;Lightcurve&#39;-objects
                The light curves to be used in the coherence computation.
        
    m: int
        Number of bins per segment.
    
    noise: {&#39;Poisson&#39;,&#39;Gaussian&#39;}.
        For a light curve with &#39;Poisson&#39;/&#39;Gaussian&#39; errors. 
    
    percentage_limit: float, optional, default: 90
            Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.
    
    output: boolean
        If True, plot the whole light curve as well as the &#34;conditions met&#34;-figures. Also, helpful statements are printed.
    
    Returns:
    ------------
    xf: np.ndarray
        The frequency-vector (which depends on m).
        
    gamma2: np.ndarray
        The coherence function.
    
    delta_gamma2: np.ndarray 
        One sigma uncertainty in the coherence function. 
        
    C_mean: np.ndarray
        The cross spectra, can be used for time lag estimations.
    &#34;&#34;&#34;
    
    # Compute power spectra, cross spectra, and signal powers
    ps_v, C_v = cross_spec(lcs,m,percentage_limit,noise=noise,return_noise_wo_sub=True)
    S_v = np.array([ps_v[0].fft_rate_v,ps_v[1].fft_rate_v])

    # Need to extract freq.vector (same for ps0 and ps1), number of segments K and mean cross spectra
    xf = ps_v[0].xf 
    K = len(C_v)
    C2_mean = np.array(np.abs(np.mean(C_v,axis=0))**2 )
    
    # Compute average for all power terms over all segments and then calculate n2 once
    P1_mean = np.mean(np.abs(S_v[0])**2,axis=0) #power of signal: |S|^2
    P2_mean = np.mean(np.abs(S_v[1])**2,axis=0)
    N1_mean = np.mean(ps_v[0].Pnoise_v,axis=0) #power of noise
    N2_mean = np.mean(ps_v[1].Pnoise_v,axis=0)
    n2 = (P1_mean*N2_mean + P2_mean*N1_mean - N1_mean*N2_mean)/(K)
    
    # Compute coherence
    gamma2 = comp_gamma2(C2_mean,n2,P1_mean,N1_mean,P2_mean,N2_mean)
    
    # Compute coherence error
    delta_gamma2_int = compute_delta_gamma2_int(gamma2,C2_mean,n2,P1_mean,N1_mean,P2_mean,N2_mean,K)
    
    # Check for what frequencies the intrinsic coherence can be usefully estimaed.
    upper_freq_lim = conditions(xf,gamma2,C2_mean,P1_mean,P2_mean,N1_mean,N2_mean,n2,K,output)
    
    return xf, gamma2, delta_gamma2_int, upper_freq_lim</code></pre>
</details>
</dd>
<dt id="API.compute_delta_gamma2_int"><code class="name flex">
<span>def <span class="ident">compute_delta_gamma2_int</span></span>(<span>gamma2, C2, n2, P1, N1, P2, N2, K)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the error delta_gamma2_int. See: Epitropakis, A. (2017), Eq. (A3)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_delta_gamma2_int(gamma2,C2,n2,P1,N1,P2,N2,K):
    &#34;&#34;&#34;
    Compute the error delta_gamma2_int. See: Epitropakis, A. (2017), Eq. (A3)
    &#34;&#34;&#34;
    
    delta_gamma2 = np.sqrt(2/K)*(1-gamma2)/(np.sqrt(abs(gamma2)))
    term1 = 2*n2**2*K/(C2-n2)**2
    term2 = N1**2/(P1-N1)**2
    term3 = N2**2/(P2-N2)**2
    term4 = K*delta_gamma2**2/gamma2**2
    
    return gamma2/np.sqrt(K)*(term1+term2+term3+term4)**(1/2)</code></pre>
</details>
</dd>
<dt id="API.conditions"><code class="name flex">
<span>def <span class="ident">conditions</span></span>(<span>xf, gamma2, C2, P1, P2, N1, N2, n2, K, output)</span>
</code></dt>
<dd>
<div class="desc"><p>The intrinsic coherence can be usefully estimated when the following conditions are met:
1) sqrt(C2) &gt; sqrt(n2)
2) |S_1|^2/|N_1|^2 &gt; 1/\sqrt{m}
3) |S_2|^2/|N_2|^2 &gt; 1/\sqrt{m}</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conditions(xf,gamma2,C2,P1,P2,N1,N2,n2,K,output):
    &#34;&#34;&#34;
    The intrinsic coherence can be usefully estimated when the following conditions are met: 
    1) sqrt(C2) &gt; sqrt(n2)
    2) |S_1|^2/|N_1|^2 &gt; 1/\sqrt{m}
    3) |S_2|^2/|N_2|^2 &gt; 1/\sqrt{m}
    &#34;&#34;&#34;
    
    upper_freq_lim = []
    
    C = np.sqrt(np.abs(C2))
    n = np.sqrt(np.abs(n2))
    
    if output:
        standard_plot(h=8)
        plt.subplot(3,1,1)

        plt.loglog(xf,C,label=r&#34;$|&lt;C(f)&gt;|^2$&#34;)
        plt.loglog(xf,n,label=r&#34;$n^2$&#34;)
    
    try:
        # Condition 1
        idx = np.argwhere(np.diff(np.sign(C-n))).flatten()
        upper_freq_lim.append(xf[idx[0]])
        if output:
            plt.plot(upper_freq_lim[-1], n[idx[0]], &#39;ro&#39;,label=str(xf[idx[0]]))
    except:
        pass
    
    if output:
        plt.legend()
        plt.title(&#39;Conditions to be met for useful estimation of $\gamma_I^2$&#39;)

        plt.subplot(3,2,3)
        plt.title(r&#39;High Power (Term 2 of $\delta \gamma^2_{int}$)&#39;)
        plt.loglog(xf,(P1-N1),label=r&#39;$|S_1|^2$&#39;)
        plt.loglog(xf,N1*np.ones(np.size(xf))/np.sqrt(K),label=r&#39;$N_1/\sqrt{m}$&#39;)
    
    try:
        # Condition 2
        temp = N1*np.ones(np.size(xf))/np.sqrt(K)
        idx = np.argwhere(np.diff(np.sign(temp-(P1-N1)))).flatten()
        upper_freq_lim.append(xf[idx[0]])
        
        if output:
            plt.plot(upper_freq_lim[-1], temp[idx[0]], &#39;ro&#39;,label=str(xf[idx[0]]))
    except:
        pass
    
    if output:
        plt.legend()

        plt.subplot(3,2,4)
        plt.title(r&#39;High Power (Term 3 of $\delta \gamma^2_{int}$)&#39;)
        plt.loglog(xf,(P2-N2),label=r&#39;$|S_2|^2$&#39;)
        plt.loglog(xf,N2*np.ones(np.size(xf))/np.sqrt(K),label=r&#39;$N_2/\sqrt{m}$&#39;)
        
    try:
        # Condition 3
        temp = N2*np.ones(np.size(xf))/np.sqrt(K)
        idx = np.argwhere(np.diff(np.sign(temp-(P2-N2)))).flatten()
        upper_freq_lim.append(xf[idx[0]])
        if output:
            plt.plot(upper_freq_lim[-1], temp[idx[0]], &#39;ro&#39;,label=str(xf[idx[0]]))
    except:
        pass
    
    if output:
        plt.legend()

        plt.subplot(3,1,3)
        plt.title(&#39;High Coherence&#39;)
        plt.loglog(xf,gamma2,label=r&#39;$\gamma^2_{int}$&#39;)
        plt.loglog(xf,n**2/(P1*P2),label=r&#39;$n^2/(P_1P_2)$&#39;)
    try:
        idx = np.argwhere(np.diff(np.sign(gamma2-n**2/(P1*P2)))).flatten()
        upper_freq_lim.append(xf[idx[0]])
        if output:
            plt.plot(upper_freq_lim[-1], gamma2[idx[0]], &#39;ro&#39;,label=str(xf[idx[0]]))
    except:
        pass
    
    if output:
        plt.legend()
        plt.tight_layout()
        plt.show()
    
    if len(upper_freq_lim) == 0:
        return -1
    else:
        return np.amin(upper_freq_lim)</code></pre>
</details>
</dd>
<dt id="API.covariance_spectrum"><code class="name flex">
<span>def <span class="ident">covariance_spectrum</span></span>(<span>lcs, m, alt=1, freq_low=None, freq_high=None, percentage_limit=90, units='abs', to_plot=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the covariance spectrum, which is more robust than the rms-spectra.</p>
<h2 id="parameters">Parameters:</h2>
<p>lcs: list of two 'Lightcurve'-objects
The light curves to be used in the covariance computation.
m: int
Number of time bins per segment. </p>
<p>alt: int {1, 2}, optional, default: 1
What method to compute the covariance with.
If full freq range: 1 = for whole light curve directly, 2 = segment-wise
If smaller freq range: 1 = using FFT and inverseFFT, 2 = using coherence</p>
<p>low_freq/up_freq: floats, optional, default: None
Lower and upper frequency limits.</p>
<p>percentage_limit: float, optional, default: 90
Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.</p>
<p>units: {'abs','frac'}, optional, default: 'abs'
What unit to return the rms in.</p>
<p>to_plot: boolean (default: False)
If True, a figure for different ways to compute the covariance is displayed. </p>
<h2 id="returns">Returns:</h2>
<p>cov_norm: float
The normalised coviarance as calculated by Eq(2) and Eq(3) from Wilkinson(2009).</p>
<p>cov_err: float
The statistical error of the covariance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def covariance_spectrum(lcs,m,alt=1,freq_low=None,freq_high=None,percentage_limit=90,units=&#39;abs&#39;,to_plot=False):
    &#34;&#34;&#34;
    Compute the covariance spectrum, which is more robust than the rms-spectra.
    
    Parameters:
    -----------
    lcs: list of two &#39;Lightcurve&#39;-objects
                The light curves to be used in the covariance computation.
    m: int
        Number of time bins per segment. 
        
    alt: int {1, 2}, optional, default: 1
        What method to compute the covariance with.
        If full freq range: 1 = for whole light curve directly, 2 = segment-wise 
        If smaller freq range: 1 = using FFT and inverseFFT, 2 = using coherence
    
    low_freq/up_freq: floats, optional, default: None
        Lower and upper frequency limits.
     
    percentage_limit: float, optional, default: 90
            Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.
    
    units: {&#39;abs&#39;,&#39;frac&#39;}, optional, default: &#39;abs&#39;
        What unit to return the rms in.
    
    to_plot: boolean (default: False)
        If True, a figure for different ways to compute the covariance is displayed. 
        
    Returns:
    -----------
    cov_norm: float
        The normalised coviarance as calculated by Eq(2) and Eq(3) from Wilkinson(2009).
        
    cov_err: float 
        The statistical error of the covariance.  
    &#34;&#34;&#34;
    start = timeit.default_timer()
    
    lcs = subtract_overlapping_energybands(lcs)
    
    # Extract data
    lc_X = lcs[0]
    t_X, rate_X, err_X, R_X = lc_X.t, lc_X.rate, lc_X.err, lc_X.R
    # and from Reference Band:
    lc_Y = lcs[1]
    t_Y, rate_Y, err_Y, R_Y = lc_Y.t, lc_Y.rate, lc_Y.err, lc_Y.R
    
    errX = np.mean(err_X**2)
    errY = np.mean(err_Y**2)
    
    comparison = t_X == t_Y
    assert comparison.all(), &#39;Time arrays are not identical. The two light curves must come from the same observation...&#39;

    # Useful parameters (take from reference band, but is the same as the corresponding X-band values)
    dt = t_Y[1]-t_Y[0]
    N = len(t_Y)
    
    # Full frequency range
    if freq_low == None and freq_high == None: 
        
        if alt == 1:
            ## ------------------------------------ Cov for whole light curve ---------------------------------------------
            print(&#39;---------------------------------------------------------------------------------------------------&#39;)
            print(&#39;                           Computing covariance for whole light curve directly...&#39;)
            print(&#39;---------------------------------------------------------------------------------------------------\n&#39;)

            # Prep Work to normalize covariance and to compute covariance error
            errX = np.mean(err_X**2)
            errY = np.mean(err_Y**2)

            var_ex_X_lc = 1/(len(rate_X)-1)*np.sum((rate_X-np.mean(rate_X))**2)-errX
            var_ex_Y_lc = 1/(len(rate_Y)-1)*np.sum((rate_Y-np.mean(rate_Y))**2)-errY
            assert var_ex_X_lc &lt; var_ex_Y_lc, &#34;The reference band should be the band with highest absolute variability.&#34;
            Fvar_lc = np.sqrt(var_ex_Y_lc)/np.mean(rate_Y) 

            # Compute Covariance
            cov = 1/(len(rate_Y)-1)*np.sum((rate_X-np.mean(rate_X))*(rate_Y-np.mean(rate_Y)))
                
            # Normalize
            cov = cov/np.sqrt(var_ex_Y_lc)
            # Error
            cov_err = np.sqrt((var_ex_X_lc*errY+var_ex_Y_lc*errX+errX*errY)/(len(t_X)*var_ex_Y_lc))
            
            if units != &#39;abs&#39;: 
                cov /= R_X 
                cov_err /= R_X
                
            print(&#39;Cov = &#39;,cov,&#39; pm &#39;,cov_err,&#39;\n&#39;)

        elif alt == 2:
            ## ------------------------------------- Cov for segments ---------------------------------------------------------
            print(&#39;---------------------------------------------------------------------------------------------------&#39;)
            print(&#39;                           Computing covariance segment-wise...&#39;)
            print(&#39;---------------------------------------------------------------------------------------------------\n&#39;)
            
            # Prep Work to normalize covariance and to compute covariance error
            ps_X = PowerSpectrum(lcs[0],m=m,percentage_limit=percentage_limit,timer_on=False,save_all=True)
            ps_Y = PowerSpectrum(lcs[1],m=m,percentage_limit=percentage_limit,timer_on=False,save_all=True)
            
            fft_rate_meanX = ps_X.fft_rate
            fft_rate_meanY = ps_Y.fft_rate

            var_ex_X = [1/(m-1)*np.sum((rate_seg-R_seg)**2)-np.mean(err_seg**2) for rate_seg, R_seg, err_seg in zip(ps_X.rate_seg, ps_X.R_seg, ps_X.err_seg)]
            var_ex_Y = [1/(m-1)*np.sum((rate_seg-R_seg)**2)-np.mean(err_seg**2) for rate_seg, R_seg, err_seg in zip(ps_Y.rate_seg, ps_Y.R_seg, ps_Y.err_seg)]

            var_err_X = [np.mean(e**2) for e in ps_X.err_seg]
            var_err_Y = [np.mean(e**2) for e in ps_Y.err_seg]
            
            P_X_mean = np.mean(ps_X.fft_rate)
            P_Y_mean = np.mean(ps_Y.fft_rate)

            # Should we take the average over all segments to get the final variance error? 
            var_err_X_mean = np.mean(var_err_X,axis=0)
            var_err_Y_mean = np.mean(var_err_Y,axis=0)

            # Excess Variance 
            var_ex_X_mean = np.mean(var_ex_X)
            var_ex_Y_mean = np.mean(var_ex_Y)
            assert var_ex_X_mean &lt; var_ex_Y_mean, &#34;The reference band should be the band with highest absolute variability.&#34;

            # Quantities neeeded:
            cov = [1/(m-1)*np.sum((rate_seg_X-R_X_seg)*(rate_seg_Y-R_Y_seg)) for rate_seg_X, rate_seg_Y, R_X_seg, R_Y_seg in zip(ps_X.rate_seg, ps_Y.rate_seg, ps_X.R_seg, ps_Y.R_seg)]
            if units != &#39;abs&#39;: 
                cov = [c/R for c,R in zip(cov,ps_X.R_seg)]
            
            # 1) Small difference between taking average over all covariances and using full excess variance to normalize
            cov_mean = np.mean(cov,axis=0)
            cov_norm_alt1 = cov_mean/np.sqrt(var_ex_Y_mean)
            print(&#39;cov_norm = &#39;,cov_norm_alt1)
            # 2) vs using excess variance from each segment to normalize and then taking the average 
            cov_seg_norm = np.array(cov)/np.sqrt(var_ex_Y)
            cov_norm_alt2 = np.mean(cov_seg_norm,axis=0)
            print(&#39;cov_norm = &#39;,cov_norm_alt2)
            
            cov = cov_norm_alt2
            cov_err = np.sqrt((var_ex_X_mean*var_err_Y_mean+var_ex_Y_mean*var_err_X_mean+var_err_X_mean*var_err_Y_mean)/(N*var_ex_Y_mean))
            if units != &#39;abs&#39;: 
                cov_err /= R_X
            print(&#39;Cov = &#39;,cov,&#39; pm &#39;,cov_err,&#39;\n&#39;)
                  
    # If not full freq. range
    else:
        if alt == 1:
            ## ------------------------------------ Cov using FFT and inverseFFT ---------------------------------------------       
            print(&#39;---------------------------------------------------------------------------------------------------&#39;)
            print(&#39;                           Computing covariance using FFT and inverseFFT...&#39;)
            print(&#39;---------------------------------------------------------------------------------------------------\n&#39;)
            
            xf = np.array(fftfreq(N, dt))[1:N//2]
            # Pick out frequency range 
            if freq_low == None:
                freq_low = xf[0]
            if freq_high == None:
                freq_high = xf[-1]
                
            rate_X = pick_out_freq_from_lc(lcs[0], freq_low, freq_high)
            rate_Y = pick_out_freq_from_lc(lcs[1], freq_low, freq_high)
                        
            # rms needs to be taken from normalized power spectra in the relevant freq range
            ps_X, ps_Y = PowerSpectrum(lcs[0],m=N,timer_on=False,save_all=True,percentage_limit=0), PowerSpectrum(lcs[1],m=N,timer_on=False,save_all=True,percentage_limit=0)
            fft_rate_meanX, fft_rate_meanY = ps_X.fft_rate, ps_Y.fft_rate
            
            xf, fft_rates = remove_freq(xf,[fft_rate_meanX,fft_rate_meanY],freq_low,geq=True,disregard=False)
            xf, fft_rates = remove_freq(xf,[fft_rates[0],fft_rates[1]],freq_high,leq=True,disregard=False)
            
            FvarX, FvarY = Fvar_from_ps(xf, fft_rates[0]), Fvar_from_ps(xf, fft_rates[1])
            #print(&#39;FvarY = &#39;,FvarY)
            var_ex_X_lc, var_ex_Y_lc = (FvarX * R_X)**2, (FvarY * R_Y)**2
            assert var_ex_X_lc &lt; var_ex_Y_lc, &#34;The reference band should be the band with highest absolute variability.&#34;
            
            # Compute cov
            cov = 1/(len(rate_Y)-1)*np.sum((rate_X-np.mean(rate_X))*(rate_Y-np.mean(rate_Y)))
            # Normalize                                     
            cov = cov/np.sqrt(var_ex_Y_lc)
            
            # Error
            P_noise_X_full = np.mean(err_X**2)/(R_X**2*1/(2*dt))
            P_noise_Y_full = np.mean(err_Y**2)/(R_Y**2*1/(2*dt))
            errX = P_noise_X_full*R_X**2*(freq_high-freq_low)
            errY = P_noise_Y_full*R_Y**2*(freq_high-freq_low)
            cov_err = np.sqrt((var_ex_X_lc*errY+var_ex_Y_lc*errX+errX*errY)/(len(t_X)*var_ex_Y_lc))
            
            if units != &#39;abs&#39;: 
                cov /= R_X 
                cov_err /= R_X
            print(&#39;Cov = &#39;,cov,&#39; pm &#39;,cov_err,&#39;\n&#39;)
            
        elif alt == 2:
            ## ----------------------------------------------- Cov using coherence ------------------------------------------------------------
            print(&#39;---------------------------------------------------------------------------------------------------&#39;)
            print(&#39;                          Covariance using coherence...&#39;)
            print(&#39;---------------------------------------------------------------------------------------------------\n&#39;)

            upper_freq_lim = freq_high
            assert upper_freq_lim != None, &#34;You need to set an upper freq. limit; will be the same for all lc.&#34;

            # Compute intrinsic coherence 
            xf_coh, gamma2, delta_gamma2_int = coherence_intrinsic(lcs,m_init=2**16)
            xf_coh, gamma2 = remove_freq(xf_coh,gamma2,limit=upper_freq_lim,leq=True)
            dnu = upper_freq_lim-xf_coh[0]

            # Should use the fft_rate_meanX/fft_rate_meanY from the coherence computation (that uses different m), 
            # but this is not implemented...
            # ---- This part will do until fixed.  Small difference... -----------
            ps_X = PowerSpectrum(lcs[0],m=m,percentage_limit=percentage_limit,timer_on=False,save_all=True)
            ps_Y = PowerSpectrum(lcs[1],m=m,percentage_limit=percentage_limit,timer_on=False,save_all=True)
            xf = ps_X.xf

            fft_rate_meanX = ps_X.fft_rate
            fft_rate_meanY = ps_Y.fft_rate
            
            # ----------------------------------------------------------------------------------------------------

            _, fft_rate_meanX_test = remove_freq(xf,fft_rate_meanX,limit=upper_freq_lim,leq=True)
            P_X_mean = np.mean(fft_rate_meanX_test)

            _, fft_rate_meanY_test = remove_freq(xf,fft_rate_meanY,limit=upper_freq_lim,leq=True)
            P_Y_mean = np.mean(fft_rate_meanY_test)

            # Compute covariance
            cov = R_X*np.sqrt(np.mean(gamma2)*P_X_mean*dnu)
            print(&#39;Cov = &#39;,cov)

            # Error
            P_noise_X_full = np.mean(err_X**2)/(R_X**2*1/(2*dt))
            P_noise_Y_full = np.mean(err_Y**2)/(R_Y**2*1/(2*dt))

            errX = R_X*P_noise_X_full*dnu
            errY = R_Y*P_noise_Y_full*dnu
            var_ex_X_lc = R_X*P_X_mean*dnu
            var_ex_Y_lc = R_Y*P_Y_mean*dnu
            assert var_ex_X_lc &lt; var_ex_Y_lc, &#34;The reference band should be the band with highest absolute variability.&#34;
            
            cov_err = np.sqrt((cov_through_gamma2**2*errY+var_ex_Y_lc*errX+errX*errY)/(len(t_X)*var_ex_Y_lc))

            if units != &#39;abs&#39;: 
                cov /= R_X 
                cov_err /= R_X
            
            print(&#39;Cov = &#39;,cov,&#39; pm &#39;,cov_err,&#39;\n&#39;)
    
    time_taken = timeit.default_timer()-start
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
    print(&#39;                           Covariance found (in {:.2f} sec).&#39;.format(time_taken))
    print(&#39;---------------------------------------------------------------------------------------------------&#39;) 
    
    return cov, cov_err</code></pre>
</details>
</dd>
<dt id="API.cross_spec"><code class="name flex">
<span>def <span class="ident">cross_spec</span></span>(<span>data, m, percentage_limit=90, noise=None, return_noise_wo_sub=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the cross spectrum.</p>
<h2 id="parameters">Parameters:</h2>
<p>data: Either lc (1) or ps (2).
(1) class: list of two 'Lightcurve'-objects
The light curves to be used in the coherence computation.
(2) class: list of two 'PowerSpectrum'-objects
The power spectras to be used in the coherence computation.</p>
<p>m: int
Number of time bins per segment.</p>
<p>percentage_limit: float, optional, default: 90
Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.</p>
<p>noise: {'Poisson','Gaussian'}, optional, default: None.
For a light curve with 'Poisson'/'Gaussian' errors. </p>
<p>return_noise_wo_sub: boolean, optional, default: False
If True, noise is returned but not subtracted from powspec.
If False, returned and subtracted.</p>
<h2 id="returns">Returns:</h2>
<p>C_v: np.ndarray
The cross-spectrum (see e.g. section 3 of Epitropakis, A. (2017)) after averaging over K segments.</p>
<p>S1_v, S2_v: np.ndarrays
The discrete Fourier transform of the two signals.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cross_spec(data,m,percentage_limit=90,noise=None,return_noise_wo_sub=False):
    &#34;&#34;&#34;
    Computes the cross spectrum.
    
    Parameters:
    ----------- 
    data: Either lc (1) or ps (2).
        (1) class: list of two &#39;Lightcurve&#39;-objects
                The light curves to be used in the coherence computation.
        (2) class: list of two &#39;PowerSpectrum&#39;-objects
                The power spectras to be used in the coherence computation.
        
    m: int 
        Number of time bins per segment.
        
    percentage_limit: float, optional, default: 90
            Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.
    
    noise: {&#39;Poisson&#39;,&#39;Gaussian&#39;}, optional, default: None.
        For a light curve with &#39;Poisson&#39;/&#39;Gaussian&#39; errors. 
        
    return_noise_wo_sub: boolean, optional, default: False
        If True, noise is returned but not subtracted from powspec.
        If False, returned and subtracted.
    
    Returns:
    --------
    C_v: np.ndarray
        The cross-spectrum (see e.g. section 3 of Epitropakis, A. (2017)) after averaging over K segments.
        
    S1_v, S2_v: np.ndarrays
        The discrete Fourier transform of the two signals.
    &#34;&#34;&#34;
    
    print(&#39;Computing the cross spectrum...\n&#39;)

    if isinstance(data[0], lightcurve) and isinstance(data[1], lightcurve):
    
        # Segment-wise: 
        ps_v = [PowerSpectrum(lc, m=m, normalization=None, noise=noise, percentage_limit=percentage_limit,                               timer_on=False, return_noise_wo_sub=return_noise_wo_sub) for lc in data]

    if isinstance(data[0], PowerSpectrum) and isinstance(data[1], PowerSpectrum):
        ps_v = data
        
    S_v = np.array([ps_v[0].fft_rate_v,ps_v[1].fft_rate_v])
    C_v = np.array([np.conjugate(S1)*S2 for S1,S2 in zip(S_v[0],S_v[1])])

    print(&#39;Cross spectrum computed.&#39;)    
    
    return ps_v, C_v</code></pre>
</details>
</dd>
<dt id="API.error_change"><code class="name flex">
<span>def <span class="ident">error_change</span></span>(<span>err, err_lim=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Set error to zero if exceeds error_lim.</p>
<h2 id="parameters">Parameters:</h2>
<p>err: np.ndarray
Error vector.</p>
<p>err_lim: float
Error limit. If an error element is larger than err_lim, it is set to zero. </p>
<h2 id="returns">Returns:</h2>
<p>err: np.ndarray
Error vector after setting elements that exceeds err_lim to zero.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def error_change(err,err_lim=1):
    &#34;&#34;&#34;
    Set error to zero if exceeds error_lim.
    
    Parameters:
    -----------
    err: np.ndarray
        Error vector.
        
    err_lim: float
        Error limit. If an error element is larger than err_lim, it is set to zero. 
    
    Returns:
    --------
    err: np.ndarray
        Error vector after setting elements that exceeds err_lim to zero.
    &#34;&#34;&#34;
    
    e_temp = []
    for e in err:
        if e &lt; err_lim:
            e_temp.append(e)
        else:
            e_temp.append(0)
    err = e_temp
    return err</code></pre>
</details>
</dd>
<dt id="API.extract_fits"><code class="name flex">
<span>def <span class="ident">extract_fits</span></span>(<span>filename, keywords=[], p=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract (and print info about) a fits-file. </p>
<h2 id="parameters">Parameters:</h2>
<p>filename: string
The path to the .fits-file.</p>
<p>keywords: list of strings
Keywords (and corresponding values) apart from the ones mentioned below to return.
<em>All data-keys are always automatically returned (for a lightcurve, these are: {"TIME", "RATE", "ERROR"}).
</em>Note also that the header-keys {"CONTENT","OBJECT","CPIX","MINCHAN","MAXCHAN"} are returned if they exist. </p>
<p>p: boolean
If True, print the headers of the fits-file.</p>
<h2 id="returns">Returns:</h2>
<p>data: dictionary
Keys and the corresponding data values.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_fits(filename,keywords=[],p=True):
    &#34;&#34;&#34; 
    Extract (and print info about) a fits-file. 
    
    Parameters:
    -----------
    filename: string
        The path to the .fits-file.
    
    keywords: list of strings
        Keywords (and corresponding values) apart from the ones mentioned below to return. 
        *All data-keys are always automatically returned (for a lightcurve, these are: {&#34;TIME&#34;, &#34;RATE&#34;, &#34;ERROR&#34;}).
        *Note also that the header-keys {&#34;CONTENT&#34;,&#34;OBJECT&#34;,&#34;CPIX&#34;,&#34;MINCHAN&#34;,&#34;MAXCHAN&#34;} are returned if they exist. 
    
    p: boolean
        If True, print the headers of the fits-file.
        
    Returns:
    --------
    data: dictionary
        Keys and the corresponding data values.
    &#34;&#34;&#34;
    
    print(&#39;Loading fits from filename: &#39;,filename)
    with fits.open(filename) as hdulist: 
    
        # HEADER
        header0 = hdulist[0].header
        header1 = hdulist[1].header
        
        # DATA KEYS
        binaryext = hdulist[1].data
        binarytable = Table(binaryext)
        keys = binarytable.keys()
        
        data = {}
        for key in keys:
            data[key] = hdulist[1].data[key]
        
        # HEADER KEYS
        try:
            data[&#39;CONTENT&#39;] = header0[&#39;CONTENT&#39;]
            content_exist = True
        except KeyError:
            print(&#39;There is no content-information for this fits.&#39;)
            content_exist = False
        try:
            data[&#39;OBJECT&#39;] = header0[&#39;OBJECT&#39;]
        except KeyError:   
            print(&#39;There is no object-information for this fits.&#39;)
        try:
            data[&#39;CPIX&#39;] = header1[&#39;CPIX&#39;]
            data[&#39;MINCHAN&#39;] = header1[&#39;MINCHAN&#39;]
            data[&#39;MAXCHAN&#39;] = header1[&#39;MAXCHAN&#39;]
        except KeyError:   
            print(&#39;There is no CPIX-information for this fits.&#39;)
            
        # Extract extra keyswords
        for extra_key in keywords:
            
            try: 
                data[extra_key] = header0[extra_key]
                print(&#34;Found key {} in header0&#34;.format(extra_key))
                cannot_find = False
            except KeyError:
                print(&#34;There is no key {} in header0, let&#39;s have a look at header1.&#34;.format(extra_key))
            try: 
                data[extra_key] = header1[extra_key]
                print(&#34;Found key {} in header1.&#34;.format(extra_key))
                cannot_find = False
            except KeyError:
                print(&#34;There is no key {} in header1 either, sorry...&#34;.format(extra_key))
                cannot_find = True
            
            if cannot_find: 
                matchingK0 = matchingKeys(header0, extra_key)
                matchingK1 = matchingKeys(header1, extra_key)
                print(&#39;Matching keys in header0 = &#39;,matchingK0)
                print(&#39;Matching keys in header1 = &#39;,matchingK1)
                for K in matchingK0:
                    use = input(&#39;Do you want to extract the header0 key {}? [y/n] &#39;.format(K))
                    if use == &#39;y&#39; or use == &#39;yes&#39; or use == &#39;Y&#39;:
                        data[K] = header0[K]
                for K in matchingK1:
                    use = input(&#39;Do you want to extract the header1 key {}? [y/n] &#39;.format(K))
                    if use == &#39;y&#39; or use == &#39;yes&#39; or use == &#39;Y&#39;:
                        data[K] = header1[K]
            
        if p:
            print(&#39;hdu.info()&#39;)
            print(hdulist.info(),&#39;\n&#39;)
            print(&#39;Header0:&#39;)
            print(repr(header0),&#39;\n&#39;) #repr() prints the info into neat columns
            print(&#39;Header1:&#39;)
            print(repr(header1),&#39;\n&#39;) #repr() prints the info into neat columns
            print(binarytable[0:10],&#39;\n&#39;)
        else:
            if content_exist:
                print(&#39;The keys to the {} data are: {}&#39;.format(data[&#39;CONTENT&#39;],data.keys()))
            else:
                print(&#39;The keys to the data are: {}&#39;.format(data.keys()))
        print(&#39;Loading fits done. \n&#39;)
        
        return data</code></pre>
</details>
</dd>
<dt id="API.ifft_smallfreqband"><code class="name flex">
<span>def <span class="ident">ifft_smallfreqband</span></span>(<span>fft_rate)</span>
</code></dt>
<dd>
<div class="desc"><p>Inverse transformation from freq domain to time domain. Is called upon in pick_out_freq_from_lc().</p>
<h2 id="parameters">Parameters:</h2>
<p>fft_rate: np.ndarray
The power spectra after having set values outside freq range to 0.</p>
<h2 id="returns">Returns:</h2>
<p>rate: np.ndarray
New rate vector, now only containing the frequencies in fft_rate.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ifft_smallfreqband(fft_rate):
    &#34;&#34;&#34;
    Inverse transformation from freq domain to time domain. Is called upon in pick_out_freq_from_lc().
    
    Parameters:
    -----------
    fft_rate: np.ndarray
        The power spectra after having set values outside freq range to 0.
        
    Returns:
    --------
    rate: np.ndarray
        New rate vector, now only containing the frequencies in fft_rate. 
    &#34;&#34;&#34;
    
    # Add the negative freq again (that were removed due to [1:m//2])
    y_together = np.append(np.zeros(1),fft_rate)
    y_together = np.append(y_together,np.zeros(1))
    y_together = np.append(y_together,np.conjugate(np.flip(fft_rate)))
    # Perform ifft
    yinv = ifft(y_together)
    # Make to np.ndarray and extract only the real values
    yinv = np.array(yinv)
    rate = yinv.real  
    return rate </code></pre>
</details>
</dd>
<dt id="API.load_lightcurve"><code class="name flex">
<span>def <span class="ident">load_lightcurve</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"><p>Split the data from a light curve into time, flux, and error vectors. </p>
<h2 id="parameters">Parameters:</h2>
<p>data: dictionary
Should contain the keys ['TIME', 'RATE', 'ERROR', 'FRACEXP', 'CONTENT'] with corresponding values.
The data['CONTENT'] ought to be 'LIGHT CURVE'</p>
<h2 id="returns">Returns:</h2>
<p>t, rate, error: np.ndarrays
The data values for a light curve: time, rate, rate_error, respectively.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_lightcurve(data):
    &#34;&#34;&#34; 
    Split the data from a light curve into time, flux, and error vectors. 
    
    Parameters:
    -----------
    data: dictionary
        Should contain the keys [&#39;TIME&#39;, &#39;RATE&#39;, &#39;ERROR&#39;, &#39;FRACEXP&#39;, &#39;CONTENT&#39;] with corresponding values.
        The data[&#39;CONTENT&#39;] ought to be &#39;LIGHT CURVE&#39;
        
    Returns:
    --------
    t, rate, error: np.ndarrays
        The data values for a light curve: time, rate, rate_error, respectively.
    &#34;&#34;&#34;

    assert data[&#39;CONTENT&#39;]==&#39;LIGHT CURVE&#39;, &#39;Data does not come from a light curve object.&#39;

    t = np.array(data[&#39;TIME&#39;])
    rate = np.array(data[&#39;RATE&#39;])
    err = np.array(data[&#39;ERROR&#39;])
    
    return t, rate, err</code></pre>
</details>
</dd>
<dt id="API.log_rebin"><code class="name flex">
<span>def <span class="ident">log_rebin</span></span>(<span>xf, take_average_of, err=[], low_lim=None, high_lim=None, num=50)</span>
</code></dt>
<dd>
<div class="desc"><p>Distributes the data into logarithmic frequency bins and computes the bin-average of the data.</p>
<h2 id="parameters">Parameters:</h2>
<p>xf: np.ndarray
The frequency-vector (with the current binning).</p>
<p>take_average_of: np.ndarray
The quantity to take average of. </p>
<p>err: np.ndarray, optional, default: empty list
Errors corresponding to the "take_average_of"-quantity.
</p>
<p>low_lim, high_lim: floats, optional, default: None
The interval to bin into log-bins: 10^(low_lim) to 10^(high_lim).
If None, limits are given by the end points of the frequency vector.</p>
<p>num: int
The number of logarithmic frequency bins to create.</p>
<h2 id="returns">Returns:</h2>
<p>middle_of_log_bins: np.ndarray
The logarithmic midpoint of each log-bin, computed as: 10*<em>(1/2</em>(np.log10(log_bins[i])+np.log10(log_bins[i+1])))</p>
<p>average: np.ndarray
The average of the data within a log-bin.</p>
<p>error: np.ndarray
The standard deviation of all points within a log-bin.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_rebin(xf,take_average_of,err=[],low_lim=None,high_lim=None,num=50):
    &#34;&#34;&#34;
    Distributes the data into logarithmic frequency bins and computes the bin-average of the data.
    
    Parameters:
    -----------
    xf: np.ndarray 
        The frequency-vector (with the current binning).
    
    take_average_of: np.ndarray 
        The quantity to take average of. 
        
    err: np.ndarray, optional, default: empty list
        Errors corresponding to the &#34;take_average_of&#34;-quantity.  
    
    low_lim, high_lim: floats, optional, default: None
        The interval to bin into log-bins: 10^(low_lim) to 10^(high_lim).
        If None, limits are given by the end points of the frequency vector.
    
    num: int
        The number of logarithmic frequency bins to create.
    
    Returns:
    --------
    middle_of_log_bins: np.ndarray 
        The logarithmic midpoint of each log-bin, computed as: 10**(1/2*(np.log10(log_bins[i])+np.log10(log_bins[i+1])))
        
    average: np.ndarray
        The average of the data within a log-bin.
        
    error: np.ndarray
        The standard deviation of all points within a log-bin. 
    &#34;&#34;&#34;
    
    # Average over the frequencies (logarithmic)
    if low_lim == None:
        low_lim = np.log10(np.amin(xf))
    if high_lim == None:
        high_lim = np.log10(np.amax(xf))
    log_bins = np.logspace(low_lim, high_lim, num=num)
    middle_of_log_bins = []
    for i in range(0,len(log_bins)-1):
        middle_of_log_bins.append(10**(1/2*(np.log10(log_bins[i])+np.log10(log_bins[i+1]))))
    
    # Determine what freq-values correspond to what bin
    digitized = np.digitize(xf, log_bins)
    
    # Sort into bins and make sure no bin is empty 
    middle_of_log_bins = [middle_of_log_bins[i-1] for i in range(1, len(log_bins)) if len(take_average_of[digitized == i])!=0]
    average = [take_average_of[digitized == i].mean() for i in range(1, len(log_bins)) if len(take_average_of[digitized == i])!=0]
    
    # Standard deviation for points in this bin if no error as input:
    if len(err) == 0:
        error = [take_average_of[digitized == i].std() for i in range(1, len(log_bins)) if len(take_average_of[digitized == i])!=0]
    # If error as input, compute error propagation:
    else:
        error = [np.sqrt(np.sum((err[digitized == i])**2))/len(err[digitized == i]) for i in range(1, len(log_bins)) if len(take_average_of[digitized == i])!=0]
        
    return np.array(middle_of_log_bins), np.array(average), np.array(error)</code></pre>
</details>
</dd>
<dt id="API.matchingKeys"><code class="name flex">
<span>def <span class="ident">matchingKeys</span></span>(<span>dictionary, searchString)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def matchingKeys(dictionary, searchString):
    return [key for key,val in dictionary.items() if searchString in key]    </code></pre>
</details>
</dd>
<dt id="API.multiply_w_spectra"><code class="name flex">
<span>def <span class="ident">multiply_w_spectra</span></span>(<span>lcs, rms_v, rms_err_v, channel_to_kev, spectral_data)</span>
</code></dt>
<dd>
<div class="desc"><p>Multiplies rms (or covariance) with spectra do obtain rms/covariance spectra.
Might only work well for RXTE (where channels are given by their energy max): <a href="https://heasarc.gsfc.nasa.gov/docs/xte/e-c_table.html">https://heasarc.gsfc.nasa.gov/docs/xte/e-c_table.html</a> </p>
<h2 id="parameters">Parameters:</h2>
<p>lcs: class: list of 'Lightcurve'-objects
The light curves to be used.</p>
<p>rms_v: np.ndarray
Absolute/fractional rms.</p>
<p>rms_err_v: np.ndarray
The error in (absolute/fractional) rms.</p>
<p>channel_to_kev: np.ndarray
Conversion from channel (index) to energy (keV).</p>
<p>spectral_data: dict or None, optional, default: None
Spectral data of the observation. If dict, rms is scaled with the spectral_data to yield the rms spectra.</p>
<h2 id="returns">Returns:</h2>
<p>rms_v: np.ndarray
Absolute/fractional rms mulitplied with the spectra.</p>
<p>rms_errv_: np.ndarray
The error in (absolute/fractional) rms mulitplied with the spectra.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def multiply_w_spectra(lcs,rms_v,rms_err_v,channel_to_kev,spectral_data):
    &#34;&#34;&#34;
    Multiplies rms (or covariance) with spectra do obtain rms/covariance spectra.
    Might only work well for RXTE (where channels are given by their energy max): https://heasarc.gsfc.nasa.gov/docs/xte/e-c_table.html 
    
    Parameters:
    -----------
    lcs: class: list of &#39;Lightcurve&#39;-objects
                The light curves to be used.
            
    rms_v: np.ndarray 
        Absolute/fractional rms.
        
    rms_err_v: np.ndarray 
        The error in (absolute/fractional) rms.
        
    channel_to_kev: np.ndarray
        Conversion from channel (index) to energy (keV).
    
    spectral_data: dict or None, optional, default: None
        Spectral data of the observation. If dict, rms is scaled with the spectral_data to yield the rms spectra.
        
    Returns:
    --------
    rms_v: np.ndarray 
        Absolute/fractional rms mulitplied with the spectra.
        
    rms_errv_: np.ndarray 
        The error in (absolute/fractional) rms mulitplied with the spectra.
    &#34;&#34;&#34;
    
    for i in range(0,len(rms_v)):
        lc = lcs[i]

        # Covert channel min/max to energy min/max
        minchan = lc.minchan
        if minchan != 0:
            minchan -= 1 # For RXTE: since each channel only corresponds to its energy max, we need to sub to get its min (the former channel&#39;s max)
        minene = channel_to_kev[minchan]
        if minchan == 0:
            minene = 0
        maxchan = lc.maxchan
        maxene = channel_to_kev[maxchan]

        scale_rms = np.mean(spectral_data[&#39;COUNTS/SEC&#39;][minchan:maxchan])/lc.deltaE
        scale_err = np.mean(spectral_data[&#39;STATERR/SEC&#39;][minchan:maxchan])/lc.deltaE
        #print(&#39;dkeV, counts, scale_rms, scale_err = &#39;,lc.deltaE,&#39;, &#39;,np.mean(data[&#39;COUNTS/SEC&#39;][minchan:maxchan]),&#39;, &#39;,scale_rms,&#39;, &#39;,scale_err,&#39;\n&#39;)

        rms_v[i] *= scale_rms
        rms_err_v[i] *= scale_err
        
    return rms_v, rms_err_v</code></pre>
</details>
</dd>
<dt id="API.percentage_of_filled_time_bins"><code class="name flex">
<span>def <span class="ident">percentage_of_filled_time_bins</span></span>(<span>t_seg, dt, to_return=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Percentage of time bins being filled, i.e. without gaps.</p>
<h2 id="parameters">Parameters:</h2>
<p>t_seg: np.ndarray
Segment's time vector. Should start from zero, i.e. t_seg[0] = 0. </p>
<p>dt: np.float
Time resolution of observation.</p>
<p>to_return: boolean (default: False)
If false, print gap percentage, otherwise return it. </p>
<h2 id="returns">Returns:</h2>
<p>perc_wo_gaps: np.float
Percentage of time bins being filled, i.e. without gaps.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def percentage_of_filled_time_bins(t_seg,dt,to_return=True):
    &#34;&#34;&#34;
    Percentage of time bins being filled, i.e. without gaps.
    
    Parameters:
    ----------- 
    t_seg: np.ndarray
        Segment&#39;s time vector. Should start from zero, i.e. t_seg[0] = 0. 
        
    dt: np.float
        Time resolution of observation.
        
    to_return: boolean (default: False)
        If false, print gap percentage, otherwise return it. 
        
    Returns:
    --------
    perc_wo_gaps: np.float
        Percentage of time bins being filled, i.e. without gaps.
    &#34;&#34;&#34;
    
    t_seg_temp = np.copy(t_seg)
    
    # Linear transformation of segment&#39;s first element to t_seg=0    
    if int(t_seg_temp[0]) != 0:
        t_seg_temp -= t_seg_temp[0]
    
    # Count number of filled (= non-empty = no gap) bins
    num_bins = math.ceil(t_seg_temp[-1]/dt)
    hist, edges = np.histogram(t_seg_temp,bins=num_bins,range=(0, dt*num_bins))
    
    # Percentage of filled bins (i.e. without gap)
    perc_wo_gaps = np.sum(hist)/len(hist)*100
    
    if to_return:
        return perc_wo_gaps 
    else: 
        print(&#39;perc_wo_gaps = {:.4f}&#39;.format(perc_wo_gaps))</code></pre>
</details>
</dd>
<dt id="API.pick_out_freq_from_lc"><code class="name flex">
<span>def <span class="ident">pick_out_freq_from_lc</span></span>(<span>lc, low_freq, up_freq, to_plot=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Fourier transforms a light curve, sets the power = 0 for all freq outside freq_range: freq_low-freq_high,
and then performs an inverse Fourier transform back to time-domain.</p>
<h2 id="parameters">Parameters:</h2>
<p>lc: class 'Lightcurve'-object
The light curve, whose rms is to be found.</p>
<p>low_freq/up_freq: floats, optional, default: None
Lower and upper frequency limits.</p>
<h2 id="returns">Returns:</h2>
<p>rate_ifft: np.ndarray
The inverse Fourier transformed rate-vector, with mean = 0.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pick_out_freq_from_lc(lc, low_freq, up_freq, to_plot=False):
    &#34;&#34;&#34;
    Fourier transforms a light curve, sets the power = 0 for all freq outside freq_range: freq_low-freq_high,
    and then performs an inverse Fourier transform back to time-domain.
    
    Parameters:
    -----------
    lc: class &#39;Lightcurve&#39;-object
            The light curve, whose rms is to be found.
            
    low_freq/up_freq: floats, optional, default: None
        Lower and upper frequency limits.
    
    Returns:
    --------
    rate_ifft: np.ndarray
        The inverse Fourier transformed rate-vector, with mean = 0.
    &#34;&#34;&#34;
    
    # Pick out relevant quantties from the lightcurves
    t, dt, rate, err, R, N = lc.t, lc.dt, lc.rate, lc.err, lc.R, lc.N
    
    # FFT on full light curve
    xf = np.array(fftfreq(N, dt))[1:N//2]
    fft_rate_unnormalized = fft(rate-R)[1:N//2]
                      
    if low_freq == xf[0] and up_freq == xf[-1]: 
        print(&#34;You&#39;re using the full freq. range.&#34;)
    else:
        print(&#39;Prepare inverse FFT using only the freq interval: [{},{}]&#39;.format(low_freq, up_freq)) 
    
    xf, fft_rate = remove_freq(xf,fft_rate_unnormalized,low_freq,geq=True,disregard=False)
    xf, fft_rate = remove_freq(xf,fft_rate,up_freq,leq=True,disregard=False)
    
    # Transform back 
    rate_ifft = ifft_smallfreqband(fft_rate)
    print(&#39;Inverse FFT performed.&#39;)

    if to_plot:
        standard_plot()
        plt.plot(t,rate-R,label=&#39;Lc with all freq&#39;)
        plt.plot(t,rate_ifft,label=&#39;Lc using only f = [{},{}]&#39;.format(low_freq,up_freq))
        plt.legend()
        ax = plt.gca()
        ax.set_xlim([t[0],t[500]])
        plt.show()
        
    return rate_ifft</code></pre>
</details>
</dd>
<dt id="API.plot_coherence"><code class="name flex">
<span>def <span class="ident">plot_coherence</span></span>(<span>xf, gamma2, delta_gamma2_int, err_lim=1, num=75, save_fig=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the coherence.</p>
<h2 id="parameters">Parameters:</h2>
<p>xf: np.ndarray
The frequency-vector (which depends on m).</p>
<p>gamma2: np.ndarray
The coherence function.</p>
<p>delta_gamma2: np.ndarray
One sigma uncertainty in the coherence function. </p>
<p>err_lim: float, optional, default: 1
Error limit. If an error element is larger than err_lim, it is set to zero. </p>
<p>num: int, optional, default: 75
The number of logarithmic frequency bins to create before plotting.</p>
<p>save_fig: boolean, optional, default: False
If True, asks for path to location to save plot.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_coherence(xf,gamma2,delta_gamma2_int,err_lim=1,num=75,save_fig = False):
    &#34;&#34;&#34;
    Plot the coherence.
    
    Parameters:
    -----------
    xf: np.ndarray
        The frequency-vector (which depends on m).
        
    gamma2: np.ndarray
        The coherence function.
    
    delta_gamma2: np.ndarray 
        One sigma uncertainty in the coherence function. 
        
    err_lim: float, optional, default: 1
        Error limit. If an error element is larger than err_lim, it is set to zero. 
    
    num: int, optional, default: 75
        The number of logarithmic frequency bins to create before plotting.
        
    save_fig: boolean, optional, default: False
         If True, asks for path to location to save plot.
    &#34;&#34;&#34;
    
    standard_plot(h=4,w=6)
    ax = plt.gca()
        
    first_band = input(&#39;First energyband (write in keV as &#34;?-?&#34;)&#39;)
    second_band = input(&#39;Second energyband (write in keV as &#34;?-?&#34;)&#39;)
    ax.text(0.3,0.1,&#39;({} keV) vs. ({} keV)&#39;.format(first_band,second_band),fontsize=14,horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;, transform=ax.transAxes)
    
    if len(xf) &lt;= 3:
        for i in range(0,len(xf)):
            ax.errorbar(xf[i],gamma2[i],yerr=delta_gamma2_int[i], fmt = &#39;.&#39;,mfc=&#39;w&#39;,capsize=2, elinewidth=1, markeredgewidth=1,label=r&#39;$N=2^6$&#39;)
    else:
        xf, gamma2, error = log_rebin(xf,gamma2,delta_gamma2_int,num=num)
        error = error_change(error,err_lim)
        ax.errorbar(xf,gamma2,yerr=error, fmt = &#39;.k&#39;,mfc=&#39;w&#39;,capsize=2, elinewidth=1, markeredgewidth=1,label=r&#39;$N=2^6$&#39;)
    
    ax.set_ylim([0,1.4])
    ax.axhline(1,color=&#39;k&#39;,linewidth=1,alpha=0.7)
    ax.set_xscale(&#34;log&#34;)
    plt.xlabel(&#39;Frequency [Hz]&#39;)

    plt.ylabel(&#39;$\gamma_{int}^2$&#39;)
    if save_fig:
        path = input(&#39;Path to location to save plot [end with .png]: &#39;) 
        plt.savefig(path,bbox_inches=&#39;tight&#39;)
    plt.show()</code></pre>
</details>
</dd>
<dt id="API.plot_timelag"><code class="name flex">
<span>def <span class="ident">plot_timelag</span></span>(<span>xf_v, tau_v, dtau_v, num=50, save_fig=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the time lag.</p>
<h2 id="parameters">Parameters:</h2>
<p>xf_v: np.ndarray
Frequencies.</p>
<p>tau_v: np.ndarray
Time lag.</p>
<p>dtau_v: np.ndarray
Error in time lag.</p>
<p>save_fig: boolean, optional, default: False
If True, asks for path to location to save plot.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_timelag(xf_v,tau_v,dtau_v,num=50,save_fig=False):
    &#34;&#34;&#34;
    Plot the time lag.
    
    Parameters:
    -----------
    xf_v: np.ndarray
        Frequencies.
    
    tau_v: np.ndarray
        Time lag.
    
    dtau_v: np.ndarray
        Error in time lag.
        
    save_fig: boolean, optional, default: False
         If True, asks for path to location to save plot.
    &#34;&#34;&#34;
    
    # Rebin
    xf_rebin,tau,dtau = log_rebin(xf_v, tau_v, dtau_v,num=num)
    
    ax = standard_plot(w=6)
    
    # Separate into pos/neg values of tau
    tau_n, tau_p = [-t for t in tau if t&lt;0 ],[t for t in tau if t&gt;0]
    x_n, x_p = [x for x,t in zip(xf_rebin,tau) if t&lt;0],[x for x,t in zip(xf_rebin,tau) if t&gt;0]
    dtau_n, dtau_p = [x for x,t in zip(dtau,tau) if t&lt;0],[x for x,t in zip(dtau,tau) if t&gt;0]
    
    # Loop over neg vs pos lags
    for x,tau,dtau,c,s in zip([x_n, x_p],[tau_n, tau_p],[dtau_n, dtau_p],[&#39;w&#39;,&#39;k&#39;],[5,4.5]):

        # Replace errors that go below 0 with a downarrow
        for i in range(0,len(x)):
            
            if abs(dtau[i]) &gt; abs(tau[i]): #if error is larger than tau-value
                # Plot lower error as an arrow
                ax.errorbar(x[i],tau[i],yerr=[[0.7*tau[i]],[0]], fmt = &#39;ok&#39;, uplims = True, mfc=c, markersize=s, elinewidth=1)
                # Plot upper error as normal
                ax.errorbar(x[i],tau[i],yerr=[[0],[dtau[i]]], fmt = &#39;ok&#39;, mfc=c, markersize=s, capsize=2, elinewidth=1, markeredgewidth=1)
            else:
                pass
                # Plot as normal
                ax.errorbar(x[i],tau[i],yerr=[[dtau[i]],[dtau[i]]], fmt = &#39;ok&#39;, mfc=c, markersize=s, capsize=2, elinewidth=1, markeredgewidth=1)

    ax.set_xscale(&#39;log&#39;)
    ax.set_yscale(&#39;log&#39;)
    plt.xlabel(&#39;Frequency [Hz]&#39;)
    plt.ylabel(&#39;Lag (sec.)&#39;)
    first_band = &#39;0-3.9&#39;
    second_band = &#39;3.9-6.0&#39;
    ax.text(0.73,0.93,&#39;({} keV) vs. ({} keV)&#39;.format(first_band,second_band),fontsize=12,horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;, transform=ax.transAxes)
    if save_fig:
        path = input(&#39;Path to location to save plot [end with .png]: &#39;) 
        plt.savefig(path,bbox_inches=&#39;tight&#39;)
    plt.show()</code></pre>
</details>
</dd>
<dt id="API.remove_freq"><code class="name flex">
<span>def <span class="ident">remove_freq</span></span>(<span>xf, other_quantites, limit, leq=False, l=False, geq=False, g=False, disregard=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Remove frequencies (f) and other quantites' (OQ) corresponding values for those f.
Example: limit = 10 and leq = True: means that only f &lt;= 10 are saved.</p>
<h2 id="parameters">Parameters:</h2>
<p>xf: np.ndarray</p>
<p>other_quantites: list of np.ndarrays</p>
<p>limit: float
The numerical value of the limit. The type of limit is determined next</p>
<p>leq, l, geq, g: Boolean (default: False)
less or equal than (leq), less than (l), greater or equal to (geq), greater than (g)
the limit will be SAVED.</p>
<p>disregard: Boolean (default: False)
If True, then the frequency and OQ will be cropped and returned as shorter vectors.
If False, returned in the same length with OQs' elements outside given range being set to 0.</p>
<h2 id="returns">Returns:</h2>
<p>freq, : np.ndarray
The frequency vector. </p>
<p>other_quantites: list of np.ndarrays or np.ndarray (if just one quantity)
After disregarding values (or setting them to zero) corresponding to frequencies outside the desired interval.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_freq(xf,other_quantites,limit,leq=False,l=False,geq=False,g=False,disregard=True):
    &#34;&#34;&#34;
    Remove frequencies (f) and other quantites&#39; (OQ) corresponding values for those f. 
    Example: limit = 10 and leq = True: means that only f &lt;= 10 are saved.
    
    Parameters:
    -----------
    xf: np.ndarray
        
    other_quantites: list of np.ndarrays
    
    limit: float
        The numerical value of the limit. The type of limit is determined next
    
    leq, l, geq, g: Boolean (default: False)
        less or equal than (leq), less than (l), greater or equal to (geq), greater than (g) 
        the limit will be SAVED.
        
    disregard: Boolean (default: False)
        If True, then the frequency and OQ will be cropped and returned as shorter vectors.
        If False, returned in the same length with OQs&#39; elements outside given range being set to 0.

    Returns:
    --------
    freq, : np.ndarray
        The frequency vector. 
        
    other_quantites: list of np.ndarrays or np.ndarray (if just one quantity)
        After disregarding values (or setting them to zero) corresponding to frequencies outside the desired interval. 
    &#34;&#34;&#34;
    
    assert type(limit) != list, &#39;Unfortunately, you cannot fix more than one limit at once.&#39;
    
    try: 
        return_as_nparray = False
        if type(other_quantites) != list: # i.e. we only have one quantity 
            other_quantites = [other_quantites]
            return_as_nparray = True

        if leq:
            for i in range(0,len(other_quantites)):
                if disregard:
                    other_quantites[i] = np.array(other_quantites[i][xf&lt;=limit])
                else:
                    other_quantites[i][xf&gt;limit] = 0
            if disregard:
                xf = xf[xf&lt;=limit]
            else:
                pass

        if l:
            for i in range(0,len(other_quantites)):
                if disregard:
                    other_quantites[i] = np.array(other_quantites[i][xf&lt;limit])
                else:
                    other_quantites[i][xf&gt;=limit] = 0
            if disregard:
                xf = xf[xf&lt;limit]
            else:
                pass

        if geq:
            for i in range(0,len(other_quantites)):
                if disregard: 
                    other_quantites[i] = np.array(other_quantites[i][xf&gt;=limit])
                else:
                    other_quantites[i][xf&lt;limit] = 0
            if disregard:
                xf = xf[xf&gt;=limit]
            else:
                pass

        if g:
            for i in range(0,len(other_quantites)):
                if disregard: 
                    other_quantites[i] = np.array(other_quantites[i][xf&gt;limit])
                else:
                    other_quantites[i][xf&lt;=limit] = 0
            if disregard:
                xf = xf[xf&gt;limit]
            else:
                pass

        if return_as_nparray:
            return np.array(xf), other_quantites[0]
        else:
            return np.array(xf), other_quantites
    
    except IndexError:
        print(&#39;Could not change anything, try again with new limits.&#39;)
        
        return xf, other_quantites</code></pre>
</details>
</dd>
<dt id="API.rms_freqband"><code class="name flex">
<span>def <span class="ident">rms_freqband</span></span>(<span>lc, ps, m, low_freq=None, up_freq=None, units='abs')</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the rms in a given frequency band for a given energy (as given by
the lightcurve and corresponding power spectrum).</p>
<h2 id="parameters">Parameters:</h2>
<p>lc: class 'Lightcurve'-object
The light curve, whose rms is to be found.</p>
<p>ps: class: 'PowerSpectrum'-object
The corresponding (important!) power spectra to be used.</p>
<p>m: int
Number of bins per segment.</p>
<p>low_freq/up_freq: floats
Lower and upper frequency limits.</p>
<p>units: {'abs','frac'}, optional, default: 'abs'
What unit to return the rms in.</p>
<h2 id="returns">Returns:</h2>
<p>sigma: np.float
Absolute/fractional rms.</p>
<p>sigma_err: np.float
The error in (absolute/fractional) rms. See Eq. (14) of Uttley (2014): "X-ray reverberation around accreting black holes"</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rms_freqband(lc,ps,m,low_freq=None,up_freq=None,units=&#39;abs&#39;):
    &#34;&#34;&#34;
    Computes the rms in a given frequency band for a given energy (as given by 
    the lightcurve and corresponding power spectrum).
    
    Parameters:
    -----------
    lc: class &#39;Lightcurve&#39;-object
            The light curve, whose rms is to be found.
    
    ps: class: &#39;PowerSpectrum&#39;-object
            The corresponding (important!) power spectra to be used.
    
    m: int
        Number of bins per segment.
        
    low_freq/up_freq: floats
        Lower and upper frequency limits.
        
    units: {&#39;abs&#39;,&#39;frac&#39;}, optional, default: &#39;abs&#39;
        What unit to return the rms in.
        
    Returns:
    --------
    sigma: np.float
        Absolute/fractional rms.
    
    sigma_err: np.float
        The error in (absolute/fractional) rms. See Eq. (14) of Uttley (2014): &#34;X-ray reverberation around accreting black holes&#34;
    &#34;&#34;&#34;
    
    # If no low/high freq limits given:
    if low_freq == None:
        low_freq = ps.xf[0]
    if up_freq == None:
        up_freq = ps.xf[-1]
    # Frequency range
    dnu = up_freq-low_freq
    print(&#39;Freq range = {:.5f}-{:.5f} Hz&#39;.format(low_freq,up_freq))
    
    # Noise Power is constant in freq!
    P_noise = ps.averagePnoise
    
    # Error variance
    if units == &#39;abs&#39;:
        sigma_noise2 = P_noise*dnu*lc.R**2
        print(&#39;R = &#39;,lc.R)
    else:
        sigma_noise2 = P_noise*dnu #now it is rather rms, not sigma...
    
    # Find rms within the given interval
    xf, fft_rate = remove_freq(ps.xf,ps.fft_rate,limit=low_freq,geq=True,disregard=True)
    xf, fft_rate = remove_freq(xf,fft_rate,limit=up_freq,leq=True,disregard=True)
    
    rms = np.sqrt(dnu * np.mean(fft_rate)) #\approx same as Fvar_from_ps(xf,fft_rate)
    #rms_lc = Fvar_from_lc(lc,m,percentage_limit=90) # --&gt; yield the same value
    
    if units == &#39;abs&#39;: 
        sigma = rms*lc.R
    else:
        sigma = rms #now it is rather rms, not sigma...
    
    # Error: 
    sigma_err = np.sqrt((2*sigma**2*sigma_noise2+sigma_noise2**2)/(len(lc.t)*sigma**2))
    
    return sigma, sigma_err</code></pre>
</details>
</dd>
<dt id="API.rms_vs_energy"><code class="name flex">
<span>def <span class="ident">rms_vs_energy</span></span>(<span>lcs, ps_v, channel_to_kev, m=8192, low_freq=None, up_freq=None, units='abs')</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the fractional variance amplitude (rms) for multiple light curves in a given frequency band.</p>
<h2 id="parameters">Parameters:</h2>
<p>lcs: class: list of 'Lightcurve'-objects
The light curves to be used.</p>
<p>ps_v: class: list of 'PowerSpectrum'-objects
The corresponding (important!) power spectras to be used.</p>
<p>m: int
Number of bins per segment.</p>
<p>low_freq/up_freq: floats
Lower and upper frequency limits.</p>
<p>units: {'abs','frac'}, optional, default: 'abs'
What unit to return the rms in.
</p>
<h2 id="returns">Returns:</h2>
<p>energy_mid_v: np.ndarray
The average energy from each light curve's energy band. </p>
<p>energy_err_v: np.ndarray
Half the size of each light curve's energy band. </p>
<p>rms_v: np.ndarray
Absolute/fractional rms.</p>
<p>rms_err_: np.ndarray
The error in (absolute/fractional) rms.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rms_vs_energy(lcs,ps_v,channel_to_kev,m=2**13,low_freq=None,up_freq=None,units=&#39;abs&#39;):
    &#34;&#34;&#34;
    Compute the fractional variance amplitude (rms) for multiple light curves in a given frequency band.
    
    Parameters:
    -----------
    lcs: class: list of &#39;Lightcurve&#39;-objects
                The light curves to be used.
    
    ps_v: class: list of &#39;PowerSpectrum&#39;-objects
                The corresponding (important!) power spectras to be used.
    
    m: int
        Number of bins per segment.
        
    low_freq/up_freq: floats
        Lower and upper frequency limits.
        
    units: {&#39;abs&#39;,&#39;frac&#39;}, optional, default: &#39;abs&#39;
        What unit to return the rms in.        

           
    Returns:
    --------
    energy_mid_v: np.ndarray 
        The average energy from each light curve&#39;s energy band. 
    
    energy_err_v: np.ndarray 
        Half the size of each light curve&#39;s energy band. 
        
    rms_v: np.ndarray 
        Absolute/fractional rms.
        
    rms_err_: np.ndarray 
        The error in (absolute/fractional) rms.
    &#34;&#34;&#34;
    
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
    print(&#39;                           Computing the rms...&#39;)
    print(&#39;---------------------------------------------------------------------------------------------------\n&#39;)
    start = timeit.default_timer()
    
    rms_v, rms_err_v = [], []
    
    for lc,ps in zip(lcs,ps_v):     
        # Find rms                
        rms, rms_err = rms_freqband(lc,ps,m,low_freq,up_freq,units=units)
        
        rms_v.append(rms)
        rms_err_v.append(rms_err)
        
    time_taken = timeit.default_timer()-start
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
    print(&#39;                           Rms found (in {:.2f} sec).&#39;.format(time_taken))
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
     
    return np.array(rms_v),np.array(rms_err_v)</code></pre>
</details>
</dd>
<dt id="API.standard_plot"><code class="name flex">
<span>def <span class="ident">standard_plot</span></span>(<span>h=4, w=10)</span>
</code></dt>
<dd>
<div class="desc"><p>Standard plot to enable use of the same figsize, fontsize and font.family in all figures.</p>
<h2 id="parameters">Parameters:</h2>
<p>h,w: (float, float), optional, default: h=4, w=10
Height and width of the figure, i.e., figsize=(w,h).</p>
<h2 id="returns">Returns:</h2>
<p>ax: Axes,
Axes of the figure.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def standard_plot(h=4,w=10):
    &#34;&#34;&#34;Standard plot to enable use of the same figsize, fontsize and font.family in all figures.
    
    Parameters:
    -----------
    h,w: (float, float), optional, default: h=4, w=10
        Height and width of the figure, i.e., figsize=(w,h).
        
    Returns:
    --------
    ax: Axes,
        Axes of the figure.
    &#34;&#34;&#34;
    
    fig = plt.figure(figsize=(w,h))
    plt.rcParams.update(plt.rcParamsDefault)
    plt.rcParams.update({&#39;font.size&#39;: 16})
    plt.rcParams[&#39;font.family&#39;] = &#39;Times&#39;
    plt.rc(&#39;text&#39;, usetex=True) 
    
    return plt.gca()</code></pre>
</details>
</dd>
<dt id="API.subtract_overlapping_energybands"><code class="name flex">
<span>def <span class="ident">subtract_overlapping_energybands</span></span>(<span>lcs)</span>
</code></dt>
<dd>
<div class="desc"><p>When the covariance is being calculated for an energy channel inside the reference band,
the channel of interest is removed from the reference band. The reasoning behind this is
that if the channel of interest is duplicated in the reference band, the Poisson error
contribution for that channel will not cancel and will contaminate the covariance.</p>
<h2 id="parameters">Parameters:</h2>
<p>lcs: list of two 'Lightcurve'-objects
The light curves to be used in the covariance computation.
lcs[0] = light curve of interest, lcs[1] = reference band.</p>
<h2 id="returns">Returns:</h2>
<p>lcs: list of two 'Lightcurve'-objects
The reference light curve, from which we potentially have subtracted the light curve of interest.
Or the light curve of interest, from which we potentially have subtracted the reference light curve.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def subtract_overlapping_energybands(lcs):
    &#34;&#34;&#34;
    When the covariance is being calculated for an energy channel inside the reference band, 
    the channel of interest is removed from the reference band. The reasoning behind this is 
    that if the channel of interest is duplicated in the reference band, the Poisson error 
    contribution for that channel will not cancel and will contaminate the covariance.
    
    Parameters:
    -----------
    lcs: list of two &#39;Lightcurve&#39;-objects
            The light curves to be used in the covariance computation. 
            lcs[0] = light curve of interest, lcs[1] = reference band.
            
    Returns:
    ------------
    lcs: list of two &#39;Lightcurve&#39;-objects
        The reference light curve, from which we potentially have subtracted the light curve of interest. 
        Or the light curve of interest, from which we potentially have subtracted the reference light curve.
    &#34;&#34;&#34;
    
    lc_X = lcs[0]
    lc_Y = lcs[1]
    
    ebandX = lc_X.channels
    eminX = lc_X.minchan
    emaxX = lc_X.maxchan
    ebandY = lc_Y.channels
    eminY = lc_Y.minchan
    emaxY = lc_Y.maxchan
    
    # X entirely within Y
    if eminX &gt;= eminY and emaxX &lt;= emaxY: 
        lc_Y.rate -= lc_X.rate
        lc_Y.err -= lc_X.err
        lc_Y.R = np.mean(lc_Y.rate)
        print(&#39;Removed rate and err of light curve of interest from the reference light curve.&#39;)
        
    # Y entirely within X
    if eminX &lt;= eminY and emaxX &gt;= emaxY: 
        lc_X.rate -= lc_Y.rate
        lc_X.err -= lc_Y.err
        lc_X.R = np.mean(lc_X.rate)
        print(&#39;Removed rate and err of the reference light curve from the light curve of interest.&#39;)

    return [lc_X,lc_Y]</code></pre>
</details>
</dd>
<dt id="API.time_lag"><code class="name flex">
<span>def <span class="ident">time_lag</span></span>(<span>lcs, m_v, percentage_limit=90)</span>
</code></dt>
<dd>
<div class="desc"><p>Find the time lag between two light curves. </p>
<h2 id="parameters">Parameters:</h2>
<p>lcs: class: list of two 'Lightcurve'-objects
The light curves to be used in the coherence computation.</p>
<p>m_v: np.ndarray
Array of number of segments to use, in increasing order.
format: m_v[m_1,m_2,&hellip;], where type(m_i) = int, preferably a power of 2, and m_1 &lt; m_2 etc.
</p>
<p>percentage_limit: float, optional, default: 90
Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.
co
Returns:</p>
<hr>
<p>xf_v: np.ndarray
Frequencies.</p>
<p>tau_v: np.ndarray
Time lag.</p>
<p>dtau_v: np.ndarray
Error in time lag.</p>
<p>num: int
The number of logarithmic frequency bins to create before plotting.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def time_lag(lcs,m_v,percentage_limit=90):
    &#34;&#34;&#34;
    Find the time lag between two light curves. 
    
    Parameters:
    -----------
    lcs: class: list of two &#39;Lightcurve&#39;-objects
                The light curves to be used in the coherence computation.
    
    m_v: np.ndarray
        Array of number of segments to use, in increasing order.
        format: m_v[m_1,m_2,...], where type(m_i) = int, preferably a power of 2, and m_1 &lt; m_2 etc.  
        
    percentage_limit: float, optional, default: 90
            Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.
    co
    Returns:
    --------
    xf_v: np.ndarray
        Frequencies.
    
    tau_v: np.ndarray
        Time lag.
    
    dtau_v: np.ndarray
        Error in time lag.
        
    num: int
        The number of logarithmic frequency bins to create before plotting.
    &#34;&#34;&#34;
    
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
    print(&#39;                           Computing the time lag...&#39;)
    print(&#39;---------------------------------------------------------------------------------------------------\n&#39;)
    start = timeit.default_timer()
        
    assert isinstance(lcs[0], lightcurve) and isinstance(lcs[1], lightcurve), &#39;The lcs-input does not contain two light curve objects.&#39;
    if len(m_v) &gt;= 2:
        for i in range(0,len(m_v)-1):
            assert m_v[i] &lt; m_v[i+1], &#39;Make sure that m_v[i] &lt; m_v[i+1] for all i&#39;
    
    xf_v = np.array([])
    tau_v = np.array([])
    dtau_v = np.array([])
    dt = lcs[0].dt
    
    freq_uplim = 1/(2*dt)
    for i in range(0,len(m_v)):
        m = m_v[i]
        print(&#39;Iteration {}) Computing using m = {} bins per segment, i.e. f in [{:.3f},{:.3f}]&#39;.format(i+1,m,1/(m*dt),freq_uplim))
        print(&#39;---------------------------------------------------------------------------------------------------&#39;)
        
        # Find cross spectra
        ps_v, C_v = cross_spec(lcs,m=m,percentage_limit=percentage_limit)

        # Coherence
        xf, gamma2, delta_gamma2, _ = coherence_noiseless(ps_v,m=m,C_v=C_v)
        
        K = np.size(C_v,axis=0) #number of segments 
        C = np.mean(C_v,axis=0) #mean cross spectra
        
        tau = np.angle(C)/(2*np.pi*xf)
        dtau = np.sqrt((1-gamma2)/(2*gamma2*K))/(2*np.pi*xf) 
        
        if freq_uplim != -1:
            print(&#39;Only use freq &lt; {:.3f} to avoid to much overlaping.&#39;.format(freq_uplim))
            tau = tau[xf &lt; freq_uplim]
            dtau = dtau[xf &lt; freq_uplim]
            xf = xf[xf &lt; freq_uplim]
        
        xf_v = np.append(xf_v,xf)
        tau_v = np.append(tau_v,tau)
        dtau_v = np.append(dtau_v,dtau)
        
        # If len(m) &gt;= 2, then we don&#39;t want the two computations to overlap too much... 
        # Now we make them overlap a factor 10 larger than the lowest frequency from the small m
        # E.g. if m = [2**8,2**17], dt = 0.002 --&gt; freq_uplim = 1.96*10, meaning that even though 
        # m=2**17 covers f\in[0.0038,249] we only look at f\in[0.0038,19.6]. The high freq interval was
        # taken care of by m = [2**8].
        freq_uplim = np.amin(xf_v)*10
        print(&#39;Time lag for m = {} computed. \n&#39;.format(m))
     
    time_taken = timeit.default_timer()-start
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
    print(&#39;                           Time lag found (in {:.2f} sec).&#39;.format(time_taken))
    print(&#39;---------------------------------------------------------------------------------------------------&#39;)
        
    return xf_v, tau_v, dtau_v</code></pre>
</details>
</dd>
<dt id="API.timer"><code class="name flex">
<span>def <span class="ident">timer</span></span>(<span>i, nr, start, clear=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Prints out the loop progress, and tries to estimate the time remaining.</p>
<h2 id="parameters">Parameters:</h2>
<p>i, nr: ints
The current loop, and totalt number of loops.</p>
<p>start: float
Start of the loop, as given by: timeit.default_timer(). </p>
<p>clear: boolean
Clear the output between loops.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def timer(i,nr,start,clear=True):
    &#34;&#34;&#34;
    Prints out the loop progress, and tries to estimate the time remaining.
    
    Parameters:
    -----------
    i, nr: ints
        The current loop, and totalt number of loops.
    
    start: float
        Start of the loop, as given by: timeit.default_timer(). 
    
    clear: boolean
        Clear the output between loops. 
    &#34;&#34;&#34;
    
    stop=timeit.default_timer()
    if (i/nr*100) &lt; 10:
        expected_time=&#34;Calculating...&#34;
    else:
        time_perc=timeit.default_timer()
        expected_time=np.round(((time_perc-start)/(i/nr))/60,2)
    if clear == True:
        clear_output(wait=True)
    
    print(&#34;Calculating the power spectra...&#34;)
    print(&#34;Current progress: {}%&#34;.format(np.round(i/nr*100,2)))
    print(&#34;Current run time: &#34;,np.round((stop-start)/60,2),&#34; minutes&#34;)
    print(&#34;Expected run time: &#34;,expected_time,&#34; minutes&#34;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="API.PowerSpectrum"><code class="flex name class">
<span>class <span class="ident">PowerSpectrum</span></span>
<span>(</span><span>lc, m=8192, normalization='rms', noise='Gaussian', percentage_limit=90, timer_on=True, return_noise_wo_sub=False, save_all=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds power spectra by splitting the light curve into segments. A power spectra for
each segment is computed and the final power spectra is the average over all these.</p>
<h2 id="parameters">Parameters:</h2>
<p>lc: class: 'Lightcurve'-object
The light curve data to be Fourier-transformed.</p>
<p>m: int
Number of time bins per segment. </p>
<p>normalization: {'rms' (Miyamoto), 'abs', 'Leahy', or 'none'}, optional, default: 'rms'.
What normalization to use, see Vaughan(2003, MNRAS 345).</p>
<p>noise: {'Poisson','Gaussian'}, optional, default: 'Gaussian'.
For a light curve with 'Poisson'/'Gaussian' errors. </p>
<p>percentage_limit: float, optional, default: 90
Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.</p>
<p>timer_on: boolean, optional, default: True
Prints out progress during computation.</p>
<p>return_noise_wo_sub: boolean, optional, default: False
If True, noise is returned but not subtracted from powspec.
If False, returned and subtracted.</p>
<p>save_all: boolean, optional, default: False
If True, save rate_seg, err_seg, R_seg. These are needed for covariance-computation.</p>
<h2 id="attributes">Attributes:</h2>
<p>m: int
Number of time bins per segment. </p>
<p>channels: string
Channels used during the observation.</p>
<p>minchan, maxchan: ints
Min and max channels used.</p>
<p>xf: np.ndarray
The Fourier frequencies. </p>
<p>df: float
Frequency resolution.</p>
<p>fft_rate: np.ndarray
Power spectra. Fast fourier transformation of, and average over all, rate_seg.</p>
<p>fft_rate_v: list of np.ndarrays
Power spectra for each segment.</p>
<p>averagePnoise: float
Average noise power for whole light curve.</p>
<p>Pnoise_v: list of floats
Noise power for each segment.</p>
<p>Fvar: float
Fractional variance amplitude (=rms) computed from the power spectra by integration.</p>
<p>middle_of_log_bins: np.ndarray
Logarithmic mid point </p>
<p>middle_of_log_bins: np.ndarray
The logarithmic midpoint of each log-bin, computed as: 10*<em>(1/2</em>(np.log10(log_bins[i])+np.log10(log_bins[i+1])))</p>
<p>fPf: np.ndarray
Power spectra multiplied with f and re-binned logarithmically.</p>
<p>fPferror: np.ndarray
The standard deviation of all points within a log-bin for the fPf-vector.</p>
<p>Pf: np.ndarray
Power spectra re-binned logarithmically.</p>
<p>Pferror: np.ndarray
The standard deviation of all points within a log-bin for the Pf-vector.
</p>
<p>In addition, if save_all:
rate_seg: np.ndarray
All segments' rate-vectors.</p>
<pre><code>err_seg: np.ndarray
    All segments' rate-error-vectors.

R_seg: np.ndarray
    All segments' mean count rates.
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PowerSpectrum():
    &#34;&#34;&#34;
    Finds power spectra by splitting the light curve into segments. A power spectra for 
    each segment is computed and the final power spectra is the average over all these.

    Parameters:
    -----------
    lc: class: &#39;Lightcurve&#39;-object
        The light curve data to be Fourier-transformed.

    m: int
        Number of time bins per segment. 

    normalization: {&#39;rms&#39; (Miyamoto), &#39;abs&#39;, &#39;Leahy&#39;, or &#39;none&#39;}, optional, default: &#39;rms&#39;.
        What normalization to use, see Vaughan(2003, MNRAS 345).

    noise: {&#39;Poisson&#39;,&#39;Gaussian&#39;}, optional, default: &#39;Gaussian&#39;.
        For a light curve with &#39;Poisson&#39;/&#39;Gaussian&#39; errors. 

    percentage_limit: float, optional, default: 90
        Lower limit for percentage of time bins having to be filled, i.e. without gaps, for that segment to be used.

    timer_on: boolean, optional, default: True
        Prints out progress during computation.
        
    return_noise_wo_sub: boolean, optional, default: False
        If True, noise is returned but not subtracted from powspec.
        If False, returned and subtracted.
        
    save_all: boolean, optional, default: False
        If True, save rate_seg, err_seg, R_seg. These are needed for covariance-computation.
    
    Attributes:
    -----------
    m: int
        Number of time bins per segment. 
    
    channels: string
        Channels used during the observation.
    
    minchan, maxchan: ints
        Min and max channels used.
    
    xf: np.ndarray
        The Fourier frequencies. 
    
    df: float
        Frequency resolution.
    
    fft_rate: np.ndarray
        Power spectra. Fast fourier transformation of, and average over all, rate_seg.
    
    fft_rate_v: list of np.ndarrays
        Power spectra for each segment.
        
    averagePnoise: float
        Average noise power for whole light curve.
    
    Pnoise_v: list of floats
        Noise power for each segment.
    
    Fvar: float
        Fractional variance amplitude (=rms) computed from the power spectra by integration.
    
    middle_of_log_bins: np.ndarray
        Logarithmic mid point 
    
    middle_of_log_bins: np.ndarray 
        The logarithmic midpoint of each log-bin, computed as: 10**(1/2*(np.log10(log_bins[i])+np.log10(log_bins[i+1])))
    
    fPf: np.ndarray
        Power spectra multiplied with f and re-binned logarithmically.
    
    fPferror: np.ndarray
        The standard deviation of all points within a log-bin for the fPf-vector.
    
    Pf: np.ndarray
        Power spectra re-binned logarithmically.
    
    Pferror: np.ndarray
        The standard deviation of all points within a log-bin for the Pf-vector.      
        
    In addition, if save_all:
        rate_seg: np.ndarray
            All segments&#39; rate-vectors.
            
        err_seg: np.ndarray
            All segments&#39; rate-error-vectors.
            
        R_seg: np.ndarray
            All segments&#39; mean count rates.
    &#34;&#34;&#34;
    
    def __init__(self, lc, m=2**13, normalization=&#39;rms&#39;, noise=&#39;Gaussian&#39;,                  percentage_limit=90, timer_on=True, return_noise_wo_sub=False, save_all = False):
        self.m = m
        self.channels, self.minchan, self.maxchan = lc.channels, lc.minchan, lc.maxchan
        self.xf, self.fft_rate, self.fft_rate_v, self.Pnoise_v = self.powspec(lc, normalization, noise, percentage_limit, timer_on, return_noise_wo_sub, save_all)
        self.df = self.xf[1]-self.xf[0]
        self.averagePnoise = np.mean(self.Pnoise_v)
        self.Fvar = self.Fvar_from_ps()
        self.middle_of_log_bins, self.fPf, self.fPferror, self.Pf, self.Pferror = self.rebin(init=True)
    
    def powspec(self,lc,normalization,noise,percentage_limit,timer_on,return_noise_wo_sub, save_all):
        print(&#39;Computing the power spectra using {} bins per segment, normalization &#34;{}&#34;, and noise dist &#34;{}&#34;...&#39;.format(self.m,normalization,noise))
        
        # Useful parameters
        #dt = (lc.t[-1]-lc.t[0])/lc.N
        dt = lc.dt
        K = int(np.floor(lc.N/self.m)) #num_of_line_seg

        # Average over time segments 
        fft_rate_v = []
        P_noise_v = []
        rate_seg_v = []
        err_seg_v = []
        R_seg_v = []
        num_discarded_segments = 0
        start = timeit.default_timer()
        perc_temp_v = []
        for i in range(0,K):

            if timer_on:
                timer(i,K-1,start,clear=True)

            # Pick out one line segment (ls)
            t_seg, rate_seg, err_seg, N_gamma, R_seg, T_seg = lc.extract_seg(self.m,n=i,to_print=False,to_plot=False)

            if save_all:
                rate_seg_v.append(rate_seg)
                err_seg_v.append(err_seg)
                R_seg_v.append(R_seg)
            
            # Check gaps:mu
            ## Old version: if abs(dt*m - T_seg) &lt; 1e1: 
            perc_temp = percentage_of_filled_time_bins(t_seg,dt,to_return=True)
            if timer_on:
                print(&#39;Percentage of filled time bins (segment {}): {:.2f}&#39;.format(i,perc_temp))

            if perc_temp &gt; percentage_limit: # if True, then there is no large gap in the segment
                # Perform FFT to find Power spectra for one seg
                xf, fft_rate_normalized, P_noise = self.fft_seg(dt,t_seg,rate_seg,err_seg,N_gamma,R_seg,noise=noise,normalization=normalization,return_noise_wo_sub=return_noise_wo_sub)
                fft_rate_v.append(fft_rate_normalized)
                P_noise_v.append(P_noise)
            else:
                perc_temp_v.append([i,perc_temp])
                num_discarded_segments += 1

        print(&#39;{} of {} segments were disregarded due to lower percentage limit set to {:.2f}%:&#39;.format(num_discarded_segments,K,percentage_limit))
        print(&#39;\n&#39;.join(&#39;Seg nr = {}, Percentage of filled time bins = {:.2f}&#39;.format(k[1][0],k[1][1]) for k in enumerate(perc_temp_v)))
        print(&#39;Power spectra done! \n&#39;)

        if save_all:
            setattr(self, &#39;rate_seg&#39;, rate_seg_v)
            setattr(self, &#39;err_seg&#39;, err_seg_v)
            setattr(self, &#39;R_seg&#39;, R_seg_v)
        
        fft_rate_mean = np.mean(fft_rate_v,axis=0)
        return xf, fft_rate_mean, fft_rate_v, P_noise_v

    def fft_seg(self,dt,t_seg,rate_seg,err_seg,N_gamma,R_seg,normalization=&#39;rms&#39;,noise=&#39;Gaussian&#39;,t_d=0,return_noise_wo_sub=False,to_plot=True):
        &#34;&#34;&#34;
        Perform discrete FFT (fast-Fourier transform) on a light curve (lc) segment.

        Parameters: 
        -----------
        dt: float
            Time resolution.
        
        t_seg: np.ndarray
            The segment&#39;s time-vector in seconds.

        rate_seg: np.ndarray
            The segment&#39;s count rate(=flux)-vector in counts/seconds.

        N_gamma: int
            Number of counted photons in the segment.

        R_seg: float
            Mean count rate in the segment.

        normalization: {&#39;rms&#39; (Miyamoto), &#39;abs&#39;, &#39;Leahy&#39;, or &#39;none&#39;}, optional, default: &#39;rms&#39;.
            What normalization to use, see Vaughan(2003, MNRAS 345).

        noise: {&#39;Poisson&#39;,&#39;Gaussian&#39;}, optional, default: &#39;Gaussian&#39;.
            For a light curve with &#34;noise&#34; errors.

        t_d: float (not implemented yet)
            Dead-time of the instrument. 

        Returns:
        ------------
        xf: np.array
            The Fourier frequencies. 

        fft_rate_noise_subtracted: np.array
            Fast fourier transformation of rate_seg. Noise subtraction has been made.
        &#34;&#34;&#34;
        
        # Perform FFT
        xf = np.array(fftfreq(self.m, dt)[1:self.m//2]) #only want positive freq
        fft_rate = fft(rate_seg)[1:self.m//2]

        # Normalize
        fft_rate = self.normalize_power(fft_rate,R_seg,dt,normalization)

        # Remove Noise
        P_noise = self.noise_power(dt,err_seg,R_seg,noise,normalization)
        if not return_noise_wo_sub:
            fft_rate = np.array([x-P_noise for x in fft_rate])

        return xf, fft_rate, P_noise

    def noise_power(self,dt,err_seg,R,noise=&#39;Gaussian&#39;,normalization=&#39;rms&#39;,B_noise=0,t_d=0):
        &#34;&#34;&#34;
        Calculates and returns the Poisson noise power. See Appendix A of Vaughan (2003, MNRAS 345). Does not take dead time into account.

        Parameters:
        -----------
        
        R: float
            Mean count rate in the segment.

        noise: {&#39;Poisson&#39;,&#39;Gaussian&#39;}, optional, default: &#39;Gaussian&#39;.
            For a light curve with &#34;noise&#34; errors.

        normalization: {&#39;rms&#39; (Miyamoto), &#39;abs&#39;, &#39;Leahy&#39;, or &#39;none&#39;}, optional, default: &#39;rms&#39;.
            What normalization to use, see Vaughan(2003, MNRAS 345).

        B_noise: float, optional, default: 0
            Background noise level to be used in the formula for the noise, see Eq. (A2) of Vaughan (2003, MNRAS 345).
            B_noise = 2 means that B=np.sqrt(R).

        t_d: float (not implemented)
            Dead time of instrument in seconds. 

        Returns:
        --------
        P_noise: float
            Poisson noise power.
        &#34;&#34;&#34;

        dT_samp_over_dT_bin = 1
        
        if noise == &#39;Poisson&#39;: 
            if normalization == &#39;rms&#39;:

                if B_noise &lt; 1:
                    B=B_noise*R #B_noise=0.1 works for &#34;first&#34; CYG X-1 data
                elif B_noise == 2:
                    B=np.sqrt(R)

                P_noise = 2*(R+B)/R**2*dT_samp_over_dT_bin
            elif normalization == &#39;Leahy&#39;: 
                P_noise = 2*(R+B)/R*dT_samp_over_dT_bin #should simply be =2 in most cases
            elif normalization == &#39;abs&#39;: 
                P_noise = 2*(R+B)/R**2*dT_samp_over_dT_bin
            else: #Poisson noise but w/o normalization
                P_noise = (R+B)*self.m/dt* dT_samp_over_dT_bin
        
        elif noise == &#39;Gaussian&#39;:
            if normalization == &#39;rms&#39;:
                P_noise = dT_samp_over_dT_bin*np.mean(err_seg**2)/(R**2*1/(2*dt))
            elif normalization == &#39;Leahy&#39; or normalization == &#39;abs&#39;:
                print(&#39;Sorry, cannot perform that normalization! Not implemented...!&#39;)
            else: #Gaussian noise but w/o normalization
                P_noise = np.mean(np.abs(err_seg)**2)*self.m
        
        else: #noise == &#39;None&#39;:
            P_noise = 0
            
        return P_noise

    def normalize_power(self,fft_rate,R,dt,normalization=&#39;rms&#39;):
        &#34;&#34;&#34;
        Normalize the power spectra. 

        Parameters:
        -----------
        fft_rate: np.array
            Fast fourier transformation of light curve. To be normalized.

        R: float
            Mean count rate in the segment.

        dt: float
            Time resoltuion in observation.

        normalization: {&#39;rms&#39; (Miyamoto), &#39;abs&#39;, &#39;Leahy&#39;, or &#39;none&#39;}, optional, default: &#39;rms&#39;.
            What normalization to use, see Vaughan(2003, MNRAS 345).

        Returns:
        ------------
        normalized_power: np.ndarray
            The normalized power. 
        &#34;&#34;&#34;
        
        if normalization == &#39;rms&#39;:
            return np.array([2*dt/(R**2*self.m)*np.abs(x)**2 for x in fft_rate])
        elif normalization == &#39;Leahy&#39;:
            return np.array([2*dt/(R*self.m)*np.abs(x)**2 for x in fft_rate])    
        elif normalization == &#39;abs&#39;:      
            return np.array([2*dt/self.m*np.abs(x)**2 for x in fft_rate])    
        else: # no-normalization
            return np.array(fft_rate)
    
    def Fvar_from_ps(self):
        &#34;&#34;&#34;
        Calculate the fractional root mean square (rms) variability amplitude by 
        integrating the power spectra over the full frequency interval.
        
        If wants the Fvar in a smaller freq-band, first use remove_freq() to set power to 
        zero outside given freq band and then use Fvar_from_ps.

        Returns:
        ------------
        F_var: float
            The fractional root mean square (rms) variability amplitude.
        &#34;&#34;&#34;

        return Fvar_from_ps(self.xf,self.fft_rate)

    def rebin(self, num=50, init=False):
        &#34;&#34;&#34;
        Logarithmic rebinning.
        
        Parameters:
        -----------
        num: int
            The number of logarithmic frequency bins to create.
        
        init: boolean, optional, default: False
            If False, creates the &#34;middle_of_log_bins, fPf, fPferror, Pf, Pferror&#34; for the first time with num=50 bins.
            If True, updates these attributes, using num bins.
            
        Returns:
        --------
        middle_of_log_bins, fPf, fPferror, Pf, Pferror: np.ndarrays
            See class documentation. 
        &#34;&#34;&#34;
        
        middle_of_log_bins, fPf, fPferror = log_rebin(self.xf,self.fft_rate*self.xf,num=num)
        middle_of_log_bins, Pf, Pferror = log_rebin(self.xf,self.fft_rate,num=num)
        
        if init:
            return middle_of_log_bins, fPf, fPferror, Pf, Pferror
        else:
            setattr(self, &#39;middle_of_log_bins&#39;, middle_of_log_bins)
            setattr(self, &#39;fPf&#39;, fPf)
            setattr(self, &#39;fPferror&#39;, fPferror)
            setattr(self, &#39;Pf&#39;, Pf)
            setattr(self, &#39;Pferror&#39;, Pferror)
    
    def plot(self,to_plot=&#39;fPf&#39;,w=4,with_noise=False):
        &#34;&#34;&#34;
        Plot Power Spectra times Freq vs Freq. (fPf) or Power Spectra vs Freq. (Pf).
        
        Parameters:
        -----------
        to_plot: {&#39;fPf&#39;,&#39;Pf&#39;}   
            What to plot.
        &#34;&#34;&#34;
        
        ax = standard_plot(w=w)
        if to_plot == &#39;fPf&#39;:
            plt.loglog(self.middle_of_log_bins,self.fPf,&#39;.-&#39;,label=&#39;Channels: {}&#39;.format(self.channels),markersize=7)
            plt.ylabel(&#39;$f \cdot P_f$&#39;) # P_f in [(RMS/Average)$^2$/Hz]
            if with_noise:
                plt.loglog(self.xf,self.xf*self.averagePnoise,label=&#39;Average Noise Power&#39;,color=&#39;k&#39;)
        elif to_plot == &#39;Pf&#39;:
            plt.loglog(self.middle_of_log_bins,self.Pf,&#39;.-&#39;,label=&#39;Channels: {}&#39;.format(self.channels),markersize=7)
            plt.ylabel(&#39;$P_f$&#39;) 
            if with_noise:
                ax.axhline(self.averagePnoise,label=&#39;Average Noise Power&#39;,color=&#39;k&#39;)
        plt.xlabel(&#39;Hz&#39;)
        plt.legend()
        plt.show()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="API.PowerSpectrum.Fvar_from_ps"><code class="name flex">
<span>def <span class="ident">Fvar_from_ps</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the fractional root mean square (rms) variability amplitude by
integrating the power spectra over the full frequency interval.</p>
<p>If wants the Fvar in a smaller freq-band, first use remove_freq() to set power to
zero outside given freq band and then use Fvar_from_ps.</p>
<h2 id="returns">Returns:</h2>
<p>F_var: float
The fractional root mean square (rms) variability amplitude.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Fvar_from_ps(self):
    &#34;&#34;&#34;
    Calculate the fractional root mean square (rms) variability amplitude by 
    integrating the power spectra over the full frequency interval.
    
    If wants the Fvar in a smaller freq-band, first use remove_freq() to set power to 
    zero outside given freq band and then use Fvar_from_ps.

    Returns:
    ------------
    F_var: float
        The fractional root mean square (rms) variability amplitude.
    &#34;&#34;&#34;

    return Fvar_from_ps(self.xf,self.fft_rate)</code></pre>
</details>
</dd>
<dt id="API.PowerSpectrum.fft_seg"><code class="name flex">
<span>def <span class="ident">fft_seg</span></span>(<span>self, dt, t_seg, rate_seg, err_seg, N_gamma, R_seg, normalization='rms', noise='Gaussian', t_d=0, return_noise_wo_sub=False, to_plot=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform discrete FFT (fast-Fourier transform) on a light curve (lc) segment.</p>
<h2 id="parameters">Parameters:</h2>
<p>dt: float
Time resolution.</p>
<p>t_seg: np.ndarray
The segment's time-vector in seconds.</p>
<p>rate_seg: np.ndarray
The segment's count rate(=flux)-vector in counts/seconds.</p>
<p>N_gamma: int
Number of counted photons in the segment.</p>
<p>R_seg: float
Mean count rate in the segment.</p>
<p>normalization: {'rms' (Miyamoto), 'abs', 'Leahy', or 'none'}, optional, default: 'rms'.
What normalization to use, see Vaughan(2003, MNRAS 345).</p>
<p>noise: {'Poisson','Gaussian'}, optional, default: 'Gaussian'.
For a light curve with "noise" errors.</p>
<p>t_d: float (not implemented yet)
Dead-time of the instrument. </p>
<h2 id="returns">Returns:</h2>
<p>xf: np.array
The Fourier frequencies. </p>
<p>fft_rate_noise_subtracted: np.array
Fast fourier transformation of rate_seg. Noise subtraction has been made.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fft_seg(self,dt,t_seg,rate_seg,err_seg,N_gamma,R_seg,normalization=&#39;rms&#39;,noise=&#39;Gaussian&#39;,t_d=0,return_noise_wo_sub=False,to_plot=True):
    &#34;&#34;&#34;
    Perform discrete FFT (fast-Fourier transform) on a light curve (lc) segment.

    Parameters: 
    -----------
    dt: float
        Time resolution.
    
    t_seg: np.ndarray
        The segment&#39;s time-vector in seconds.

    rate_seg: np.ndarray
        The segment&#39;s count rate(=flux)-vector in counts/seconds.

    N_gamma: int
        Number of counted photons in the segment.

    R_seg: float
        Mean count rate in the segment.

    normalization: {&#39;rms&#39; (Miyamoto), &#39;abs&#39;, &#39;Leahy&#39;, or &#39;none&#39;}, optional, default: &#39;rms&#39;.
        What normalization to use, see Vaughan(2003, MNRAS 345).

    noise: {&#39;Poisson&#39;,&#39;Gaussian&#39;}, optional, default: &#39;Gaussian&#39;.
        For a light curve with &#34;noise&#34; errors.

    t_d: float (not implemented yet)
        Dead-time of the instrument. 

    Returns:
    ------------
    xf: np.array
        The Fourier frequencies. 

    fft_rate_noise_subtracted: np.array
        Fast fourier transformation of rate_seg. Noise subtraction has been made.
    &#34;&#34;&#34;
    
    # Perform FFT
    xf = np.array(fftfreq(self.m, dt)[1:self.m//2]) #only want positive freq
    fft_rate = fft(rate_seg)[1:self.m//2]

    # Normalize
    fft_rate = self.normalize_power(fft_rate,R_seg,dt,normalization)

    # Remove Noise
    P_noise = self.noise_power(dt,err_seg,R_seg,noise,normalization)
    if not return_noise_wo_sub:
        fft_rate = np.array([x-P_noise for x in fft_rate])

    return xf, fft_rate, P_noise</code></pre>
</details>
</dd>
<dt id="API.PowerSpectrum.noise_power"><code class="name flex">
<span>def <span class="ident">noise_power</span></span>(<span>self, dt, err_seg, R, noise='Gaussian', normalization='rms', B_noise=0, t_d=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates and returns the Poisson noise power. See Appendix A of Vaughan (2003, MNRAS 345). Does not take dead time into account.</p>
<h2 id="parameters">Parameters:</h2>
<p>R: float
Mean count rate in the segment.</p>
<p>noise: {'Poisson','Gaussian'}, optional, default: 'Gaussian'.
For a light curve with "noise" errors.</p>
<p>normalization: {'rms' (Miyamoto), 'abs', 'Leahy', or 'none'}, optional, default: 'rms'.
What normalization to use, see Vaughan(2003, MNRAS 345).</p>
<p>B_noise: float, optional, default: 0
Background noise level to be used in the formula for the noise, see Eq. (A2) of Vaughan (2003, MNRAS 345).
B_noise = 2 means that B=np.sqrt(R).</p>
<p>t_d: float (not implemented)
Dead time of instrument in seconds. </p>
<h2 id="returns">Returns:</h2>
<p>P_noise: float
Poisson noise power.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def noise_power(self,dt,err_seg,R,noise=&#39;Gaussian&#39;,normalization=&#39;rms&#39;,B_noise=0,t_d=0):
    &#34;&#34;&#34;
    Calculates and returns the Poisson noise power. See Appendix A of Vaughan (2003, MNRAS 345). Does not take dead time into account.

    Parameters:
    -----------
    
    R: float
        Mean count rate in the segment.

    noise: {&#39;Poisson&#39;,&#39;Gaussian&#39;}, optional, default: &#39;Gaussian&#39;.
        For a light curve with &#34;noise&#34; errors.

    normalization: {&#39;rms&#39; (Miyamoto), &#39;abs&#39;, &#39;Leahy&#39;, or &#39;none&#39;}, optional, default: &#39;rms&#39;.
        What normalization to use, see Vaughan(2003, MNRAS 345).

    B_noise: float, optional, default: 0
        Background noise level to be used in the formula for the noise, see Eq. (A2) of Vaughan (2003, MNRAS 345).
        B_noise = 2 means that B=np.sqrt(R).

    t_d: float (not implemented)
        Dead time of instrument in seconds. 

    Returns:
    --------
    P_noise: float
        Poisson noise power.
    &#34;&#34;&#34;

    dT_samp_over_dT_bin = 1
    
    if noise == &#39;Poisson&#39;: 
        if normalization == &#39;rms&#39;:

            if B_noise &lt; 1:
                B=B_noise*R #B_noise=0.1 works for &#34;first&#34; CYG X-1 data
            elif B_noise == 2:
                B=np.sqrt(R)

            P_noise = 2*(R+B)/R**2*dT_samp_over_dT_bin
        elif normalization == &#39;Leahy&#39;: 
            P_noise = 2*(R+B)/R*dT_samp_over_dT_bin #should simply be =2 in most cases
        elif normalization == &#39;abs&#39;: 
            P_noise = 2*(R+B)/R**2*dT_samp_over_dT_bin
        else: #Poisson noise but w/o normalization
            P_noise = (R+B)*self.m/dt* dT_samp_over_dT_bin
    
    elif noise == &#39;Gaussian&#39;:
        if normalization == &#39;rms&#39;:
            P_noise = dT_samp_over_dT_bin*np.mean(err_seg**2)/(R**2*1/(2*dt))
        elif normalization == &#39;Leahy&#39; or normalization == &#39;abs&#39;:
            print(&#39;Sorry, cannot perform that normalization! Not implemented...!&#39;)
        else: #Gaussian noise but w/o normalization
            P_noise = np.mean(np.abs(err_seg)**2)*self.m
    
    else: #noise == &#39;None&#39;:
        P_noise = 0
        
    return P_noise</code></pre>
</details>
</dd>
<dt id="API.PowerSpectrum.normalize_power"><code class="name flex">
<span>def <span class="ident">normalize_power</span></span>(<span>self, fft_rate, R, dt, normalization='rms')</span>
</code></dt>
<dd>
<div class="desc"><p>Normalize the power spectra. </p>
<h2 id="parameters">Parameters:</h2>
<p>fft_rate: np.array
Fast fourier transformation of light curve. To be normalized.</p>
<p>R: float
Mean count rate in the segment.</p>
<p>dt: float
Time resoltuion in observation.</p>
<p>normalization: {'rms' (Miyamoto), 'abs', 'Leahy', or 'none'}, optional, default: 'rms'.
What normalization to use, see Vaughan(2003, MNRAS 345).</p>
<h2 id="returns">Returns:</h2>
<p>normalized_power: np.ndarray
The normalized power.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize_power(self,fft_rate,R,dt,normalization=&#39;rms&#39;):
    &#34;&#34;&#34;
    Normalize the power spectra. 

    Parameters:
    -----------
    fft_rate: np.array
        Fast fourier transformation of light curve. To be normalized.

    R: float
        Mean count rate in the segment.

    dt: float
        Time resoltuion in observation.

    normalization: {&#39;rms&#39; (Miyamoto), &#39;abs&#39;, &#39;Leahy&#39;, or &#39;none&#39;}, optional, default: &#39;rms&#39;.
        What normalization to use, see Vaughan(2003, MNRAS 345).

    Returns:
    ------------
    normalized_power: np.ndarray
        The normalized power. 
    &#34;&#34;&#34;
    
    if normalization == &#39;rms&#39;:
        return np.array([2*dt/(R**2*self.m)*np.abs(x)**2 for x in fft_rate])
    elif normalization == &#39;Leahy&#39;:
        return np.array([2*dt/(R*self.m)*np.abs(x)**2 for x in fft_rate])    
    elif normalization == &#39;abs&#39;:      
        return np.array([2*dt/self.m*np.abs(x)**2 for x in fft_rate])    
    else: # no-normalization
        return np.array(fft_rate)</code></pre>
</details>
</dd>
<dt id="API.PowerSpectrum.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, to_plot='fPf', w=4, with_noise=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot Power Spectra times Freq vs Freq. (fPf) or Power Spectra vs Freq. (Pf).</p>
<h2 id="parameters">Parameters:</h2>
<p>to_plot: {'fPf','Pf'} <br>
What to plot.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(self,to_plot=&#39;fPf&#39;,w=4,with_noise=False):
    &#34;&#34;&#34;
    Plot Power Spectra times Freq vs Freq. (fPf) or Power Spectra vs Freq. (Pf).
    
    Parameters:
    -----------
    to_plot: {&#39;fPf&#39;,&#39;Pf&#39;}   
        What to plot.
    &#34;&#34;&#34;
    
    ax = standard_plot(w=w)
    if to_plot == &#39;fPf&#39;:
        plt.loglog(self.middle_of_log_bins,self.fPf,&#39;.-&#39;,label=&#39;Channels: {}&#39;.format(self.channels),markersize=7)
        plt.ylabel(&#39;$f \cdot P_f$&#39;) # P_f in [(RMS/Average)$^2$/Hz]
        if with_noise:
            plt.loglog(self.xf,self.xf*self.averagePnoise,label=&#39;Average Noise Power&#39;,color=&#39;k&#39;)
    elif to_plot == &#39;Pf&#39;:
        plt.loglog(self.middle_of_log_bins,self.Pf,&#39;.-&#39;,label=&#39;Channels: {}&#39;.format(self.channels),markersize=7)
        plt.ylabel(&#39;$P_f$&#39;) 
        if with_noise:
            ax.axhline(self.averagePnoise,label=&#39;Average Noise Power&#39;,color=&#39;k&#39;)
    plt.xlabel(&#39;Hz&#39;)
    plt.legend()
    plt.show()</code></pre>
</details>
</dd>
<dt id="API.PowerSpectrum.powspec"><code class="name flex">
<span>def <span class="ident">powspec</span></span>(<span>self, lc, normalization, noise, percentage_limit, timer_on, return_noise_wo_sub, save_all)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def powspec(self,lc,normalization,noise,percentage_limit,timer_on,return_noise_wo_sub, save_all):
    print(&#39;Computing the power spectra using {} bins per segment, normalization &#34;{}&#34;, and noise dist &#34;{}&#34;...&#39;.format(self.m,normalization,noise))
    
    # Useful parameters
    #dt = (lc.t[-1]-lc.t[0])/lc.N
    dt = lc.dt
    K = int(np.floor(lc.N/self.m)) #num_of_line_seg

    # Average over time segments 
    fft_rate_v = []
    P_noise_v = []
    rate_seg_v = []
    err_seg_v = []
    R_seg_v = []
    num_discarded_segments = 0
    start = timeit.default_timer()
    perc_temp_v = []
    for i in range(0,K):

        if timer_on:
            timer(i,K-1,start,clear=True)

        # Pick out one line segment (ls)
        t_seg, rate_seg, err_seg, N_gamma, R_seg, T_seg = lc.extract_seg(self.m,n=i,to_print=False,to_plot=False)

        if save_all:
            rate_seg_v.append(rate_seg)
            err_seg_v.append(err_seg)
            R_seg_v.append(R_seg)
        
        # Check gaps:mu
        ## Old version: if abs(dt*m - T_seg) &lt; 1e1: 
        perc_temp = percentage_of_filled_time_bins(t_seg,dt,to_return=True)
        if timer_on:
            print(&#39;Percentage of filled time bins (segment {}): {:.2f}&#39;.format(i,perc_temp))

        if perc_temp &gt; percentage_limit: # if True, then there is no large gap in the segment
            # Perform FFT to find Power spectra for one seg
            xf, fft_rate_normalized, P_noise = self.fft_seg(dt,t_seg,rate_seg,err_seg,N_gamma,R_seg,noise=noise,normalization=normalization,return_noise_wo_sub=return_noise_wo_sub)
            fft_rate_v.append(fft_rate_normalized)
            P_noise_v.append(P_noise)
        else:
            perc_temp_v.append([i,perc_temp])
            num_discarded_segments += 1

    print(&#39;{} of {} segments were disregarded due to lower percentage limit set to {:.2f}%:&#39;.format(num_discarded_segments,K,percentage_limit))
    print(&#39;\n&#39;.join(&#39;Seg nr = {}, Percentage of filled time bins = {:.2f}&#39;.format(k[1][0],k[1][1]) for k in enumerate(perc_temp_v)))
    print(&#39;Power spectra done! \n&#39;)

    if save_all:
        setattr(self, &#39;rate_seg&#39;, rate_seg_v)
        setattr(self, &#39;err_seg&#39;, err_seg_v)
        setattr(self, &#39;R_seg&#39;, R_seg_v)
    
    fft_rate_mean = np.mean(fft_rate_v,axis=0)
    return xf, fft_rate_mean, fft_rate_v, P_noise_v</code></pre>
</details>
</dd>
<dt id="API.PowerSpectrum.rebin"><code class="name flex">
<span>def <span class="ident">rebin</span></span>(<span>self, num=50, init=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Logarithmic rebinning.</p>
<h2 id="parameters">Parameters:</h2>
<p>num: int
The number of logarithmic frequency bins to create.</p>
<p>init: boolean, optional, default: False
If False, creates the "middle_of_log_bins, fPf, fPferror, Pf, Pferror" for the first time with num=50 bins.
If True, updates these attributes, using num bins.</p>
<h2 id="returns">Returns:</h2>
<p>middle_of_log_bins, fPf, fPferror, Pf, Pferror: np.ndarrays
See class documentation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rebin(self, num=50, init=False):
    &#34;&#34;&#34;
    Logarithmic rebinning.
    
    Parameters:
    -----------
    num: int
        The number of logarithmic frequency bins to create.
    
    init: boolean, optional, default: False
        If False, creates the &#34;middle_of_log_bins, fPf, fPferror, Pf, Pferror&#34; for the first time with num=50 bins.
        If True, updates these attributes, using num bins.
        
    Returns:
    --------
    middle_of_log_bins, fPf, fPferror, Pf, Pferror: np.ndarrays
        See class documentation. 
    &#34;&#34;&#34;
    
    middle_of_log_bins, fPf, fPferror = log_rebin(self.xf,self.fft_rate*self.xf,num=num)
    middle_of_log_bins, Pf, Pferror = log_rebin(self.xf,self.fft_rate,num=num)
    
    if init:
        return middle_of_log_bins, fPf, fPferror, Pf, Pferror
    else:
        setattr(self, &#39;middle_of_log_bins&#39;, middle_of_log_bins)
        setattr(self, &#39;fPf&#39;, fPf)
        setattr(self, &#39;fPferror&#39;, fPferror)
        setattr(self, &#39;Pf&#39;, Pf)
        setattr(self, &#39;Pferror&#39;, Pferror)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="API.lightcurve"><code class="flex name class">
<span>class <span class="ident">lightcurve</span></span>
<span>(</span><span>filename, keywords=[], p=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Make a light curve object from fits-file.</p>
<h2 id="parameters">Parameters:</h2>
<p>filename: string
The path to the .fits-file.</p>
<p>keywords: list of strings
Keywords (and corresponding values) apart from the ones mentioned below to return.
<em>All data-keys are always automatically returned (for a lightcurve, these are: {"TIME", "RATE", "ERROR"}).
</em>Note also that the header-keys {"CONTENT","OBJECT","CPIX","MINCHAN","MAXCHAN"} are returned if they exist. </p>
<p>p: boolean
If True, print the headers of the fits-file.</p>
<h2 id="attributes">Attributes:</h2>
<p>t: np.ndarray
Time vector (in seconds).</p>
<p>rate: np.ndarray
Rate vector (in counts/s).</p>
<p>err: np.ndarray
Error vector in rate (in counts/s).</p>
<p>object: string
Object for which we have constructed the lightcurve object.</p>
<p>dt: float
Time resolution (in seconds).</p>
<p>R: np.float
Mean count rate.</p>
<p>N: int
Number of data points in observation.</p>
<p>Fvar: float
Fractional variance amplitude (=rms) computed from the light curve.</p>
<p>channels: string
Channels used during the observation.</p>
<p>minchan, maxchan: ints
Min and max channels used.</p>
<h1 id="after-having-called-lcenergieschannel_to_kev">After having called lc.energies(channel_to_kev):</h1>
<p>deltaE: float
Energy range between min and max channels used.</p>
<p>E_mean: float
Energy mean over the range between min and max channels used.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class lightcurve:
    &#34;&#34;&#34;
    Make a light curve object from fits-file.
    
    Parameters:
    -----------
    filename: string
        The path to the .fits-file.
        
    keywords: list of strings
        Keywords (and corresponding values) apart from the ones mentioned below to return. 
        *All data-keys are always automatically returned (for a lightcurve, these are: {&#34;TIME&#34;, &#34;RATE&#34;, &#34;ERROR&#34;}).
        *Note also that the header-keys {&#34;CONTENT&#34;,&#34;OBJECT&#34;,&#34;CPIX&#34;,&#34;MINCHAN&#34;,&#34;MAXCHAN&#34;} are returned if they exist. 
    
    p: boolean
        If True, print the headers of the fits-file.
        
    Attributes:
    -----------
    t: np.ndarray 
        Time vector (in seconds).
        
    rate: np.ndarray
        Rate vector (in counts/s).
        
    err: np.ndarray
        Error vector in rate (in counts/s).
    
    object: string
        Object for which we have constructed the lightcurve object.
        
    dt: float
        Time resolution (in seconds).
        
    R: np.float
        Mean count rate.
        
    N: int
        Number of data points in observation.
    
    Fvar: float
        Fractional variance amplitude (=rms) computed from the light curve.
        
    channels: string
        Channels used during the observation.
    
    minchan, maxchan: ints
        Min and max channels used.
    
    # After having called lc.energies(channel_to_kev): 
    
    deltaE: float
        Energy range between min and max channels used.
    
    E_mean: float
        Energy mean over the range between min and max channels used.
    
    &#34;&#34;&#34;
      
    def __init__(self, filename, keywords=[], p=0):
        if isinstance(filename,str):
            self.t, self.rate, self.err, self.object, self.channels, self.minchan, self.maxchan = self.extract_lc(filename, keywords, p)
            self.dt = self.t[1]-self.t[0] # should not (but could) be a gap here... 
            self.R = np.mean(self.rate)
            self.N = len(self.t)  
            
            self.Fvar = self.Fvar_from_lc(m=self.N//100) #use 100 segments as default; can be changed.
            
    def extract_lc(self, filename, keywords, p):
        data = extract_fits(filename, keywords, p=p)
        assert data[&#39;CONTENT&#39;]==&#39;LIGHT CURVE&#39;, &#39;Data does not come from a light curve object.&#39;

        t = np.array(data[&#39;TIME&#39;])
        rate = np.array(data[&#39;RATE&#39;])
        err = np.array(data[&#39;ERROR&#39;])
        obj = data[&#39;OBJECT&#39;]
        channels = data[&#39;CPIX&#39;]
        minchan = data[&#39;MINCHAN&#39;]
        maxchan = data[&#39;MAXCHAN&#39;]
        
        if len(data) &gt; 8:
            keys = [key for key,val in data.items()]   
            for i in range(7,len(data)):
                setattr(self, keys[i], data[keys[i]])
        
        return t, rate, err, obj, channels, minchan, maxchan
    
    def plot(self,m=0,n=0):
        &#34;&#34;&#34;
        Display the full lightcurve.

        Parameters:
        -----------
        m: int, optional, default = 0
            Number of time bins per segment.

        n: int, optional, default: 0 
            The n:th segment will be displayed in the full light curve.
        &#34;&#34;&#34;
        
        # Parameters 
        if m!=0:
            K = int(np.floor(self.N/m))
            print(r&#34;Number of line segments of \approx {:.0f}s will be: &#34;.format((self.t[1]-self.t[0])*m),K)
        else: 
            K = 1

        # Plot 
        ax = standard_plot()
        #plt.errorbar(t_seg,rate_seg,err_seg,fmt=&#39;o&#39;,markersize=2)
        plt.plot(self.t,self.rate,&#39;-&#39;)
        ax.axhline(self.R,color=&#39;r&#39;,label=&#39;R = {:.0f}&#39;.format(self.R))
        if m &gt; 0:
            ax.axvline(self.t[n*m],color=&#39;k&#39;)
            ax.axvline(self.t[(n+1)*m],color=&#39;k&#39;)
            ax.axvspan(self.t[n*m], self.t[(n+1)*m], alpha=0.4, color=&#39;k&#39;,label=&#39;Line segment&#39;)

        plt.legend(loc=&#39;best&#39;)
        plt.xlabel(&#39;Time [$s$]&#39;)
        plt.ylabel(&#39;Rate [$c/s$]&#39;)
        plt.title(self.object.replace(&#34;_&#34;,&#34; &#34;))
        plt.show()
        
    def extract_seg(self,m,n=0,to_print=False,to_plot=False):
        &#34;&#34;&#34;
        Extract (and potentially display) a light curve segment.

        Parameters:
        -----------
        m: int
            Number of time bins per segment. 

        n: int, optional, default: 0 
            The start (end) of the segment will be bin number m*n (m*(n+1)) 

        Returns: 
        --------
        t_seg, rate_seg, err_seg: np.ndarrays
            Time, rate, and error vectors for the segment.

        N_gamma, R, T: floats
            Number of photons (in counts), mean count rate (in counts/s) and length (in seconds) respectively. 
        &#34;&#34;&#34;

        # Pick out segment
        start = m*n
        stop = m*(n+1)
        
        t_seg = self.t[start:stop]
        rate_seg = self.rate[start:stop]
        err_seg = self.err[start:stop]

        # Relevant parameters
        T = t_seg[-1]-t_seg[0]
        R = np.mean(rate_seg)
        N_gamma = T*R

        # Subtract mean rate 
        #rate_seg = [x-R for x in rate_seg]

        # Print
        if to_print:
            print(&#39;R = average count rate = &#39;,R)
            print(&#39;N_\gamma = total counts = &#39;,N_gamma)
            print(&#39;T = total time = &#39;,T) 
            print(&#39;128s corresponds to {} time-elements&#39;.format(int(128/(t_seg[1]-t_seg[0]))))

        # Plot
        if to_plot:
            ax = standard_plot()
            #plt.errorbar(t_seg,rate_seg,err_seg,fmt=&#39;o&#39;,markersize=2)
            plt.plot(t_seg,rate_seg,&#39;-&#39;)
            ax.axhline(np.mean(rate_seg),color=&#39;r&#39;,label=&#39;R = {}&#39;.format(R))
            plt.legend(loc=&#39;upper right&#39;)
            plt.xlabel(&#39;Time [$s$]&#39;)
            plt.ylabel(&#39;Rate [$c/s$]&#39;)
            plt.title(&#39;CygX1 - light curve segment {}&#39;.format(n+1))
            plt.show()
        else:
            return t_seg, rate_seg, err_seg, N_gamma, R, T
        
    def energies(self, channel_to_kev):
        &#34;&#34;&#34;
        Channel to corresponding energy.
        
        Parameters:
        -----------
        channel_to_kev: np.ndarray
            Conversion from channel (index) to energy (keV).
        
        &#34;&#34;&#34;
        
        minchan = self.minchan
        if minchan != 0:
            minchan -= 1
        minene = channel_to_kev[minchan]
        if minchan == 0:
            minene = 0
        maxchan = self.maxchan
        maxene = channel_to_kev[maxchan]
        print(&#39;Energy channels (keV): &#39;,self.minchan,&#39;-&#39;,self.maxchan,&#39;(&#39;,minene,&#39;-&#39;,maxene,&#39;)&#39;)
        
        setattr(self, &#39;deltaE&#39;, maxene-minene)
        setattr(self, &#39;E_mean&#39;, (maxene+minene)/2)
        # setattr(self, &#39;E_err&#39;, (maxene-minene)/2) = deltaE/2
        
    def Fvar_from_lc(self,m,percentage_limit=0):
        &#34;&#34;&#34;
        Calculate the fractional root mean square (rms) variability amplitude from 
        the variance (S^2) in the light curve, by averaging over K=int(np.floor(lc.N/m)) segments. 
        &#34;&#34;&#34;

        return Fvar_from_lc(self,m,percentage_limit)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="API.lightcurve.Fvar_from_lc"><code class="name flex">
<span>def <span class="ident">Fvar_from_lc</span></span>(<span>self, m, percentage_limit=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the fractional root mean square (rms) variability amplitude from
the variance (S^2) in the light curve, by averaging over K=int(np.floor(lc.N/m)) segments.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Fvar_from_lc(self,m,percentage_limit=0):
    &#34;&#34;&#34;
    Calculate the fractional root mean square (rms) variability amplitude from 
    the variance (S^2) in the light curve, by averaging over K=int(np.floor(lc.N/m)) segments. 
    &#34;&#34;&#34;

    return Fvar_from_lc(self,m,percentage_limit)</code></pre>
</details>
</dd>
<dt id="API.lightcurve.energies"><code class="name flex">
<span>def <span class="ident">energies</span></span>(<span>self, channel_to_kev)</span>
</code></dt>
<dd>
<div class="desc"><p>Channel to corresponding energy.</p>
<h2 id="parameters">Parameters:</h2>
<p>channel_to_kev: np.ndarray
Conversion from channel (index) to energy (keV).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def energies(self, channel_to_kev):
    &#34;&#34;&#34;
    Channel to corresponding energy.
    
    Parameters:
    -----------
    channel_to_kev: np.ndarray
        Conversion from channel (index) to energy (keV).
    
    &#34;&#34;&#34;
    
    minchan = self.minchan
    if minchan != 0:
        minchan -= 1
    minene = channel_to_kev[minchan]
    if minchan == 0:
        minene = 0
    maxchan = self.maxchan
    maxene = channel_to_kev[maxchan]
    print(&#39;Energy channels (keV): &#39;,self.minchan,&#39;-&#39;,self.maxchan,&#39;(&#39;,minene,&#39;-&#39;,maxene,&#39;)&#39;)
    
    setattr(self, &#39;deltaE&#39;, maxene-minene)
    setattr(self, &#39;E_mean&#39;, (maxene+minene)/2)</code></pre>
</details>
</dd>
<dt id="API.lightcurve.extract_lc"><code class="name flex">
<span>def <span class="ident">extract_lc</span></span>(<span>self, filename, keywords, p)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_lc(self, filename, keywords, p):
    data = extract_fits(filename, keywords, p=p)
    assert data[&#39;CONTENT&#39;]==&#39;LIGHT CURVE&#39;, &#39;Data does not come from a light curve object.&#39;

    t = np.array(data[&#39;TIME&#39;])
    rate = np.array(data[&#39;RATE&#39;])
    err = np.array(data[&#39;ERROR&#39;])
    obj = data[&#39;OBJECT&#39;]
    channels = data[&#39;CPIX&#39;]
    minchan = data[&#39;MINCHAN&#39;]
    maxchan = data[&#39;MAXCHAN&#39;]
    
    if len(data) &gt; 8:
        keys = [key for key,val in data.items()]   
        for i in range(7,len(data)):
            setattr(self, keys[i], data[keys[i]])
    
    return t, rate, err, obj, channels, minchan, maxchan</code></pre>
</details>
</dd>
<dt id="API.lightcurve.extract_seg"><code class="name flex">
<span>def <span class="ident">extract_seg</span></span>(<span>self, m, n=0, to_print=False, to_plot=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract (and potentially display) a light curve segment.</p>
<h2 id="parameters">Parameters:</h2>
<p>m: int
Number of time bins per segment. </p>
<p>n: int, optional, default: 0
The start (end) of the segment will be bin number m<em>n (m</em>(n+1)) </p>
<h2 id="returns">Returns:</h2>
<p>t_seg, rate_seg, err_seg: np.ndarrays
Time, rate, and error vectors for the segment.</p>
<p>N_gamma, R, T: floats
Number of photons (in counts), mean count rate (in counts/s) and length (in seconds) respectively.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_seg(self,m,n=0,to_print=False,to_plot=False):
    &#34;&#34;&#34;
    Extract (and potentially display) a light curve segment.

    Parameters:
    -----------
    m: int
        Number of time bins per segment. 

    n: int, optional, default: 0 
        The start (end) of the segment will be bin number m*n (m*(n+1)) 

    Returns: 
    --------
    t_seg, rate_seg, err_seg: np.ndarrays
        Time, rate, and error vectors for the segment.

    N_gamma, R, T: floats
        Number of photons (in counts), mean count rate (in counts/s) and length (in seconds) respectively. 
    &#34;&#34;&#34;

    # Pick out segment
    start = m*n
    stop = m*(n+1)
    
    t_seg = self.t[start:stop]
    rate_seg = self.rate[start:stop]
    err_seg = self.err[start:stop]

    # Relevant parameters
    T = t_seg[-1]-t_seg[0]
    R = np.mean(rate_seg)
    N_gamma = T*R

    # Subtract mean rate 
    #rate_seg = [x-R for x in rate_seg]

    # Print
    if to_print:
        print(&#39;R = average count rate = &#39;,R)
        print(&#39;N_\gamma = total counts = &#39;,N_gamma)
        print(&#39;T = total time = &#39;,T) 
        print(&#39;128s corresponds to {} time-elements&#39;.format(int(128/(t_seg[1]-t_seg[0]))))

    # Plot
    if to_plot:
        ax = standard_plot()
        #plt.errorbar(t_seg,rate_seg,err_seg,fmt=&#39;o&#39;,markersize=2)
        plt.plot(t_seg,rate_seg,&#39;-&#39;)
        ax.axhline(np.mean(rate_seg),color=&#39;r&#39;,label=&#39;R = {}&#39;.format(R))
        plt.legend(loc=&#39;upper right&#39;)
        plt.xlabel(&#39;Time [$s$]&#39;)
        plt.ylabel(&#39;Rate [$c/s$]&#39;)
        plt.title(&#39;CygX1 - light curve segment {}&#39;.format(n+1))
        plt.show()
    else:
        return t_seg, rate_seg, err_seg, N_gamma, R, T</code></pre>
</details>
</dd>
<dt id="API.lightcurve.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, m=0, n=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Display the full lightcurve.</p>
<h2 id="parameters">Parameters:</h2>
<p>m: int, optional, default = 0
Number of time bins per segment.</p>
<p>n: int, optional, default: 0
The n:th segment will be displayed in the full light curve.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(self,m=0,n=0):
    &#34;&#34;&#34;
    Display the full lightcurve.

    Parameters:
    -----------
    m: int, optional, default = 0
        Number of time bins per segment.

    n: int, optional, default: 0 
        The n:th segment will be displayed in the full light curve.
    &#34;&#34;&#34;
    
    # Parameters 
    if m!=0:
        K = int(np.floor(self.N/m))
        print(r&#34;Number of line segments of \approx {:.0f}s will be: &#34;.format((self.t[1]-self.t[0])*m),K)
    else: 
        K = 1

    # Plot 
    ax = standard_plot()
    #plt.errorbar(t_seg,rate_seg,err_seg,fmt=&#39;o&#39;,markersize=2)
    plt.plot(self.t,self.rate,&#39;-&#39;)
    ax.axhline(self.R,color=&#39;r&#39;,label=&#39;R = {:.0f}&#39;.format(self.R))
    if m &gt; 0:
        ax.axvline(self.t[n*m],color=&#39;k&#39;)
        ax.axvline(self.t[(n+1)*m],color=&#39;k&#39;)
        ax.axvspan(self.t[n*m], self.t[(n+1)*m], alpha=0.4, color=&#39;k&#39;,label=&#39;Line segment&#39;)

    plt.legend(loc=&#39;best&#39;)
    plt.xlabel(&#39;Time [$s$]&#39;)
    plt.ylabel(&#39;Rate [$c/s$]&#39;)
    plt.title(self.object.replace(&#34;_&#34;,&#34; &#34;))
    plt.show()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="API.Fvar_from_lc" href="#API.Fvar_from_lc">Fvar_from_lc</a></code></li>
<li><code><a title="API.Fvar_from_ps" href="#API.Fvar_from_ps">Fvar_from_ps</a></code></li>
<li><code><a title="API.coherence_intrinsic" href="#API.coherence_intrinsic">coherence_intrinsic</a></code></li>
<li><code><a title="API.coherence_noiseless" href="#API.coherence_noiseless">coherence_noiseless</a></code></li>
<li><code><a title="API.comp_gamma2" href="#API.comp_gamma2">comp_gamma2</a></code></li>
<li><code><a title="API.compute_coherence_intrinsic" href="#API.compute_coherence_intrinsic">compute_coherence_intrinsic</a></code></li>
<li><code><a title="API.compute_delta_gamma2_int" href="#API.compute_delta_gamma2_int">compute_delta_gamma2_int</a></code></li>
<li><code><a title="API.conditions" href="#API.conditions">conditions</a></code></li>
<li><code><a title="API.covariance_spectrum" href="#API.covariance_spectrum">covariance_spectrum</a></code></li>
<li><code><a title="API.cross_spec" href="#API.cross_spec">cross_spec</a></code></li>
<li><code><a title="API.error_change" href="#API.error_change">error_change</a></code></li>
<li><code><a title="API.extract_fits" href="#API.extract_fits">extract_fits</a></code></li>
<li><code><a title="API.ifft_smallfreqband" href="#API.ifft_smallfreqband">ifft_smallfreqband</a></code></li>
<li><code><a title="API.load_lightcurve" href="#API.load_lightcurve">load_lightcurve</a></code></li>
<li><code><a title="API.log_rebin" href="#API.log_rebin">log_rebin</a></code></li>
<li><code><a title="API.matchingKeys" href="#API.matchingKeys">matchingKeys</a></code></li>
<li><code><a title="API.multiply_w_spectra" href="#API.multiply_w_spectra">multiply_w_spectra</a></code></li>
<li><code><a title="API.percentage_of_filled_time_bins" href="#API.percentage_of_filled_time_bins">percentage_of_filled_time_bins</a></code></li>
<li><code><a title="API.pick_out_freq_from_lc" href="#API.pick_out_freq_from_lc">pick_out_freq_from_lc</a></code></li>
<li><code><a title="API.plot_coherence" href="#API.plot_coherence">plot_coherence</a></code></li>
<li><code><a title="API.plot_timelag" href="#API.plot_timelag">plot_timelag</a></code></li>
<li><code><a title="API.remove_freq" href="#API.remove_freq">remove_freq</a></code></li>
<li><code><a title="API.rms_freqband" href="#API.rms_freqband">rms_freqband</a></code></li>
<li><code><a title="API.rms_vs_energy" href="#API.rms_vs_energy">rms_vs_energy</a></code></li>
<li><code><a title="API.standard_plot" href="#API.standard_plot">standard_plot</a></code></li>
<li><code><a title="API.subtract_overlapping_energybands" href="#API.subtract_overlapping_energybands">subtract_overlapping_energybands</a></code></li>
<li><code><a title="API.time_lag" href="#API.time_lag">time_lag</a></code></li>
<li><code><a title="API.timer" href="#API.timer">timer</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="API.PowerSpectrum" href="#API.PowerSpectrum">PowerSpectrum</a></code></h4>
<ul class="two-column">
<li><code><a title="API.PowerSpectrum.Fvar_from_ps" href="#API.PowerSpectrum.Fvar_from_ps">Fvar_from_ps</a></code></li>
<li><code><a title="API.PowerSpectrum.fft_seg" href="#API.PowerSpectrum.fft_seg">fft_seg</a></code></li>
<li><code><a title="API.PowerSpectrum.noise_power" href="#API.PowerSpectrum.noise_power">noise_power</a></code></li>
<li><code><a title="API.PowerSpectrum.normalize_power" href="#API.PowerSpectrum.normalize_power">normalize_power</a></code></li>
<li><code><a title="API.PowerSpectrum.plot" href="#API.PowerSpectrum.plot">plot</a></code></li>
<li><code><a title="API.PowerSpectrum.powspec" href="#API.PowerSpectrum.powspec">powspec</a></code></li>
<li><code><a title="API.PowerSpectrum.rebin" href="#API.PowerSpectrum.rebin">rebin</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="API.lightcurve" href="#API.lightcurve">lightcurve</a></code></h4>
<ul class="">
<li><code><a title="API.lightcurve.Fvar_from_lc" href="#API.lightcurve.Fvar_from_lc">Fvar_from_lc</a></code></li>
<li><code><a title="API.lightcurve.energies" href="#API.lightcurve.energies">energies</a></code></li>
<li><code><a title="API.lightcurve.extract_lc" href="#API.lightcurve.extract_lc">extract_lc</a></code></li>
<li><code><a title="API.lightcurve.extract_seg" href="#API.lightcurve.extract_seg">extract_seg</a></code></li>
<li><code><a title="API.lightcurve.plot" href="#API.lightcurve.plot">plot</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>